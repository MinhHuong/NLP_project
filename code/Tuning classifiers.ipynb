{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries and Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading libraries takes 0.0003 s\n"
     ]
    }
   ],
   "source": [
    "# import time to find consuming steps\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "# utility libraries\n",
    "import numpy as np\n",
    "import csv as csv\n",
    "from sklearn import preprocessing as pre\n",
    "from itertools import cycle\n",
    "from scipy import interp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# classifier for classification\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate, cross_val_score, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import recall_score, roc_curve, auc, average_precision_score, precision_recall_curve\n",
    "\n",
    "end = time.time()\n",
    "print('Loading libraries takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading dataset (training, testing, node information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '../data/' # path to the data\n",
    "path_submission = '../submission/' # path to submission files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training set & extracting labels takes 3.0183 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== read training data as str ====== #\n",
    "training = np.genfromtxt(path_data + 'training_set.txt', dtype=str)\n",
    "\n",
    "# ====== extract labels ====== #\n",
    "labels = training[:, 2].astype(int) # get the labels\n",
    "\n",
    "end = time.time()\n",
    "print('Reading training set & extracting labels takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training features takes 11.7031 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== read training features ====== #\n",
    "orig_training_features = np.genfromtxt(path_data + 'training_features.csv', delimiter=',', skip_header=1, dtype=float)\n",
    "\n",
    "end = time.time()\n",
    "print('Reading training features takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading testing features takes 0.5495 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== read testing features as str ====== #\n",
    "orig_testing_features = np.genfromtxt(path_data + 'testing_features.csv', delimiter=',', skip_header=1, dtype=float)\n",
    "\n",
    "end = time.time()\n",
    "print('Reading testing features takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features: (615512, 17)\n",
      "Labels: (615512,)\n",
      "Testing features: (32648, 17)\n"
     ]
    }
   ],
   "source": [
    "print('Training features:', orig_training_features.shape)\n",
    "print('Labels:', labels.shape)\n",
    "print('Testing features:', orig_testing_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Picking up some features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we might need to remove some features read from file. Here, we remove features by its index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_features = [\n",
    "    'temporal_difference', # 0\n",
    "    'common_authors', # 1\n",
    "    'same_journal', # 2\n",
    "    'cosine_sim', # 3\n",
    "    'overlapping_title', # 4\n",
    "    'max_degrees', # 5\n",
    "    'common_neighbors', # 6\n",
    "    'jaccard_coefficient', # 7\n",
    "    'max_pagerank', # 8\n",
    "    'max_betweenness', # 9\n",
    "    'in_kcore', # 10\n",
    "    'adamic_adar', # 11\n",
    "    'katz_index', # 12\n",
    "    'cosine_sim_w2v', # 13\n",
    "    'katz_linkpred', # 14\n",
    "    'pref_attach', # 15\n",
    "    'res_alloc' # 16\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features: (615512, 15)\n"
     ]
    }
   ],
   "source": [
    "# remove very features before training\n",
    "to_remove = [14,13]\n",
    "training_features = np.nan_to_num(np.delete(orig_training_features, to_remove, 1))\n",
    "testing_features = np.nan_to_num(np.delete(orig_testing_features, to_remove, 1))\n",
    "features = np.delete(orig_features, to_remove)\n",
    "\n",
    "print('Training features:', training_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: [[ 0.00000000e+00  0.00000000e+00  1.00000000e+00  1.99966215e-01\n",
      "   2.00000000e+00  1.20000000e+01  1.00000000e+00  5.88235294e-02\n",
      "  -1.04952174e+01  1.05093708e+01  0.00000000e+00  5.13898342e-01\n",
      "  -5.58044870e+00  4.27666612e+00  1.42857143e-01]\n",
      " [ 1.00000000e+00  0.00000000e+00  0.00000000e+00  6.43694475e-02\n",
      "   1.00000000e+00  1.47000000e+02  2.00000000e+01  9.70873786e-02\n",
      "  -9.04507102e+00  1.13932537e+01  1.00000000e+00  4.32036615e+00\n",
      "  -4.21648550e+00  9.35988044e+00  2.26400795e-01]\n",
      " [ 2.00000000e+00  0.00000000e+00  0.00000000e+00  2.05371115e-02\n",
      "   0.00000000e+00  5.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  -1.10308202e+01  9.53921731e+00  0.00000000e+00  0.00000000e+00\n",
      "  -5.64652461e+00  1.60943791e+00  0.00000000e+00]\n",
      " [ 4.00000000e+00  0.00000000e+00  0.00000000e+00  5.93784382e-02\n",
      "   0.00000000e+00  2.00000000e+01  0.00000000e+00  0.00000000e+00\n",
      "  -1.06715646e+01  8.04019582e+00  5.00000000e-01  0.00000000e+00\n",
      "  -5.25171866e+00  5.63478960e+00  0.00000000e+00]\n",
      " [ 5.00000000e+00  0.00000000e+00  0.00000000e+00  9.85264277e-02\n",
      "   0.00000000e+00  2.40000000e+01  0.00000000e+00  0.00000000e+00\n",
      "  -1.04743869e+01  9.38724288e+00  5.00000000e-01  0.00000000e+00\n",
      "  -5.36014155e+00  5.12396398e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  3.95819232e-01\n",
      "   0.00000000e+00  3.80000000e+01  1.40000000e+01  2.37288136e-01\n",
      "  -1.02789087e+01  8.87265376e+00  1.00000000e+00  3.17502987e+00\n",
      "  -4.96955029e+00  7.19293422e+00  2.13351568e-01]\n",
      " [ 4.00000000e+00  1.00000000e+00  0.00000000e+00  1.87225690e-01\n",
      "   0.00000000e+00  7.39000000e+02  1.20000000e+01  1.52284264e-02\n",
      "  -7.28060674e+00  1.59955066e+01  1.00000000e+00  2.46874101e+00\n",
      "  -2.88068326e+00  1.07161718e+01  1.16598409e-01]\n",
      " [ 7.00000000e+00  0.00000000e+00  0.00000000e+00  8.62705384e-02\n",
      "   1.00000000e+00  8.60000000e+01  0.00000000e+00  0.00000000e+00\n",
      "  -9.25626085e+00  1.26364092e+01  5.00000000e-01  0.00000000e+00\n",
      "  -4.86388004e+00  7.16239750e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  4.18143621e-02\n",
      "   0.00000000e+00  1.00000000e+02  5.00000000e+00  3.67647059e-02\n",
      "  -9.19986647e+00  1.30247650e+01  1.00000000e+00  9.42862303e-01\n",
      "  -4.29294528e+00  8.31874225e+00  3.30145480e-02]\n",
      " [ 8.00000000e+00  0.00000000e+00  0.00000000e+00  6.04475137e-02\n",
      "   0.00000000e+00  3.00000000e+01  0.00000000e+00  0.00000000e+00\n",
      "  -9.55846355e+00  1.19906480e+01  0.00000000e+00  0.00000000e+00\n",
      "  -5.46850804e+00  4.78749174e+00  0.00000000e+00]]\n",
      "Testing: [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.18040092e-01\n",
      "   0.00000000e+00  5.90000000e+01  0.00000000e+00  0.00000000e+00\n",
      "  -9.71123221e+00  1.25886650e+01  1.00000000e+00  0.00000000e+00\n",
      "  -4.69918791e+00  6.96790920e+00  0.00000000e+00]\n",
      " [ 1.00000000e+00  0.00000000e+00  0.00000000e+00  3.07862648e-01\n",
      "   2.00000000e+00  3.02000000e+02  2.40000000e+01  7.43034056e-02\n",
      "  -8.05014677e+00  1.47780702e+01  1.00000000e+00  5.37797275e+00\n",
      "  -3.80632847e+00  9.51708951e+00  3.11534723e-01]\n",
      " [ 2.00000000e+00  0.00000000e+00  1.00000000e+00  2.07538045e-01\n",
      "   1.00000000e+00  7.39000000e+02  5.90000000e+01  6.53377630e-02\n",
      "  -7.28060674e+00  1.59955066e+01  1.00000000e+00  1.50536117e+01\n",
      "  -2.88068326e+00  1.20124697e+01  1.34259427e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  1.00000000e+00  1.61124069e-01\n",
      "   1.00000000e+00  6.50000000e+01  2.10000000e+01  2.21052632e-01\n",
      "  -9.78108665e+00  1.12049512e+01  1.00000000e+00  4.89942438e+00\n",
      "  -4.78584888e+00  8.10621290e+00  2.98418991e-01]\n",
      " [ 5.00000000e+00  0.00000000e+00  0.00000000e+00  3.18244526e-01\n",
      "   0.00000000e+00  1.50000000e+02  0.00000000e+00  0.00000000e+00\n",
      "  -8.84688464e+00  1.31624585e+01  5.00000000e-01  0.00000000e+00\n",
      "  -4.22347698e+00  6.95654544e+00  0.00000000e+00]\n",
      " [ 4.00000000e+00  1.00000000e+00  0.00000000e+00  3.46687156e-02\n",
      "   0.00000000e+00  3.50000000e+01  0.00000000e+00  0.00000000e+00\n",
      "  -1.03515694e+01  1.02670683e+01  5.00000000e-01  0.00000000e+00\n",
      "  -4.97580013e+00  6.19440539e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  2.49026627e-02\n",
      "   1.00000000e+00  4.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  -1.11370020e+01  5.53651846e+00  0.00000000e+00  0.00000000e+00\n",
      "  -5.65542043e+00  1.38629436e+00  0.00000000e+00]\n",
      " [ 1.00000000e+00  0.00000000e+00  1.00000000e+00  1.99910478e-01\n",
      "   1.00000000e+00  4.20000000e+01  6.00000000e+00  1.05263158e-01\n",
      "  -9.81535402e+00  1.10795443e+01  1.00000000e+00  1.46868886e+00\n",
      "  -5.30505257e+00  6.78219206e+00  1.01750117e-01]\n",
      " [ 7.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  7.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  -1.09464534e+01  7.44026286e+00  0.00000000e+00  0.00000000e+00\n",
      "  -5.61555170e+00  2.63905733e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  1.00000000e+00  3.28366499e-01\n",
      "   1.00000000e+00  1.30000000e+01  4.00000000e+00  1.90476190e-01\n",
      "  -1.08153264e+01  8.02905778e+00  0.00000000e+00  1.27898053e+00\n",
      "  -5.50044342e+00  5.04985601e+00  1.88131313e-01]]\n"
     ]
    }
   ],
   "source": [
    "print('Training:', training_features[0:10])\n",
    "print('Testing:', testing_features[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_submission(filename, pred):\n",
    "    '''\n",
    "    Write prediction result in a submission file\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filename: name of submission file\n",
    "    pred: prediction array\n",
    "    \n",
    "    '''\n",
    "    with open(path_submission + filename, 'w', newline='') as f:\n",
    "        csv_out = csv.writer(f)\n",
    "        csv_out.writerow(['id','category'])\n",
    "        for row in pred:\n",
    "            csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Scaling features ====== #\n",
    "training_features_scale = pre.scale(training_features)\n",
    "testing_features_scale = pre.scale(testing_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_size = len(testing_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running cross-validation for: 0\n",
      "Running cross-validation for: 1\n",
      "Running cross-validation for: 2\n",
      "Tuning parameters for SVM takes 1768.1033 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# a list of svm classifiers with differenet settings\n",
    "clfs_svm = [\n",
    "    svm.LinearSVC(penalty='l2', loss='squared_hinge', C=1.0, fit_intercept=True),\n",
    "    svm.LinearSVC(penalty='l2', loss='hinge', C=1.0, fit_intercept=True),\n",
    "    svm.LinearSVC(penalty='l1', loss='squared_hinge', C=1.0, fit_intercept=True, dual=False)\n",
    "]\n",
    "\n",
    "# check for best settings (without tuning C)\n",
    "tune_svm_scores = []\n",
    "for index, clf in enumerate(clfs_svm):\n",
    "    print('Running cross-validation for:', index)\n",
    "    tune_svm_scores.append(cross_val_score(clf, training_features_scale, labels, cv=10, scoring='f1'))\n",
    "    \n",
    "end = time.time()\n",
    "print('Tuning parameters for SVM takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.96474375 0.96784078 0.96441065]\n",
      "Best setting for SVM: LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
      "     penalty='l2', random_state=None, tol=0.0001, verbose=0)\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(tune_svm_scores,axis=1))\n",
    "best_clf_svm = clfs_svm[np.argmax(np.mean(tune_svm_scores,axis=1))]\n",
    "print('Best setting for SVM:', best_clf_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with SVM Linear SVC takes 136.2333 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "best_clf_svm = svm.LinearSVC(penalty='l2', loss='hinge', C=1.0, fit_intercept=True)\n",
    "\n",
    "# ====== training and prediction with SVM and scaled features ====== #\n",
    "best_clf_svm.fit(training_features, labels)\n",
    "pred_svm = list(best_clf_svm.predict(testing_features))\n",
    "pred_svm = zip(range(testing_size), pred_svm)\n",
    "\n",
    "end = time.time()\n",
    "print('Training with SVM Linear SVC takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_submission('tuned_submission_svm_01.csv', pred_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score with SVM: 0.9678 (+/- 0.0005)\n"
     ]
    }
   ],
   "source": [
    "svm_scores = cross_val_score(best_clf_svm, training_features_scale, labels, cv=5, scoring='f1')\n",
    "print(\"F1-score with SVM: %0.4f (+/- %0.4f)\" % (svm_scores.mean(), svm_scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-A. SVM without scaling (do not use, always scale for better performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with SVM Linear SVC takes 4.2013 s\n"
     ]
    }
   ],
   "source": [
    "# start = time.time()\n",
    "\n",
    "# # ====== training and predicting with SVM ====== #\n",
    "# clf_svm = svm.LinearSVC(dual=False, C=1.0)\n",
    "# clf_svm.fit(training_features, labels)\n",
    "# pred_svm = list(clf_svm.predict(testing_features))\n",
    "# pred_svm = zip(range(len(testing_features)), pred_svm)\n",
    "\n",
    "# end = time.time()\n",
    "# print('Training with SVM Linear SVC takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_submission('submission_svm_11.csv', pred_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score with SVM: 0.9650 (+/- 0.0021)\n"
     ]
    }
   ],
   "source": [
    "# svm_scores = cross_val_score(clf_svm, training_features, labels, cv=5, scoring='f1')\n",
    "# print(\"F1-score with SVM: %0.4f (+/- %0.4f)\" % (svm_scores.mean(), svm_scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-B. SVM with scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with SVM Linear SVC + scaling takes 1.8097 s\n"
     ]
    }
   ],
   "source": [
    "# start = time.time()\n",
    "\n",
    "# # ====== training and prediction with SVM and scaled features ====== #\n",
    "# clf_svm_scale = svm.LinearSVC(dual=False, C=1.0)\n",
    "# clf_svm_scale.fit(training_features_scale, labels)\n",
    "# pred_svm_scale = list(clf_svm_scale.predict(testing_features_scale))\n",
    "# pred_svm_scale = zip(range(testing_size), pred_svm_scale)\n",
    "\n",
    "# end = time.time()\n",
    "# print('Training with SVM Linear SVC + scaling takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_submission('submission_svm_16_scale.csv', pred_svm_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score with SVM scaling: 0.9650 (+/- 0.0021)\n"
     ]
    }
   ],
   "source": [
    "# svm_scale_scores = cross_val_score(clf_svm_scale, training_features, labels, cv=5, scoring='f1')\n",
    "# print(\"F1-score with SVM scaling: %0.4f (+/- %0.4f)\" % (svm_scale_scores.mean(), svm_scale_scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RandomForest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running cross-validation for: 0\n",
      "Tuning parameters for Random Forest, clf[0] takes 144.8391 s\n",
      "Running cross-validation for: 1\n",
      "Tuning parameters for Random Forest, clf[1] takes 141.8941 s\n",
      "Running cross-validation for: 2\n",
      "Tuning parameters for Random Forest, clf[2] takes 142.2216 s\n",
      "Running cross-validation for: 3\n",
      "Tuning parameters for Random Forest, clf[3] takes 142.0169 s\n"
     ]
    }
   ],
   "source": [
    "clfs_rf = [\n",
    "    RandomForestClassifier(),\n",
    "    RandomForestClassifier(max_features='sqrt'),\n",
    "    RandomForestClassifier(max_features='log2'),\n",
    "    RandomForestClassifier(max_features=0.3),\n",
    "]\n",
    "\n",
    "# check for best settings (without tuning C)\n",
    "tune_rf_scores = []\n",
    "for index, clf in enumerate(clfs_rf):\n",
    "    start = time.time()\n",
    "    \n",
    "    print('Running cross-validation for:', index)\n",
    "    tune_rf_scores.append(cross_val_score(clf, training_features, labels, cv=10, scoring='f1'))\n",
    "\n",
    "    end = time.time()\n",
    "    print('Tuning parameters for Random Forest, clf[%d] takes %.4f s' % (index, end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.96977626 0.9697922  0.96988879 0.97002613]\n",
      "Best setting for Logistic Regression: RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.3, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(tune_rf_scores, axis=1))\n",
    "best_clf_rf = clfs_rf[np.argmax(np.mean(tune_rf_scores,axis=1))]\n",
    "print('Best setting for Logistic Regression:', best_clf_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running cross-validation for: 0\n",
      "Tuning parameters for Random Forest, clf[0] takes 155.6879 s\n",
      "Running cross-validation for: 1\n",
      "Tuning parameters for Random Forest, clf[1] takes 437.9482 s\n",
      "Running cross-validation for: 2\n",
      "Tuning parameters for Random Forest, clf[2] takes 1808.1578 s\n",
      "Running cross-validation for: 3\n",
      "Tuning parameters for Random Forest, clf[3] takes 990.1995 s\n",
      "Running cross-validation for: 4\n",
      "Tuning parameters for Random Forest, clf[4] takes 1310.2704 s\n"
     ]
    }
   ],
   "source": [
    "max_feat = 0.3\n",
    "\n",
    "clfs_rf = [\n",
    "    RandomForestClassifier(max_features=max_feat, n_estimators=10),\n",
    "    RandomForestClassifier(max_features=max_feat, n_estimators=30),\n",
    "    RandomForestClassifier(max_features=max_feat, n_estimators=50),\n",
    "    RandomForestClassifier(max_features=max_feat, n_estimators=70),\n",
    "    RandomForestClassifier(max_features=max_feat, n_estimators=90),\n",
    "]\n",
    "\n",
    "# check for best settings (without tuning C)\n",
    "tune_rf_scores = []\n",
    "for index, clf in enumerate(clfs_rf):\n",
    "    start = time.time()\n",
    "    \n",
    "    print('Running cross-validation for:', index)\n",
    "    tune_rf_scores.append(cross_val_score(clf, training_features, labels, cv=10, scoring='f1'))\n",
    "\n",
    "    end = time.time()\n",
    "    print('Tuning parameters for Random Forest, clf[%d] takes %.4f s' % (index, end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.96991399 0.97098628 0.97121565 0.97128654 0.97139856]\n",
      "Best setting for Logistic Regression: RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.3, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=90, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(tune_rf_scores, axis=1))\n",
    "best_clf_rf = clfs_rf[np.argmax(np.mean(tune_rf_scores,axis=1))]\n",
    "print('Best setting for Logistic Regression:', best_clf_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Random Forest takes 737.4384 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "best_clf_rf = RandomForestClassifier(\n",
    "    max_features=0.3, \n",
    "    n_estimators=90\n",
    ")\n",
    "\n",
    "# ====== training and prediction with Random Forest ====== #\n",
    "best_clf_rf.fit(training_features, labels)\n",
    "pred_rf = list(best_clf_rf.predict(testing_features))\n",
    "pred_rf = zip(range(testing_size), pred_rf)\n",
    "\n",
    "end = time.time()\n",
    "print('Training with Random Forest takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_submission('tuned_submission_rf_02.csv', pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of RandomForest: 0.9716 (+/- 0.0006)\n"
     ]
    }
   ],
   "source": [
    "rf_scores = cross_val_score(best_clf_rf, training_features, labels, cv=5, scoring='f1')\n",
    "print(\"Accuracy of RandomForest: %0.4f (+/- %0.4f)\" % (rf_scores.mean(), rf_scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running cross-validation for: 0\n",
      "Running cross-validation for: 1\n",
      "Running cross-validation for: 2\n",
      "Tuning parameters for Logistic Regression takes 358.0291 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# a list of svm classifiers with differenet settings\n",
    "clfs_logreg = [\n",
    "    LogisticRegression(penalty='l2', solver='liblinear'),\n",
    "    LogisticRegression(penalty='l2', solver='newton-cg'),\n",
    "    LogisticRegression(penalty='l2', solver='lbfgs')\n",
    "    #LogisticRegression(penalty='l1', solver='saga', max_iter=5000) #takes too long\n",
    "]\n",
    "\n",
    "# check for best settings (without tuning C)\n",
    "tune_logreg_scores = []\n",
    "for index, clf in enumerate(clfs_logreg):\n",
    "    print('Running cross-validation for:', index)\n",
    "    tune_logreg_scores.append(cross_val_score(clf, training_features_scale, labels, cv=10, scoring='f1'))\n",
    "    \n",
    "end = time.time()\n",
    "print('Tuning parameters for Logistic Regression takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.96163912 0.96170016 0.96169857]\n",
      "Best setting for Logistic Regression: LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='newton-cg', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(tune_logreg_scores, axis=1))\n",
    "best_clf_logreg = clfs_logreg[np.argmax(np.mean(tune_logreg_scores,axis=1))]\n",
    "print('Best setting for Logistic Regression:', best_clf_logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Logistic Regression takes 17.7848 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== training and prediction with Logistic Regression and scaled features ====== #\n",
    "best_clf_logreg.fit(training_features_scale, labels)\n",
    "pred_logreg = list(best_clf_logreg.predict(testing_features_scale))\n",
    "pred_logreg = zip(range(testing_size), pred_logreg)\n",
    "\n",
    "end = time.time()\n",
    "print('Training with Logistic Regression takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_submission('tuned_submission_lg_02.csv', pred_logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression: 0.9616 (+/- 0.0006)\n"
     ]
    }
   ],
   "source": [
    "logreg_scores = cross_val_score(best_clf_logreg, training_features_scale, labels, cv=5, scoring='f1')\n",
    "print(\"Accuracy of Logistic Regression: %0.4f (+/- %0.4f)\" % (logreg_scores.mean(), logreg_scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Logistic Regression without scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Logistic Regression takes 7.7782 s\n"
     ]
    }
   ],
   "source": [
    "# start = time.time()\n",
    "\n",
    "# # ====== training and prediction with Logistic Regression ====== #\n",
    "# clf_lg = LogisticRegression()\n",
    "# clf_lg.fit(training_features, labels)\n",
    "# pred_lg = list(clf_lg.predict(testing_features))\n",
    "# pred_lg = zip(range(testing_size), pred_lg)\n",
    "\n",
    "# end = time.time()\n",
    "# print('Training with Logistic Regression takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_submission('submission_lg_07.csv', pred_lg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression: 0.9669 (+/- 0.0008)\n"
     ]
    }
   ],
   "source": [
    "# lg_scores = cross_val_score(clf_lg, training_features, labels, cv=5, scoring='f1')\n",
    "# print(\"Accuracy of Logistic Regression: %0.4f (+/- %0.4f)\" % (lg_scores.mean(), lg_scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Logistic Regression with scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Logistic Regression + scaling takes 2.5854 s\n"
     ]
    }
   ],
   "source": [
    "# start = time.time()\n",
    "\n",
    "# # ====== training and prediction with Logistic Regression + scaling ====== #\n",
    "# clf_lg_scale = LogisticRegression()\n",
    "# clf_lg_scale.fit(training_features_scale, labels)\n",
    "# pred_lg_scale = list(clf_lg_scale.predict(testing_features_scale))\n",
    "# pred_lg_scale = zip(range(testing_size), pred_lg_scale)\n",
    "\n",
    "# end = time.time()\n",
    "# print('Training with Logistic Regression + scaling takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_submission('submission_lg_07_scale.csv', pred_lg_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score with Logistic Regression + scaling: 1.0000 (+/- 0.0000)\n"
     ]
    }
   ],
   "source": [
    "# lg_scale_scores = cross_val_score(clf_lg_scale, training_features_scale, labels, cv=5, scoring='f1')\n",
    "# print(\"F1-score with Logistic Regression + scaling: %0.4f (+/- %0.4f)\" % (lg_scale_scores.mean(), lg_scale_scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network (simple version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Neural Network without scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Neural Networks takes 629.5872 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "clf_nn = MLPClassifier(\n",
    "    hidden_layer_sizes = (50,60,70,40,50,30,20,10),\n",
    "    activation = 'relu',\n",
    "    solver = 'adam',\n",
    "    early_stopping = True\n",
    ")\n",
    "clf_nn.fit(training_features, labels)\n",
    "pred_nn = clf_nn.predict(testing_features)\n",
    "pred_nn = zip(range(testing_size), pred_nn)\n",
    "\n",
    "end = time.time()\n",
    "print('Training with Neural Networks takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_submission('submission_nn_27.csv', pred_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# cross validation\n",
    "nn_scores = cross_val_score(clf_nn, training_features, labels, cv=5, scoring='f1')\n",
    "print(\"F1-score of Neural Network: %0.4f (+/- %0.4f)\" % (nn_scores.mean(), nn_scores.std() * 2))\n",
    "\n",
    "end = time.time()\n",
    "print('Cross validation evaluation on Neural Network takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Neural Network with scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Neural Networks + scaling takes 212.0493 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "clf_nn_scale = MLPClassifier(\n",
    "    hidden_layer_sizes = (50,60,70,40,50,30,20,10),\n",
    "    activation = 'relu',\n",
    "    solver = 'adam',\n",
    "    early_stopping = True\n",
    ")\n",
    "clf_nn_scale.fit(training_features_scale, labels)\n",
    "pred_nn_scale = clf_nn_scale.predict(testing_features_scale)\n",
    "pred_nn_scale = zip(range(testing_size), pred_nn_scale)\n",
    "\n",
    "end = time.time()\n",
    "print('Training with Neural Networks + scaling takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_submission('tuned_submission_nn_01_scale.csv', pred_nn_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score of Neural Network + scaling: 0.9978 (+/- 0.0022)\n"
     ]
    }
   ],
   "source": [
    "nn_scale_scores = cross_val_score(clf_nn_scale, training_features, labels, cv=5, scoring='f1')\n",
    "print(\"F1-score of Neural Network + scaling: %0.4f (+/- %0.4f)\" % (nn_scale_scores.mean(), nn_scale_scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running cross-validation for: 0\n",
      "Tuning parameters for Gradient Boosting, setting 0 takes 412.8917 s\n",
      "Running cross-validation for: 1\n",
      "Tuning parameters for Gradient Boosting, setting 1 takes 738.6681 s\n",
      "Running cross-validation for: 2\n",
      "Tuning parameters for Gradient Boosting, setting 2 takes 1298.6631 s\n",
      "Running cross-validation for: 3\n",
      "Tuning parameters for Gradient Boosting, setting 3 takes 1258.4368 s\n",
      "Running cross-validation for: 4\n",
      "Tuning parameters for Gradient Boosting, setting 4 takes 1242.2505 s\n"
     ]
    }
   ],
   "source": [
    "# a list of svm classifiers with differenet settings\n",
    "clfs_gb = [\n",
    "    GradientBoostingClassifier(n_estimators=40),\n",
    "    GradientBoostingClassifier(n_estimators=60),\n",
    "    GradientBoostingClassifier(n_estimators=80),\n",
    "    GradientBoostingClassifier(n_estimators=100),\n",
    "    GradientBoostingClassifier(n_estimators=120)\n",
    "]\n",
    "\n",
    "# check for best settings: number of estimators\n",
    "tune_gb_scores = []\n",
    "for index, clf in enumerate(clfs_gb):\n",
    "    start = time.time()\n",
    "    print('Running cross-validation for:', index)\n",
    "    tune_gb_scores.append(cross_val_score(clf, training_features_scale, labels, cv=10, scoring='f1'))\n",
    "    end = time.time()\n",
    "    print('Tuning parameters for Gradient Boosting, setting %d takes %.4f s' % (index, end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.96959925 0.97038891 0.97089067 0.97112219 0.97123316]\n",
      "Best setting for Gradient Boosting: GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=120,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(tune_gb_scores, axis=1))\n",
    "best_clf_gb = clfs_gb[np.argmax(np.mean(tune_gb_scores,axis=1))]\n",
    "print('Best setting for Gradient Boosting:', best_clf_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list of svm classifiers with differenet settings\n",
    "n_estim = 120\n",
    "_max_depth = 5\n",
    "_subsample = 0.8\n",
    "\n",
    "clfs_gb_depth = [\n",
    "    GradientBoostingClassifier(n_estimators=n_estim, max_depth=_max_depth, subsampl=_subsample), \n",
    "]\n",
    "\n",
    "# check for best settings: number of estimators\n",
    "tune_gb_scores_depth = []\n",
    "for index, clf in enumerate(clfs_gb_depth):\n",
    "    start = time.time()\n",
    "    print('Running cross-validation for:', index)\n",
    "    tune_gb_scores_depth.append(cross_val_score(clf, training_features_scale, labels, cv=10, scoring='f1'))\n",
    "    end = time.time()\n",
    "    print('Tuning parameters for Gradient Boosting, setting %d takes %.4f s' % (index, end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Logistic Regression takes 17.7848 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== training and prediction with Logistic Regression and scaled features ====== #\n",
    "best_clf_gb.fit(training_features_scale, labels)\n",
    "pred_gb = list(best_clf_gb.predict(testing_features_scale))\n",
    "pred_gb = zip(range(testing_size), pred_gb)\n",
    "\n",
    "end = time.time()\n",
    "print('Training with Gradient Boosting takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_submission('tuned_submission_gb_01.csv', pred_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score with SVM: 0.9678 (+/- 0.0005)\n"
     ]
    }
   ],
   "source": [
    "gb_scores = cross_val_score(best_gb_svm, training_features_scale, labels, cv=5, scoring='f1')\n",
    "print(\"F1-score with Gradient Boosting: %0.4f (+/- %0.4f)\" % (gb_scores.mean(), gb_scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Gradient Boosting takes 493.5955 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== Training and predicting with Gradient Boosting ====== #\n",
    "clf_gboost = GradientBoostingClassifier(\n",
    "    loss = 'deviance',\n",
    "    n_estimators = 120,\n",
    "    subsample = 0.8,\n",
    "    max_depth = 5\n",
    ")\n",
    "\n",
    "clf_gboost.fit(training_features, labels)\n",
    "pred_gboost = clf_gboost.predict(testing_features)\n",
    "pred_gboost = zip(range(testing_size), pred_gboost)\n",
    "\n",
    "end = time.time()\n",
    "print('Training with Gradient Boosting takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_submission('tuned_submission_gboost_02.csv', pred_gboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score with Gradient Boosting: 1.0000 (+/- 0.0000)\n"
     ]
    }
   ],
   "source": [
    "gd_scores = cross_val_score(clf_gboost, training_features, labels, cv=5, scoring='f1')\n",
    "print(\"F1-score with Gradient Boosting: %0.4f (+/- %0.4f)\" % (gd_scores.mean(), gd_scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Adaboost takes 356.5677 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== Training and predicting with Gradient Boosting ====== #\n",
    "clf_ada = GradientBoostingClassifier(\n",
    "    loss = 'exponential',\n",
    "    n_estimators = 120,\n",
    "    subsample = 0.8,\n",
    "    max_depth = 5\n",
    ")\n",
    "clf_ada.fit(training_features, labels)\n",
    "pred_ada = clf_ada.predict(testing_features)\n",
    "pred_ada = zip(range(testing_size), pred_ada)\n",
    "\n",
    "end = time.time()\n",
    "print('Training with Adaboost takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_submission('tuned_submission_ada_02.csv', pred_ada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-fd7392ac65dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mada_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf_ada\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'f1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"F1-score with AdaBoost: %0.4f (+/- %0.4f)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mada_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mada_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[1;32m    340\u001b[0m                                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                                 pre_dispatch=pre_dispatch)\n\u001b[0m\u001b[1;32m    343\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score)\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_train_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             return_times=True)\n\u001b[0;32m--> 206\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;31m# fit the boosting stages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m         n_stages = self._fit_stages(X, y, y_pred, sample_weight, random_state,\n\u001b[0;32m-> 1034\u001b[0;31m                                     begin_at_stage, monitor, X_idx_sorted)\n\u001b[0m\u001b[1;32m   1035\u001b[0m         \u001b[0;31m# change shape of arrays after fit (early-stopping or additional ests)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_stages\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[0;34m(self, X, y, y_pred, sample_weight, random_state, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1087\u001b[0m             y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,\n\u001b[1;32m   1088\u001b[0m                                      \u001b[0msample_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1089\u001b[0;31m                                      X_csc, X_csr)\n\u001b[0m\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m             \u001b[0;31m# track deviance (= loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[0;34m(self, i, X, y, y_pred, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m                 tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[0;32m--> 788\u001b[0;31m                          check_input=False, X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0;31m# update tree leaves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1122\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1125\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    360\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ada_scores = cross_val_score(clf_ada, training_features, labels, cv=5, scoring='f1')\n",
    "print(\"F1-score with AdaBoost: %0.4f (+/- %0.4f)\" % (ada_scores.mean(), ada_scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Neighbors (kNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validating to pick up the optimal number of neighbors takes 4485.4285 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# creating odd list of K for KNN\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_features, labels, test_size=0.35, random_state=42)\n",
    "\n",
    "myList = list(range(1,50))\n",
    "\n",
    "# subsetting just the odd ones\n",
    "neighbors = filter(lambda x: x % 2 != 0, myList)\n",
    "\n",
    "# empty list that will hold cv scores\n",
    "cv_scores = []\n",
    "\n",
    "# perform 10-fold cross validation\n",
    "for k in neighbors:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())\n",
    "    \n",
    "# changing to misclassification error\n",
    "MSE = [1 - x for x in cv_scores]\n",
    "\n",
    "# determining best k\n",
    "\n",
    "end = time.time()\n",
    "print('Cross-validating to pick up the optimal number of neighbors takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal number of neighbors is 11\n"
     ]
    }
   ],
   "source": [
    "neighbors = filter(lambda x: x % 2 != 0, myList)\n",
    "_neighbors = list(neighbors)\n",
    "optimal_k = _neighbors[np.argmin(MSE)]\n",
    "print('The optimal number of neighbors is %d' % optimal_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with k-nearest neighbors takes 44.9709 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== Training & predicting with k-Neareat Neighbors ====== #\n",
    "clf_knn = KNeighborsClassifier(\n",
    "    n_neighbors = optimal_k\n",
    ")\n",
    "clf_knn.fit(training_features, labels)\n",
    "pred_knn = clf_knn.predict(testing_features)\n",
    "pred_knn = zip(range(testing_size), pred_knn)\n",
    "\n",
    "end = time.time()\n",
    "print('Training with k-nearest neighbors takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_submission('submission_knn_01.csv', pred_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score with K-NearestNeighbors: 0.9588 (+/- 0.0008)\n"
     ]
    }
   ],
   "source": [
    "knn_scores = cross_val_score(clf_knn, training_features, labels, cv=5, scoring='f1')\n",
    "print(\"F1-score with K-NearestNeighbors: %0.4f (+/- %0.4f)\" % (knn_scores.mean(), knn_scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 'res_alloc' of importance 0.30824\n",
      "Feature 'katz_linkpred' of importance 0.19527\n",
      "Feature 'adamic_adar' of importance 0.18733\n",
      "Feature 'common_neighbors' of importance 0.11597\n",
      "Feature 'jaccard_coefficient' of importance 0.10649\n",
      "Feature 'cosine_sim' of importance 0.03725\n",
      "Feature 'pref_attach' of importance 0.01517\n",
      "Feature 'in_kcore' of importance 0.01285\n",
      "Feature 'max_pagerank' of importance 0.00584\n",
      "Feature 'max_degrees' of importance 0.00518\n",
      "Feature 'overlapping_title' of importance 0.00370\n",
      "Feature 'max_betweenness' of importance 0.00286\n",
      "Feature 'katz_index' of importance 0.00178\n",
      "Feature 'cosine_sim_w2v' of importance 0.00147\n",
      "Feature 'common_authors' of importance 0.00030\n",
      "Feature 'temporal_difference' of importance 0.00027\n",
      "Feature 'same_journal' of importance 0.00003\n"
     ]
    }
   ],
   "source": [
    "# ====== compute feature importance ====== #\n",
    "best_clf_rf.fit(np.nan_to_num(orig_training_features), labels)\n",
    "idx = np.argsort(-best_clf_rf.feature_importances_) # sort the indicator of feature important by decreasing order\n",
    "\n",
    "for i in idx:\n",
    "    print('Feature \\'%s\\' of importance %.5f' % (orig_features[i], best_clf_rf.feature_importances_[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
