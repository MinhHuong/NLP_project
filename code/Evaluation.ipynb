{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries and Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading libraries takes 1.3198 s\n"
     ]
    }
   ],
   "source": [
    "# import time to find consuming steps\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "# utility libraries\n",
    "import numpy as np\n",
    "import csv as csv\n",
    "from sklearn import preprocessing as pre\n",
    "from itertools import cycle\n",
    "from scipy import interp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# classifier for classification\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate, cross_val_score, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import recall_score, roc_curve, auc, average_precision_score, precision_recall_curve\n",
    "\n",
    "end = time.time()\n",
    "print('Loading libraries takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading dataset (training, testing, node information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '../data/' # path to the data\n",
    "path_submission = '../submission/' # path to submission files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training set & extracting labels takes 3.4442 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== read training data as str ====== #\n",
    "training = np.genfromtxt(path_data + 'training_set.txt', dtype=str)\n",
    "\n",
    "# ====== extract labels ====== #\n",
    "labels = training[:, 2].astype(int) # get the labels\n",
    "\n",
    "end = time.time()\n",
    "print('Reading training set & extracting labels takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training features takes 12.9700 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== read training features ====== #\n",
    "orig_training_features = np.genfromtxt(path_data + 'training_features.csv', delimiter=',', skip_header=1, dtype=float)\n",
    "\n",
    "end = time.time()\n",
    "print('Reading training features takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading testing features takes 0.7599 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== read testing features as str ====== #\n",
    "orig_testing_features = np.genfromtxt(path_data + 'testing_features.csv', delimiter=',', skip_header=1, dtype=float)\n",
    "\n",
    "end = time.time()\n",
    "print('Reading testing features takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features: (615512, 17)\n",
      "Labels: (615512,)\n",
      "Testing features: (32648, 17)\n"
     ]
    }
   ],
   "source": [
    "print('Training features:', orig_training_features.shape)\n",
    "print('Labels:', labels.shape)\n",
    "print('Testing features:', orig_testing_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Picking up some features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we might need to remove some features read from file. Here, we remove features by its index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_features = [\n",
    "    'temporal_difference', # 0\n",
    "    'common_authors', # 1\n",
    "    'same_journal', # 2\n",
    "    'cosine_sim', # 3\n",
    "    'overlapping_title', # 4\n",
    "    'max_degrees', # 5\n",
    "    'common_neighbors', # 6\n",
    "    'jaccard_coefficient', # 7\n",
    "    'max_pagerank', # 8\n",
    "    'max_betweenness', # 9\n",
    "    'in_kcore', # 10\n",
    "    'adamic_adar', # 11\n",
    "    'katz_index', # 12\n",
    "    'cosine_sim_w2v', # 13\n",
    "    'katz_linkpred', # 14\n",
    "    'pref_attach', # 15\n",
    "    'res_alloc' # 16\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features: (615512, 15)\n"
     ]
    }
   ],
   "source": [
    "# remove very features before training\n",
    "to_remove = [13, 14]\n",
    "training_features = np.nan_to_num(np.delete(orig_training_features, to_remove, 1))\n",
    "testing_features = np.nan_to_num(np.delete(orig_testing_features, to_remove, 1))\n",
    "features = np.delete(orig_features, to_remove)\n",
    "\n",
    "print('Training features:', training_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (400082, 15) (400082,)\n",
      "Testing set: (215430, 15) (215430,)\n"
     ]
    }
   ],
   "source": [
    "# splitting training and testing features\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_features, labels, test_size=0.35, random_state=42)\n",
    "\n",
    "print('Training set:', X_train.shape, y_train.shape)\n",
    "print('Testing set:', X_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling features\n",
    "X_train_scale = pre.scale(X_train)\n",
    "X_test_scale = pre.scale(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "clf_lg = LogisticRegression(penalty='l2', solver='newton-cg')\n",
    "\n",
    "# SVM\n",
    "clf_svm = svm.LinearSVC(penalty='l2', loss='hinge', C=1.0, fit_intercept=True)\n",
    "\n",
    "# random forest\n",
    "clf_rf = RandomForestClassifier(\n",
    "    max_features=0.3, \n",
    "    n_estimators=90\n",
    ")\n",
    "\n",
    "# neural netowork\n",
    "clf_nn = MLPClassifier(\n",
    "    hidden_layer_sizes = (50,60,70,40,50,30,20,10),\n",
    "    activation = 'relu',\n",
    "    solver = 'adam',\n",
    "    early_stopping = True\n",
    ")\n",
    "\n",
    "# gradient boosting\n",
    "clf_gboost = GradientBoostingClassifier(\n",
    "    loss = 'deviance',\n",
    "    n_estimators = 120,\n",
    "    subsample = 0.8,\n",
    "    max_depth = 5,\n",
    "    learning_rate = 0.1\n",
    ")\n",
    "\n",
    "# adaboost\n",
    "clf_ada = GradientBoostingClassifier(\n",
    "    loss = 'exponential',\n",
    "    n_estimators = 120,\n",
    "    subsample = 0.8,\n",
    "    max_depth = 5\n",
    ")\n",
    "\n",
    "# knn\n",
    "clf_knn = KNeighborsClassifier(\n",
    "    n_neighbors = 11\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on Gradient Boosting takes 199.0381 s\n",
      "Evaluation on AdaBoost takes 182.5642 s\n"
     ]
    }
   ],
   "source": [
    "# list of classifiers (classifier name, classifier, scale)\n",
    "clfs = [\n",
    "#     ('Logistic Regression', clf_lg, False),\n",
    "#     ('Logistic Regression', clf_lg, True),\n",
    "#     ('SVM', clf_svm, False),\n",
    "#     ('SVM', clf_svm, True),\n",
    "#     ('Random Forest', clf_rf, False),\n",
    "#     ('Random Forest', clf_rf, True),\n",
    "#     ('Neural Network', clf_nn, False),\n",
    "#     ('Neural Network', clf_nn, True),\n",
    "    ('Gradient Boosting', clf_gboost, False),\n",
    "#     ('Gradient Boosting', clf_gboost, True),\n",
    "    ('AdaBoost', clf_ada, False),\n",
    "#     ('AdaBoost', clf_ada, True),\n",
    "#     ('k-NN', clf_knn, False),\n",
    "#     ('k-NN', clf_knn, True)\n",
    "]\n",
    "result_score = [] # scores of each\n",
    "\n",
    "for clf in clfs:\n",
    "    start = time.time()\n",
    "    \n",
    "    _X_train = X_train_scale if clf[2] else X_train\n",
    "    _X_test = X_test_scale if clf[2] else X_test\n",
    "    clf[1].fit(_X_train, y_train)\n",
    "    result_score.append((clf[0],clf[1].score(_X_test, y_test), clf[2]))\n",
    "    \n",
    "    end = time.time()\n",
    "    print('Evaluation on %s takes %.4f s' % (clf[0], end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting (scaling: False): 0.96929\n",
      "AdaBoost (scaling: False): 0.96948\n"
     ]
    }
   ],
   "source": [
    "for res in result_score: # result of each classifier\n",
    "    print('%s (scaling: %r): %.5f' % (res[0], res[2], res[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
