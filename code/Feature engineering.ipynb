{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is solely used to train features for training and testing set. Our aim is to separate different steps in the workflow such that the code of each module, namely Feature engineering, Predictions, and Evaluation, do not mix up one to another. Having avoided to write a huge notebook that executes everything in one place, the separation brings a way to have a clean, proper code, thus making it easier for further development and maintenance.\n",
    "\n",
    "Feature engineering is one of the most important step among the entire process, since the data that we are given do not provided explicit features. Therefore, we have to come up with a set of features that may or may not contribute to the quality of prediction.\n",
    "\n",
    "The computation of several has proven to be time-consuming. Therefore, it is not practical to recompute everything from stratch every time we work on the project. That's why we decided to save computed features into files, so that we do not have to repeat the process of feature extraction.\n",
    "\n",
    "This notebook is proceeded as follows:\n",
    "- reading data sets,\n",
    "- building the citation graph,\n",
    "- computing features,\n",
    "- saving the features to files to be fed to the classifiers (in another notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries and Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we precise the necessary libraries and define several utility functions. The **execution time** is also noted down for all important steps. It is useful to bear in mind the amount of time needed to compute some specific features, that way, we must think of a way to avoid expensive computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing libraries and setting up parameters takes 0.0003 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# --- utility librairies --- #\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import csv\n",
    "\n",
    "# --- working with graph by using NetworKit --- #\n",
    "import networkit as nk\n",
    "\n",
    "# --- working with text --- #\n",
    "import nltk\n",
    "# nltk.download('stopwords') # if stopwords haven't been downloaded, please do\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "# --- plotting real cute stuffs --- #\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "end = time.time()\n",
    "print('Importing libraries and setting up parameters takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(nodes, edges):\n",
    "    '''\n",
    "    Build the graph from the set of nodes and edges.\n",
    "    NetworKit does not require labels for nodes, it only needs the index 0,1,2... of the nodes.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    nodes: set of nodes\n",
    "    edges: set of edges\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    the graph\n",
    "    '''\n",
    "    g = nk.Graph(len(nodes)) # adding nodes\n",
    "\n",
    "    for edge in edges:\n",
    "        if not g.hasEdge(edge[0], edge[1]): # avoid multiple edges\n",
    "            g.addEdge(edge[0], edge[1])\n",
    "            \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, dg_removal=True, sw_removal=True, stemming=True):\n",
    "    '''\n",
    "    Preprocess text: digit removal, stopword removal, stemming\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text: text on which preprocessing is applied\n",
    "    dg_removal: whether to apply digit removal or not\n",
    "    sw_removal: whether to apply stopword removal or not\n",
    "    stemming: whether to apply stemming or not\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    the text after preprocessing\n",
    "    '''\n",
    "    result = text\n",
    "    \n",
    "    sw = set(nltk.corpus.stopwords.words('english')) # set of stopwords\n",
    "    stemmer = nltk.stem.PorterStemmer() # stemmer\n",
    "    \n",
    "    if dg_removal:\n",
    "        result = re.sub('[0-9]', '', result)\n",
    "    \n",
    "    if sw_removal:\n",
    "        result = ' '.join([token for token in result.split() if token not in sw])\n",
    "        \n",
    "    if stemming:\n",
    "        result = ' '.join([stemmer.stem(token) for token in result.split()])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_feat_info(feat_name, set_name, arr):\n",
    "    '''\n",
    "    Print mean and standard deviation of a feature on training or testing set\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    feat_name: feature name\n",
    "    set_namme: 'training' | 'testing'\n",
    "    arr: the feature array\n",
    "    '''\n",
    "    print(\"%s: \" % set_name, arr[0:5])\n",
    "    print('--> Mean = %.3f, Std = %.3f, Non-null ratio: %.2f'\n",
    "          %(np.mean(arr), np.std(arr), float(np.count_nonzero(arr))/float(len(arr))))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the libraries and utility functions are properly set up, let's get work done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1. Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '../data/' # path to the data\n",
    "path_submission = '../submission/' # path to submission files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading node information takes 0.3328 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== read in node informations ====== #\n",
    "with open(path_data + 'node_information.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    node_info = list(reader)\n",
    "\n",
    "end = time.time()\n",
    "print('Reading node information takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training set takes 2.7601 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== read training data as str ====== #\n",
    "training = np.genfromtxt(path_data + 'training_set.txt', dtype=str)\n",
    "\n",
    "end = time.time()\n",
    "print('Reading training set takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading testing set takes 0.1431 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== read testing data as str ====== #\n",
    "testing = np.genfromtxt(path_data + 'testing_set.txt', dtype=str)\n",
    "\n",
    "end = time.time()\n",
    "print('Reading testing set takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Building the citation graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data loaded in, we should have enough information to construct the citation graph. It is observed that the number of edges is approximately half of the size of the training set i.e. the negative and positive class labels are **balanced**. Hence, it would later be easier to train classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the citation graph takes 172.3828 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== build the graph ====== #\n",
    "\n",
    "nodes = [element[0] for element in node_info] # create index list to be passed as nodes\n",
    "edges = [(nodes.index(element[0]), nodes.index(element[1])) for element in training if element[2] == '1']\n",
    "g = build_graph(nodes, edges)\n",
    "\n",
    "end = time.time()\n",
    "print('Building the citation graph takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vertices: 27770\n",
      "Number of edges (after multiple edges removal): 334690\n"
     ]
    }
   ],
   "source": [
    "# check for general information of the graph\n",
    "print('Number of vertices: %d' % g.numberOfNodes())\n",
    "print('Number of edges (after multiple edges removal): %d' % g.numberOfEdges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Computing features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of features is described as follows, and the computation rule is the same for both training and testing set.\n",
    "\n",
    "| Feature                | Explanation                                                        | Value       | Type        |\n",
    "|:----------------------:|:------------------------------------------------------------------:|:-----------:|:-----------:|\n",
    "| Common neighbors       | Number of common neighbors                                         | numercial   | topological |\n",
    "| Jaccard coefficient    | Link-based Jaccard coefficient                                     | numerical   | topological |\n",
    "| Adamic-Adar coefficient| Adamic-Adar coefficient between two nodes                          | numercial   | topological |\n",
    "| In the same k-core     | Whether both nodes/one of them/none of them are in the same k-core | categorical | topological |\n",
    "| Katz index             | (centrality package) By-pair maximum of Katz centrality value      | numerical   | topological |\n",
    "| Katz index             | (linkprediction package) The traditional approach to compute Katz  | numerical   | topological |\n",
    "| Degree                 | By-pair maximum of degree centrality values                        | numerical   | topological |\n",
    "| Betweenness centrality | By-pair maximum of betweenness centrality values                   | numerical   | topological |\n",
    "| PageRank score         | By-pair maximum of PageRank score                                  | numerical   | topological |\n",
    "| Preferential Attachment| Preferential attachment metric of a pair of nodes                  | numerical   | topological |\n",
    "| Resource Allocation    | Resource Allocation matrix of a pair of nodes                      | numerical   | topological |\n",
    "| Cosine similarity      | Cosine similarity between word vectors of titles + abstracts       | numerical   | semantic    |\n",
    "| Title overlap          | Number of overlapping words in title                               | numerical   | meta-data   |\n",
    "| Common authors         | The number of common authors between two articles                  | numerical   | meta-data   |\n",
    "| Temporal difference    | Difference in publication year (absolute value)                    | numerical   | meta-data   |\n",
    "| Same journal           | Whether two articles are published in the same journal             | binary      | meta-data   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the dictionary of (ID STRING - index INT) to accelerate access to a node's ID in the built graph\n",
    "ID = dict(zip(nodes, [nodes.index(n) for n in nodes]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to create such a mapping between an article's ID (e.g. '1001', '1002', '1003') and a node's ID in the graph (0, 1, 2), because it speeds up remarkably the computation time.\n",
    "\n",
    "Until now, we are ready to compute the set of features of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Topological features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 - Number of common neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_neighbors(ds, g):\n",
    "    '''\n",
    "    Feature: The number of common neighbors\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: dataset to compute feature from\n",
    "    g: the graph\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    common_neigh = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        \n",
    "        # collect the number of overlapping words in title\n",
    "        common_neigh[i] = len(\n",
    "            set(g.neighbors(ID[src]))\n",
    "            .intersection(set(g.neighbors(ID[dest])))\n",
    "        )\n",
    "        \n",
    "    return common_neigh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the number of common neighbors for training set takes 9.9154 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the average degree\n",
    "train_common_neigh = common_neighbors(training, g)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the number of common neighbors for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the number of common neighbors for testing set takes 0.6105 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the average degree\n",
    "test_common_neigh = common_neighbors(testing, g)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the number of common neighbors for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  [ 1. 20.  0.  0.  0.]\n",
      "--> Mean = 6.232, Std = 11.137, Non-null ratio: 0.53\n",
      "\n",
      "Testing:  [ 0. 24. 59. 21.  0.]\n",
      "--> Mean = 6.159, Std = 10.945, Non-null ratio: 0.53\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_feat_info('Common neighbors', 'Training', train_common_neigh)\n",
    "print_feat_info('Common neighbors', 'Testing', test_common_neigh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 - Jaccard coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_coeff(ds, g):\n",
    "    '''\n",
    "    Feature: Link-based Jaccard coefficient\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: dataset to compute feature from\n",
    "    g: the graph\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    coeff = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        \n",
    "        # collect the number of overlapping words in title\n",
    "        inters = len(\n",
    "            set(g.neighbors(ID[src]))\n",
    "            .intersection(set(g.neighbors(ID[dest])))\n",
    "        ) # intersection of neighbors\n",
    "        \n",
    "        union = len(\n",
    "            set(g.neighbors(ID[src]))\n",
    "            .union(set(g.neighbors(ID[dest])))\n",
    "        ) # union of neighbors\n",
    "        \n",
    "        coeff[i] = (float(inters)/float(union) if union != 0 else 0)\n",
    "        \n",
    "    return coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing link-based Jaccard coefficient for training set takes 21.0012 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the average degree\n",
    "train_jaccard_coeff = jaccard_coeff(training, g)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing link-based Jaccard coefficient for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing link-based Jaccard coefficient for testing set takes 1.0386 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the average degree\n",
    "test_jaccard_coeff = jaccard_coeff(testing, g)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing link-based Jaccard coefficient for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  [0.05882353 0.09708738 0.         0.         0.        ]\n",
      "--> Mean = 0.058, Std = 0.090, Non-null ratio: 0.53\n",
      "\n",
      "Testing:  [0.         0.07430341 0.06533776 0.22105263 0.        ]\n",
      "--> Mean = 0.061, Std = 0.097, Non-null ratio: 0.53\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_feat_info('Jaccard coefficient', 'Training', train_jaccard_coeff)\n",
    "print_feat_info('Jaccard coefficient', 'Testing', test_jaccard_coeff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 - Adamic-Adar coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adamic_adar_coeff(ds, g):\n",
    "    '''\n",
    "    Feature: Adamic-Adar coefficient\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: the dataset\n",
    "    g: graph\n",
    "    '''\n",
    "    \n",
    "    size = len(ds)\n",
    "    aa_coeff = np.zeros(size)\n",
    "    aa_index = nk.linkprediction.AdamicAdarIndex(g)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        \n",
    "        aa_coeff[i] = aa_index.run(ID[src], ID[dest])\n",
    "        \n",
    "    return aa_coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Adamic-Adar coefficient feature for training set takes 6.0754 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the adamic-adar coefficient\n",
    "train_aa_coeff = adamic_adar_coeff(training, g)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing Adamic-Adar coefficient feature for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Adamic-Adar coefficient feature for testing set takes 0.3486 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the adamic-adar coefficient\n",
    "test_aa_coeff = adamic_adar_coeff(testing, g)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing Adamic-Adar coefficient feature for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  [0.51389834 4.32036615 0.         0.         0.        ]\n",
      "--> Mean = 1.513, Std = 2.740, Non-null ratio: 0.53\n",
      "\n",
      "Testing:  [ 0.          5.37797275 15.05361173  4.89942438  0.        ]\n",
      "--> Mean = 1.498, Std = 2.692, Non-null ratio: 0.53\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_feat_info('Adamic-Adar', 'Training', train_aa_coeff)\n",
    "print_feat_info('Adamic-Adar', 'Testing', test_aa_coeff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4 - In k-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core decomposition of the graph takes 0.1697 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "core_decomp = nk.community.CoreDecomposition(g, storeNodeOrder=True)\n",
    "core_decomp.run()\n",
    "cover_g = core_decomp.getCover()\n",
    "order = 15 # important parameters\n",
    "\n",
    "end = time.time()\n",
    "print('Core decomposition of the graph takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9647 nodes that belong in the 15-core decomposition of this graph\n"
     ]
    }
   ],
   "source": [
    "print('There are %d nodes that belong in the %d-core decomposition of this graph' \n",
    "      % (len(cover_g.getMembers(order)), order))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_kcore(ds, kcore):\n",
    "    '''\n",
    "    Compute feature: whether a pair of nodes is found in the same k-core graph\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: dataset\n",
    "    kcore: the k-core graph after decomposition as a set of nodes index (ranged from 0 to 27,770)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A numpy array of ordinal values: \n",
    "        - 0 if both nodes are not in the kcores, \n",
    "        - 0.5 if one of them is in the kcores, \n",
    "        - 1 of they are both in the k-core\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    same_kcore = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        \n",
    "        # compute whether two nodes are in the given kcore | one of them is in the kcore | none of them\n",
    "        index_src = ID[src] # index of src\n",
    "        index_dest = ID[dest] # index of dest\n",
    "        \n",
    "        if index_src in kcore and index_dest in kcore:\n",
    "            result = 1.0\n",
    "        elif index_src not in kcore and index_dest not in kcore:\n",
    "            result = 0.0\n",
    "        else:\n",
    "            result = 0.5\n",
    "            \n",
    "        same_kcore[i] = result\n",
    "        \n",
    "    return same_kcore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the in k-core feature for training set takes 1.7808 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the position of two nodes wrt k-core\n",
    "train_in_kcore = in_kcore(training, cover_g.getMembers(order))\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the in k-core feature for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the in k-core feature for testing set takes 0.1182 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the position of two nodes wrt k-core\n",
    "test_in_kcore = in_kcore(testing, cover_g.getMembers(order))\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the in k-core feature for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  [0.  1.  0.  0.5 0.5]\n",
      "--> Mean = 0.564, Std = 0.420, Non-null ratio: 0.70\n",
      "\n",
      "Testing:  [1.  1.  1.  1.  0.5]\n",
      "--> Mean = 0.555, Std = 0.418, Non-null ratio: 0.70\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_feat_info('In k-core', 'Training', train_in_kcore)\n",
    "print_feat_info('In k-core', 'Testing', test_in_kcore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.5 - Katz index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Katz measure is one of the most curious feature to consider from the dataset, since it causes overfitting to every classifier, even to Naive Bayes that is often proved to be \"immune\" to overfitting due to its simple assumption of independence.\n",
    "\n",
    "`Networkit` oddly provides computation of Katz measure in two different packages, namely `centrality` and `linkprediction`. Due to the lack of detailed documentation on their website, we assume each has different behaviors, since katz index computed with `centrality` package is relatively fast (within a few seconds) meanwhile the same measure obtained by `linkprediction` package takes about 9 hours to complete. It is the latter one that causes overfitting, whereas the former one does not cause much trouble (but does not have significant contribution to the accuracy score either)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (A) *centrality* package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing katz centrality (with centrality package) takes 0.1100 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== create a Katz Centrality object and run it ====== #\n",
    "katz = nk.centrality.KatzCentrality(g)\n",
    "katz.run()\n",
    "\n",
    "end = time.time()\n",
    "print('Computing katz centrality (with centrality package) takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_katz(ds, katz_scores):\n",
    "    '''\n",
    "    By-pair maximumKatz index between a pair of nodes (using centrality package and get the maximum)\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    katz_result = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1]\n",
    "        _katz = max(katz_scores[ID[src]], katz_scores[ID[dest]])\n",
    "        katz_result[i] = np.log(_katz) if _katz != 0.0 else 0.0\n",
    "        \n",
    "    return katz_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Katz max feature for training set takes 1.7918 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the katz index for training set\n",
    "train_katz = max_katz(training, katz.scores())\n",
    "\n",
    "end = time.time()\n",
    "print('Computing Katz max feature for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Katz average feature for testing set takes 0.1408 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the katz index for testing set\n",
    "test_katz = max_katz(testing, katz.scores())\n",
    "\n",
    "end = time.time()\n",
    "print('Computing Katz average feature for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  [-5.5804487  -4.2164855  -5.64652461 -5.25171866 -5.36014155]\n",
      "--> Mean = -4.878, Std = 0.685, Non-null ratio: 1.00\n",
      "\n",
      "Testing:  [-4.69918791 -3.80632847 -2.88068326 -4.78584888 -4.22347698]\n",
      "--> Mean = -4.881, Std = 0.693, Non-null ratio: 1.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_feat_info('Katz with centrality package', 'Training', train_katz)\n",
    "print_feat_info('Katz with centrality package', 'Testing', test_katz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (B) *linkprediction* package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the computation of Katz centrality measure with `linkprediction` package is highly time-consuming, we have decided to avoid any recomputation. Instead, we ran it only once and saved the result into files (please refer to `katz_training.txt` and `katz_testing.txt`). Then, we only need to read back the data from file to reconstruct the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructing katz index for traing set takes 2.6767s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== read katz for training set from file ====== #\n",
    "_train_katz_linkpred = np.genfromtxt(path_data + 'katz_training.txt', dtype=float)\n",
    "train_katz_linkpred = [np.log(value) if value != 0 else 0 for value in _train_katz_linkpred]\n",
    "\n",
    "end = time.time()\n",
    "print('Reconstructing katz index for traing set takes %.4fs' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructing katz index for traing set takes 0.1655s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== read katz for training set from file ====== #\n",
    "_test_katz_linkpred = np.genfromtxt(path_data + 'katz_testing.txt', dtype=float)\n",
    "test_katz_linkpred = [np.log(value) if value != 0 else 0 for value in _test_katz_linkpred]\n",
    "\n",
    "end = time.time()\n",
    "print('Reconstructing katz index for traing set takes %.4fs' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  [-5.293029862567551, -5.199655114063047, 0, -13.487961947295274, -15.799096614000588]\n",
      "--> Mean = -10.194, Std = 7.904, Non-null ratio: 0.92\n",
      "\n",
      "Testing:  [-15.072323905681971, -7.365925687758105, -6.472026643304163, -7.537850990784671, -13.279108565893594]\n",
      "--> Mean = -12.131, Std = 7.010, Non-null ratio: 0.91\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_feat_info('Katz link prediction', 'Training', train_katz_linkpred)\n",
    "print_feat_info('Katz link prediction', 'Testing', test_katz_linkpred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.6 - By-pair maximum of degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_degrees(ds, g):\n",
    "    '''\n",
    "    Feature: Maximum degrees among 2 nodes\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: dataset to compute feature from\n",
    "    g: the graph\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    max_degree = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        \n",
    "        # collect the number of overlapping words in title\n",
    "        src_deg = g.degree(ID[src])\n",
    "        dest_deg = g.degree(ID[dest])\n",
    "        max_degree[i] = max(src_deg, dest_deg)\n",
    "        \n",
    "    return max_degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the by-pair maximum degree for training set takes 1.7575 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the average degree\n",
    "train_degrees = max_degrees(training, g)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the by-pair maximum degree for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the by-pair maximum degree for testing set takes 0.1244 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the average degree\n",
    "test_degrees = max_degrees(testing, g)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the by-pair maximum degree for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  [ 12. 147.   5.  20.  24.]\n",
      "--> Mean = 106.541, Std = 239.810, Non-null ratio: 1.00\n",
      "\n",
      "Testing:  [ 59. 302. 739.  65. 150.]\n",
      "--> Mean = 107.564, Std = 243.680, Non-null ratio: 1.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_feat_info('By-pair max degree', 'Training', train_degrees)\n",
    "print_feat_info('By-pair max degree', 'Testing', test_degrees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.7 - By-pair maximum of betweenness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Networkit` offers various choices for the computation of betweenness centrality, such as traditional, estimate and approximate approach. We have decided to come up with the traditional approach, that is to consider all-pair shortest paths. It is acceptable in our case because the citation is not too large (27700 nodes are a number smaller than what a social network has in the present day). The computation should be completed within 10 minutes or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute betweenness centrality of every node in the graph takes 460.1832 s\n"
     ]
    }
   ],
   "source": [
    "# ====== compute betweenness centrality ====== #\n",
    "start = time.time()\n",
    "\n",
    "# use the traditional approach of betweeness computation\n",
    "btwn = nk.centrality.Betweenness(g)\n",
    "btwn.run()\n",
    "\n",
    "end = time.time()\n",
    "print('Compute betweenness centrality of every node in the graph takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_betweeness(ds, btwn):\n",
    "    '''\n",
    "    Compute feature: by-pair maximum of betweenness centrality\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: dataset to compute feature from\n",
    "    g: the graph\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A numpy array of numerical values\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    max_btw = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        \n",
    "        # collect the average of betweenness centrality of 2 nodes\n",
    "        _max = max(btwn[ID[src]], btwn[ID[dest]])\n",
    "        max_btw[i] = np.log(_max) if _max != 0 else 0.0\n",
    "        \n",
    "    return max_btw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the average betweenness for training set takes 1.7032 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the average degree\n",
    "train_btwn = max_betweeness(training, btwn.scores())\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the average betweenness for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the average betweenness for testing set takes 0.1040 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the average degree\n",
    "test_btwn = max_betweeness(testing, btwn.scores())\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the average betweenness for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  [10.5093708  11.39325368  9.53921731  8.04019582  9.38724288]\n",
      "--> Mean = 11.460, Std = 2.127, Non-null ratio: 1.00\n",
      "\n",
      "Testing:  [12.58866497 14.77807021 15.9955066  11.20495121 13.16245853]\n",
      "--> Mean = 11.440, Std = 2.155, Non-null ratio: 1.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_feat_info('Betweenness', 'Training', train_btwn)\n",
    "print_feat_info('Betweenness', 'Testing', test_btwn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.8 - By-pair maximum of PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the PageRank index of the graph takes 0.2050 s\n"
     ]
    }
   ],
   "source": [
    "# ====== compute PageRank index ====== #\n",
    "start = time.time()\n",
    "\n",
    "page_rank_g = nk.centrality.PageRank(g)\n",
    "page_rank_g.run()\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the PageRank index of the graph takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pagerank(ds, pr):\n",
    "    '''\n",
    "    Compute feature: average of pagerank\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: dataset to compute feature from\n",
    "    pr: PageRank centrality object\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    max_pr = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        \n",
    "        # collect the average of betweenness centrality of 2 nodes\n",
    "        # log to \"dampen\" too small values\n",
    "        _max = max(pr[ID[src]], pr[ID[dest]])\n",
    "        max_pr[i] = np.log(_max) if _max != 0.0 else 0.0\n",
    "        \n",
    "    return max_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the by-pair maximum page rank for training set takes 2.5733 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the average pagerank on training set\n",
    "train_pagerank = max_pagerank(training, page_rank_g.scores())\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the by-pair maximum page rank for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the by-pair maximum page rank for testing set takes 0.1430 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the average pagerank\n",
    "test_pagerank = max_pagerank(testing, page_rank_g.scores())\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the by-pair maximum page rank for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  [-10.4952174   -9.04507102 -11.03082025 -10.67156463 -10.4743869 ]\n",
      "--> Mean = -9.655, Std = 0.869, Non-null ratio: 1.00\n",
      "\n",
      "Testing:  [-9.71123221 -8.05014677 -7.28060674 -9.78108665 -8.84688464]\n",
      "--> Mean = -9.667, Std = 0.884, Non-null ratio: 1.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_feat_info('By-pair max PageRank', 'Training', train_pagerank)\n",
    "print_feat_info('By-pair max PageRank', 'Testing', test_pagerank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.9 - Preferential Attachment index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pref_attach(ds, pa):\n",
    "    '''\n",
    "    Feature: Preferential Attachment between 2 nodes\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: dataset\n",
    "    pa: prefenrential attachment object\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    pa_result = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        _pa = pa.run(ID[src], ID[dest])\n",
    "        pa_result[i] = np.log(_pa) if _pa != 0 else 0.0\n",
    "        \n",
    "    return pa_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_object = nk.linkprediction.PreferentialAttachmentIndex(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Preferential Attachment feature for training set takes 2.2458 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the Preferential Attachment index\n",
    "train_pref_attach = pref_attach(training, pa_object)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing Preferential Attachment feature for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Preferential Attachment feature for testing set takes 0.1471 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the Preferential Attachment index\n",
    "test_pref_attach = pref_attach(testing, pa_object)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing Preferential Attachment feature for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  [4.27666612 9.35988044 1.60943791 5.6347896  5.12396398]\n",
      "--> Mean = 6.442, Std = 2.200, Non-null ratio: 1.00\n",
      "\n",
      "Testing:  [ 6.9679092   9.51708951 12.01246969  8.1062129   6.95654544]\n",
      "--> Mean = 6.379, Std = 2.223, Non-null ratio: 0.99\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_feat_info('Preferential Attachment', 'Training', train_pref_attach)\n",
    "print_feat_info('Preferential Attachment', 'Testing', test_pref_attach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.10 - Resource Allocation measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_allocation(ds, ra):\n",
    "    '''\n",
    "    Feature: ResourceAllocation index\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: dataset\n",
    "    ra: Resource Allocation object\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    ra_result = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        _ra = ra.run(ID[src], ID[dest])\n",
    "        ra_result[i] = _ra\n",
    "        \n",
    "    return ra_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "ra_object = nk.linkprediction.ResourceAllocationIndex(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Resource Allocation feature for training set takes 6.0396 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute Resource Allocation \n",
    "train_res_alloc = res_allocation(training, ra_object)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing Resource Allocation feature for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Resource Allocation feature for testing set takes 0.3480 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute Resource Allocation \n",
    "test_res_alloc = res_allocation(testing, ra_object)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing Resource Allocation feature for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  [0.14285714 0.22640079 0.         0.         0.        ]\n",
      "--> Mean = 0.125, Std = 0.247, Non-null ratio: 0.53\n",
      "\n",
      "Testing:  [0.         0.31153472 1.34259427 0.29841899 0.        ]\n",
      "--> Mean = 0.125, Std = 0.243, Non-null ratio: 0.53\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_feat_info('Resouce Allocation', 'Training', train_res_alloc)\n",
    "print_feat_info('Resouce Allocation', 'Testing', test_res_alloc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Semantic features: Cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before computing semantic features, it is mandatory to build a corpus of all texts extracted from the given data. We define as corpus as the collection of all articles' title and abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the corpus with preprocessing on words takes 50.9963 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== corpus is the set of titles + abstracts, apply preprocessing to each article ======#\n",
    "\n",
    "# nltk.download('stopwords') # uncomment if haven't downloaded stopwords\n",
    "corpus = [preprocess(element[2] + ' ' + element[5], dg_removal=True, sw_removal=True, stemming=True) \n",
    "          for element in node_info]\n",
    "\n",
    "end = time.time()\n",
    "print('Building the corpus with preprocessing on words takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim_text(ds, vectors, is_w2v):\n",
    "    '''\n",
    "    Compute feature: cosine similarity in title and abstract, cosine similarity on either TF-IDF or word2vec\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: dataset to compute feature from\n",
    "    vectors: vectors of word embeddings or TF-IDF\n",
    "    is_w2v: True if vectors contain word embeddings, False in case of TF-IDF\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A numpy array of cosine values\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    cosines = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        \n",
    "        # collect the cosine similarity\n",
    "        src_vect, dest_vect = vectors[ID[src]], vectors[ID[dest]] # get the corresponding vector in TD-IDF matrix\n",
    "\n",
    "        # compute cosine similarity\n",
    "        cos = 1 - sc.spatial.distance.cosine(src_vect, dest_vect) if is_w2v else cosine_similarity(src_vect, dest_vect)\n",
    "        \n",
    "        cosines[i] = cos\n",
    "        \n",
    "    return cosines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 - TD-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix of shape: (27770, 17080)\n",
      "Building TF-IDF matrix takes 1.8592 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== fit TF-IDF ====== #\n",
    "vectorizer = TfidfVectorizer(stop_words='english') # create a TF-IDF vectorizer\n",
    "tfidf = vectorizer.fit_transform(corpus) # TD-IDF matrix of the entire corpus (set of abstracts)\n",
    "print('TF-IDF matrix of shape:', tfidf.shape)\n",
    "\n",
    "end = time.time()\n",
    "print('Building TF-IDF matrix takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing cosine similarity for training set takes 345.2234 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the cosine similarity\n",
    "train_cos_tfidf = cosine_sim_text(training, tfidf, False)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing cosine similarity for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing cosine similarity for testing set takes 17.6878 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the cosine similarity\n",
    "test_cos_tfidf = cosine_sim_text(testing, tfidf, False)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing cosine similarity for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  [0.19996622 0.06436945 0.02053711 0.05937844 0.09852643]\n",
      "--> Mean = 0.114, Std = 0.116, Non-null ratio: 0.97\n",
      "\n",
      "Testing:  [0.11804009 0.30786265 0.20753805 0.16112407 0.31824453]\n",
      "--> Mean = 0.114, Std = 0.116, Non-null ratio: 0.97\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_feat_info('Cosine similarity with TF-IDF', 'Training', train_cos_tfidf)\n",
    "print_feat_info('Cosine similarity with TF-IDF', 'Testing', test_cos_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 - word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.utils_any2vec:loading projection weights from ../data/GoogleNews-vectors-negative300.bin.gz\n",
      "INFO:gensim.models.utils_any2vec:loaded (3000000, 300) matrix from ../data/GoogleNews-vectors-negative300.bin.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word2vec of google takes 118.7759 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== reading google vector ====== #\n",
    "google_vecs = KeyedVectors.load_word2vec_format(path_data + 'GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "\n",
    "end = time.time()\n",
    "print('Loading word2vec of google takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building documents for word2vec training takes 10.3649 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# building documents for word2vec training\n",
    "train_tag = []\n",
    "total = len(corpus)\n",
    "processed = 0\n",
    "i = 0\n",
    "#nltk.download('punkt') # uncomment if package 'punkt' not already downloaded\n",
    "for x in corpus:\n",
    "    words = []\n",
    "    sentences = sent_tokenize(x)\n",
    "    for s in sentences:\n",
    "        words.extend(word_tokenize(s)) \n",
    "    doc = words\n",
    "    i = i+1\n",
    "    train_tag.append(doc)\n",
    "\n",
    "end = time.time()\n",
    "print('Building documents for word2vec training takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec(x_data,vector):\n",
    "    print (\"Loading GoogleNews-vectors-negative300.bin\")\n",
    "    google_vecs = vector\n",
    "    print (\"GoogleNews-vectors-negative300.bin loaded\")\n",
    "    \n",
    "    print (\"Averaging Word Embeddings...\")\n",
    "    x_data_embeddings = []\n",
    "    total = len(x_data)\n",
    "    for tagged_plot in x_data:\n",
    "        count = 0  \n",
    "        doc_vector = np.zeros(300)\n",
    "        for sentence in tagged_plot:\n",
    "            try:\n",
    "                doc_vector += google_vecs[sentence]\n",
    "            except KeyError:\n",
    "                doc_vector += [0.0]*300\n",
    "                continue\n",
    "\n",
    "        x_data_embeddings.append(doc_vector)\n",
    "            \n",
    "    return np.array(x_data_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GoogleNews-vectors-negative300.bin\n",
      "GoogleNews-vectors-negative300.bin loaded\n",
      "Averaging Word Embeddings...\n",
      "Word embeddings of shape: (27770, 300)\n",
      "Embedding words takes 12.1084 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# get word embessings\n",
    "x_embeddings = word2vec(train_tag, google_vecs)\n",
    "print('Word embeddings of shape:', x_embeddings.shape)\n",
    "\n",
    "end = time.time()\n",
    "print('Embedding words takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huong/.local/lib/python3.5/site-packages/scipy/spatial/distance.py:644: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing cosine similarity using word embedding for training features takes 19.9758 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== compute cosine similarity with word embeddings in training set ====== #\n",
    "train_cos_w2v = np.nan_to_num(cosine_sim_text(training, x_embeddings, True))\n",
    "\n",
    "end = time.time()\n",
    "print('Computing cosine similarity using word embedding for training features takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huong/.local/lib/python3.5/site-packages/scipy/spatial/distance.py:644: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing cosine similarity using word embedding for testing features takes 1.0727 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ======compute cosine similarity with word embeddings in testing set ====== #\n",
    "test_cos_w2v = np.nan_to_num(cosine_sim_text(testing, x_embeddings, True))\n",
    "\n",
    "end = time.time()\n",
    "print('Computing cosine similarity using word embedding for testing features takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  [0.76839412 0.64235938 0.76174991 0.73752149 0.7232033 ]\n",
      "--> Mean = 0.691, Std = 0.097, Non-null ratio: 1.00\n",
      "\n",
      "Testing:  [0.72750916 0.69251008 0.61316547 0.60472135 0.37674628]\n",
      "--> Mean = 0.691, Std = 0.097, Non-null ratio: 1.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_feat_info('Cosine similarity with word2vec', 'Training', train_cos_w2v)\n",
    "print_feat_info('Cosine similarity with word2vec', 'Testing', test_cos_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building TaggedDocuments for train\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "train_tag = []\n",
    "print (\"Building TaggedDocuments for train\")\n",
    "labels = [counter[0] for counter in node_info]\n",
    "total = len(corpus)\n",
    "processed = 0\n",
    "i = 0\n",
    "for x in corpus:\n",
    "    words = []\n",
    "    sentences = sent_tokenize(x)\n",
    "    for s in sentences:\n",
    "        words.extend(word_tokenize(s)) \n",
    "    doc = TaggedDocument(words, labels[i])\n",
    "    i = i+1\n",
    "    train_tag.append(doc)\n",
    "print (\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huong/.local/lib/python3.5/site-packages/gensim/models/doc2vec.py:366: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "INFO:gensim.models.doc2vec:collecting all words and their counts\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.doc2vec:PROGRESS: at example #10000, processed 665191 words (2394513/s), 14582 word types, 10 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #20000, processed 1289024 words (2356692/s), 21649 word types, 10 tags\n",
      "INFO:gensim.models.doc2vec:collected 25455 word types and 10 unique tags from a corpus of 27770 examples and 1783670 words\n",
      "INFO:gensim.models.word2vec:Loading a fresh vocabulary\n",
      "INFO:gensim.models.word2vec:min_count=1 retains 25455 unique words (100% of original 25455, drops 0)\n",
      "INFO:gensim.models.word2vec:min_count=1 leaves 1783670 word corpus (100% of original 1783670, drops 0)\n",
      "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 25455 items\n",
      "INFO:gensim.models.word2vec:sample=0.0001 downsamples 713 most-common words\n",
      "INFO:gensim.models.word2vec:downsampling leaves estimated 889654 word corpus (49.9% of prior 1783670)\n",
      "INFO:gensim.models.base_any2vec:estimated required memory for 25455 words and 300 dimensions: 73833500 bytes\n",
      "INFO:gensim.models.word2vec:resetting layer weights\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \n",
      "INFO:gensim.models.base_any2vec:training model with 2 workers on 25455 vocabulary and 300 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 23.31% examples, 244825 words/s, in_qsize 4, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 52.21% examples, 276827 words/s, in_qsize 3, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 79.91% examples, 280979 words/s, in_qsize 3, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.base_any2vec:EPOCH - 1 : training on 1783670 raw words (1067839 effective words) took 3.7s, 291197 effective words/s\n",
      "INFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 26.44% examples, 276349 words/s, in_qsize 3, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 53.39% examples, 282332 words/s, in_qsize 3, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 82.78% examples, 289056 words/s, in_qsize 3, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.base_any2vec:EPOCH - 2 : training on 1783670 raw words (1069053 effective words) took 3.8s, 281970 effective words/s\n",
      "INFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 30.72% examples, 330270 words/s, in_qsize 3, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 60.27% examples, 319970 words/s, in_qsize 3, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 94.50% examples, 332651 words/s, in_qsize 3, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.base_any2vec:EPOCH - 3 : training on 1783670 raw words (1068146 effective words) took 3.2s, 331716 effective words/s\n",
      "INFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 31.82% examples, 337881 words/s, in_qsize 3, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 65.37% examples, 346608 words/s, in_qsize 3, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 97.70% examples, 345585 words/s, in_qsize 4, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.base_any2vec:EPOCH - 4 : training on 1783670 raw words (1068379 effective words) took 3.1s, 343830 effective words/s\n",
      "INFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 32.34% examples, 348572 words/s, in_qsize 4, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 58.53% examples, 313182 words/s, in_qsize 4, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 90.01% examples, 317868 words/s, in_qsize 3, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.base_any2vec:EPOCH - 5 : training on 1783670 raw words (1068672 effective words) took 3.3s, 319328 effective words/s\n",
      "INFO:gensim.models.base_any2vec:training on a 8918350 raw words (5342089 effective words) took 17.2s, 311239 effective words/s\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  if __name__ == '__main__':\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "INFO:gensim.models.base_any2vec:training model with 2 workers on 25455 vocabulary and 300 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "INFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 31.26% examples, 336145 words/s, in_qsize 4, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 64.23% examples, 340706 words/s, in_qsize 3, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 98.27% examples, 343900 words/s, in_qsize 4, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.base_any2vec:EPOCH - 1 : training on 1783670 raw words (1068038 effective words) took 3.1s, 343688 effective words/s\n",
      "INFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 32.89% examples, 353400 words/s, in_qsize 4, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 67.02% examples, 354732 words/s, in_qsize 4, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.base_any2vec:EPOCH - 2 : training on 1783670 raw words (1069320 effective words) took 3.0s, 353940 effective words/s\n",
      "INFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 31.82% examples, 342781 words/s, in_qsize 3, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 65.37% examples, 348669 words/s, in_qsize 3, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 98.88% examples, 350789 words/s, in_qsize 3, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.base_any2vec:EPOCH - 3 : training on 1783670 raw words (1068128 effective words) took 3.1s, 349778 effective words/s\n",
      "INFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 32.34% examples, 348276 words/s, in_qsize 4, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 66.45% examples, 354419 words/s, in_qsize 3, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 99.41% examples, 352504 words/s, in_qsize 2, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.base_any2vec:EPOCH - 4 : training on 1783670 raw words (1068494 effective words) took 3.0s, 352389 effective words/s\n",
      "INFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 32.34% examples, 346324 words/s, in_qsize 4, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 65.37% examples, 345411 words/s, in_qsize 3, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 99.41% examples, 349049 words/s, in_qsize 2, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.base_any2vec:EPOCH - 5 : training on 1783670 raw words (1067771 effective words) took 3.0s, 350167 effective words/s\n",
      "INFO:gensim.models.base_any2vec:training on a 8918350 raw words (5341751 effective words) took 15.3s, 349407 effective words/s\n"
     ]
    }
   ],
   "source": [
    "model = Doc2Vec(min_count=1, window=10, size=300, sample=1e-4, negative=5, workers=2)\n",
    "print (\"Building Vocabulary\")\n",
    "model.build_vocab(train_tag)\n",
    "for epoch in range(1):\n",
    "    print (\"Training epoch %s\" % epoch)\n",
    "    model.train(train_tag, total_examples=model.corpus_count , epochs=model.iter)\n",
    "    model.alpha -= 0.002  # decrease the learning rate\n",
    "    model.min_alpha = model.alpha  # fix the learning rate, no decay\n",
    "    model.train(train_tag, total_examples=model.corpus_count, epochs=model.iter)\n",
    "    \n",
    "x_train = []\n",
    "for doc_id in range(len(train_tag)):\n",
    "    inferred_vector = model.infer_vector(train_tag[doc_id].words)\n",
    "    x_train.append(inferred_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing cosine similarity using word embedding for training features takes 27.5655 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== compute cosine similarity with word embeddings in training set ====== #\n",
    "train_cos_d2v = np.nan_to_num(cosine_sim_text(training, x_train, True))\n",
    "\n",
    "end = time.time()\n",
    "print('Computing cosine similarity using word embedding for training features takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing cosine similarity using word embedding for training features takes 1.5624 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== compute cosine similarity with word embeddings in training set ====== #\n",
    "test_cos_d2v = np.nan_to_num(cosine_sim_text(testing, x_train, True))\n",
    "\n",
    "end = time.time()\n",
    "print('Computing cosine similarity using word embedding for training features takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  [-0.18935569  0.84578013  0.42042828  0.58582169  0.64102918]\n",
      "--> Mean = 0.482, Std = 0.193, Non-null ratio: 1.00\n",
      "\n",
      "Testing:  [ 0.5447917   0.39170963 -0.05685651  0.49049193  0.64354986]\n",
      "--> Mean = 0.483, Std = 0.193, Non-null ratio: 1.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_feat_info('Cosine similarity with doc2vec', 'Training', train_cos_d2v)\n",
    "print_feat_info('Cosine similarity with doc2vec', 'Testing', test_cos_d2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Meta-data features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 - Number of overlapping words in titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap_title(ds):\n",
    "    '''\n",
    "    Compute feature: number of overlapping words in the title\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: dataset to compute feature from\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A numpy array of numerical values\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    overlap_title = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        src_info, dest_info = node_info[ID[src]], node_info[ID[dest]] # get the associated node information by index\n",
    "        \n",
    "        # collect the number of overlapping words in title\n",
    "        src_title, dest_title = preprocess(src_info[2]).split(), preprocess(dest_info[2]).split()\n",
    "        overlap_title[i] = len(\n",
    "            set(src_title)\n",
    "            .intersection(set(dest_title))\n",
    "        )\n",
    "        \n",
    "    return overlap_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing number of overlapping words in title for training set takes 659.3079 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the number of overlapping words in title\n",
    "train_overlap_title = overlap_title(training)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing number of overlapping words in title for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing number of overlapping words in title for testing set takes 31.8326 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the number of overlapping words in title\n",
    "test_overlap_title = overlap_title(testing)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing number of overlapping words in title for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  [2. 1. 0. 0. 0.]\n",
      "--> Mean = 0.495, Std = 0.862, Non-null ratio: 0.32\n",
      "\n",
      "Testing:  [0. 2. 1. 1. 0.]\n",
      "--> Mean = 0.491, Std = 0.865, Non-null ratio: 0.32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_feat_info('Number of overlapping words in title', 'Training', train_overlap_title)\n",
    "print_feat_info('Number of overlapping words in title', 'Testing', test_overlap_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 - Number of common authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_authors(ds):\n",
    "    '''\n",
    "    Compute feature: number of common authors\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: dataset to compute feature from\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A numpy array\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    common_auth = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        src_info, dest_info = node_info[ID[src]], node_info[ID[dest]] # get the associated node information by index\n",
    "        \n",
    "        # compute the difference in publication year in absolute value (because we don't know which one cites the other)\n",
    "        common_auth[i] = len(\n",
    "            set(src_info[3].split(','))\n",
    "            .intersection(set(dest_info[3].split(',')))\n",
    "        )\n",
    "        \n",
    "    return common_auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the number of common authors for training set takes 2.7053 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the temporal difference\n",
    "train_common_auth = common_authors(training)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the number of common authors for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the number of common authors for testing set takes 0.1831 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the temporal difference\n",
    "test_common_auth = common_authors(testing)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the number of common authors for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  [0. 0. 0. 0. 0.]\n",
      "--> Mean = 0.083, Std = 0.357, Non-null ratio: 0.06\n",
      "\n",
      "Testing:  [0. 0. 0. 0. 0.]\n",
      "--> Mean = 0.082, Std = 0.351, Non-null ratio: 0.06\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_feat_info('Common authors', 'Training', train_common_auth)\n",
    "print_feat_info('Common authors', 'Testing', test_common_auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 - Temporal difference in publication year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_difference(ds):\n",
    "    '''\n",
    "    Compute feature: Difference in publication year\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: the dataset to compute\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A numpy array where each entry corresponds to the temporal difference of a pair of nodes\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    temp_diff = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        src_info, dest_info = node_info[ID[src]], node_info[ID[dest]] # get the associated node information by index\n",
    "        \n",
    "        # compute the difference in publication year in absolute value (because we don't know which one cites the other)\n",
    "        temp_diff[i] = abs(\n",
    "            int(src_info[1]) - int(dest_info[1])\n",
    "        )\n",
    "        \n",
    "    return temp_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing temporal difference for training set takes 2.0621 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the temporal difference\n",
    "train_temp_diff = temporal_difference(training)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing temporal difference for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing temporal difference for testing set takes 0.1297 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the temporal difference\n",
    "test_temp_diff = temporal_difference(testing)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing temporal difference for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  [0. 1. 2. 4. 5.]\n",
      "--> Mean = 2.795, Std = 2.435, Non-null ratio: 0.85\n",
      "\n",
      "Testing:  [0. 1. 2. 0. 5.]\n",
      "--> Mean = 2.814, Std = 2.443, Non-null ratio: 0.85\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_feat_info('Temporal difference', 'Training', train_temp_diff)\n",
    "print_feat_info('Temporal difference', 'Testing', test_temp_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.4 - Published in the same journal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_journal(ds):\n",
    "    '''\n",
    "    Compute feature: whether two articles are published in the same journal\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: dataset to compute feature from\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A numpy array of binary values (0|1)\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    same_journal = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        src_info, dest_info = node_info[ID[src]], node_info[ID[dest]] # get the associated node information by index\n",
    "        \n",
    "        # 1 if two articles are published in the same journal, 0 otherwise\n",
    "        same_journal[i] = int(\n",
    "            len(src_info[4])>0 and  # journal info of source not null\n",
    "            len(dest_info[4])>0 and # journal info of dest not null\n",
    "            src_info[4] == dest_info[4] # the same journal title\n",
    "        )\n",
    "        \n",
    "    return same_journal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing whether two articles are published in the same journal for training set takes 1.7626 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the temporal difference\n",
    "train_same_journal = same_journal(training)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing whether two articles are published in the same journal for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing whether two articles are published in the same journal for testing set takes 0.1241 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the temporal difference\n",
    "test_same_journal = same_journal(testing)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing whether two articles are published in the same journal for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  [1. 0. 0. 0. 0.]\n",
      "--> Mean = 0.110, Std = 0.313, Non-null ratio: 0.11\n",
      "\n",
      "Testing:  [0. 0. 1. 1. 0.]\n",
      "--> Mean = 0.109, Std = 0.312, Non-null ratio: 0.11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_feat_info('In same journal', 'Training', train_same_journal)\n",
    "print_feat_info('In same journal', 'Testing', test_same_journal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Saving features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of selected features\n",
    "features = [\n",
    "    'common_neighbors', # 0\n",
    "    'jaccard', # 1\n",
    "    'adamic_adar', # 2\n",
    "    'in_kcore', # 3\n",
    "    'katz_centrality', # 4\n",
    "    'katz_linkpred', # 5\n",
    "    'max_degrees', # 6\n",
    "    'max_betweenness', # 7\n",
    "    'max_pagerank', # 8\n",
    "    'pref_attach', # 9\n",
    "    'res_alloc', # 10\n",
    "    'cos_tfidf', # 11\n",
    "    'cos_w2v', # 12\n",
    "    'cos_d2v', # 13\n",
    "    'overlap_title', # 14\n",
    "    'common_authors', # 15\n",
    "    'temporal_diff', # 16\n",
    "    'same_journal' # 17\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Saving training features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== create array of training feature ====== #\n",
    "training_features = np.array([\n",
    "    train_common_neigh,\n",
    "    train_jaccard_coeff,\n",
    "    train_aa_coeff,\n",
    "    train_in_kcore,\n",
    "    train_katz,\n",
    "    train_katz_linkpred,\n",
    "    train_degrees,\n",
    "    train_btwn,\n",
    "    train_pagerank,\n",
    "    train_pref_attach,\n",
    "    train_res_alloc,\n",
    "    train_cos_tfidf,\n",
    "    train_cos_w2v,\n",
    "    train_cos_d2v,\n",
    "    train_overlap_title,\n",
    "    train_common_auth,\n",
    "    train_temp_diff,\n",
    "    train_same_journal\n",
    "]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Saving features (training_features) ====== #\n",
    "with open(path_data + 'training_features.csv', 'w', newline='') as f:\n",
    "    csv_out = csv.writer(f)\n",
    "    csv_out.writerow(features)\n",
    "    for row in training_features:\n",
    "        csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Saving testing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== create array of testing features ====== #\n",
    "testing_features = np.array([\n",
    "    test_common_neigh,\n",
    "    test_jaccard_coeff,\n",
    "    test_aa_coeff,\n",
    "    test_in_kcore,\n",
    "    test_katz,\n",
    "    test_katz_linkpred,\n",
    "    test_degrees,\n",
    "    test_btwn,\n",
    "    test_pagerank,\n",
    "    test_pref_attach,\n",
    "    test_res_alloc,\n",
    "    test_cos_tfidf,\n",
    "    test_cos_w2v,\n",
    "    test_cos_d2v,\n",
    "    test_overlap_title,\n",
    "    test_common_auth,\n",
    "    test_temp_diff,\n",
    "    test_same_journal\n",
    "]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Saving features (testing_features) ====== #\n",
    "with open(path_data + 'testing_features.csv', 'w', newline='') as f:\n",
    "    csv_out = csv.writer(f)\n",
    "    csv_out.writerow(features)\n",
    "    for row in testing_features:\n",
    "        csv_out.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
