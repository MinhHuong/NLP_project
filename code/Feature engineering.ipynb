{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is solely used to train features for training and testing set. Our aim is to separate different steps in the workflow such that the code of each module, namely Feature engineering, Predictions, and Evaluation, do not mix up one to another. Having avoided to write a huge notebook that executes everything in one place, the separation brings a way to have a clean, proper code, thus making it easier for further development and maintenance.\n",
    "\n",
    "Feature engineering is one of the most important step among the entire process, since the data that we are given do not provided explicit features. Therefore, we have to come up with a set of features that may or may not contribute to the quality of prediction.\n",
    "\n",
    "The computation of several has proven to be time-consuming. Therefore, it is not practical to recompute everything from stratch every time we work on the project. That's why we decided to save computed features into files, so that we do not have to repeat the process of feature extraction.\n",
    "\n",
    "This notebook is proceeded as follows:\n",
    "- reading data sets,\n",
    "- building the citation graph,\n",
    "- computing features,\n",
    "- saving the features to files to be fed to the classifiers (in another notebook),\n",
    "- additional modifications to the features (adding/replacing features that are already computed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries and Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we precise the necessary libraries and define several utility functions. The **execution time** is also noted down for all important steps. It is useful to bear in mind the amount of time needed to compute some specific features, that way, we must think of a way to avoid expensive computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing libraries and setting up parameters takes 0.0009 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# --- utility librairies --- #\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import csv\n",
    "\n",
    "# --- working with graph by using NetworKit --- #\n",
    "import networkit as nk\n",
    "\n",
    "# --- working with text --- #\n",
    "import nltk\n",
    "# nltk.download('stopwords') # if stopwords haven't been downloaded, please do\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "# --- plotting real cute stuffs --- #\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "end = time.time()\n",
    "print('Importing libraries and setting up parameters takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(nodes, edges):\n",
    "    '''\n",
    "    Build the graph from the set of nodes and edges.\n",
    "    NetworKit does not require labels for nodes, it only needs the index 0,1,2... of the nodes.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    nodes: set of nodes\n",
    "    edges: set of edges\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    the graph\n",
    "    '''\n",
    "    g = nk.Graph(len(nodes)) # adding nodes\n",
    "\n",
    "    for edge in edges:\n",
    "        if not g.hasEdge(edge[0], edge[1]): # avoid multiple edges\n",
    "            g.addEdge(edge[0], edge[1])\n",
    "            \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, dg_removal=True, sw_removal=True, stemming=True):\n",
    "    '''\n",
    "    Preprocess text: digit removal, stopword removal, stemming\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text: text on which preprocessing is applied\n",
    "    dg_removal: whether to apply digit removal or not\n",
    "    sw_removal: whether to apply stopword removal or not\n",
    "    stemming: whether to apply stemming or not\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    the text after preprocessing\n",
    "    '''\n",
    "    result = text\n",
    "    \n",
    "    sw = set(nltk.corpus.stopwords.words('english')) # set of stopwords\n",
    "    stemmer = nltk.stem.PorterStemmer() # stemmer\n",
    "    \n",
    "    if dg_removal:\n",
    "        result = re.sub('[0-9]', '', result)\n",
    "    \n",
    "    if sw_removal:\n",
    "        result = ' '.join([token for token in result.split() if token not in sw])\n",
    "        \n",
    "    if stemming:\n",
    "        result = ' '.join([stemmer.stem(token) for token in result.split()])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_feat_info(feat_name, set_name, arr):\n",
    "    '''\n",
    "    Print mean and standard deviation of a feature on training or testing set\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    feat_name: feature name\n",
    "    set_namme: 'training' | 'testing'\n",
    "    arr: the feature array\n",
    "    '''\n",
    "    print(\"%s: \" % set_name, arr[0:5])\n",
    "    print('--> Mean = %.3f, Std = %.3f, Non-null ratio: %.2f'\n",
    "          %(np.mean(arr), np.std(arr), float(np.count_nonzero(arr))/float(len(arr))))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the libraries and utility functions are properly set up, let's get work done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1. Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '../data/' # path to the data\n",
    "path_submission = '../submission/' # path to submission files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading node information takes 0.3492 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== read in node informations ====== #\n",
    "with open(path_data + 'node_information.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    node_info = list(reader)\n",
    "\n",
    "end = time.time()\n",
    "print('Reading node information takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training set takes 2.9434 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== read training data as str ====== #\n",
    "training = np.genfromtxt(path_data + 'training_set.txt', dtype=str)\n",
    "\n",
    "end = time.time()\n",
    "print('Reading training set takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading testing set takes 0.1551 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== read testing data as str ====== #\n",
    "testing = np.genfromtxt(path_data + 'testing_set.txt', dtype=str)\n",
    "\n",
    "end = time.time()\n",
    "print('Reading testing set takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Building the citation graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data loaded in, we should have enough information to construct the citation graph. It is observed that the number of edges is approximately half of the size of the training set i.e. the negative and positive class labels are **balanced**. Hence, it would later be easier to train classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the citation graph takes 172.6058 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== build the graph ====== #\n",
    "\n",
    "nodes = [element[0] for element in node_info] # create index list to be passed as nodes\n",
    "edges = [(nodes.index(element[0]), nodes.index(element[1])) for element in training if element[2] == '1']\n",
    "g = build_graph(nodes, edges)\n",
    "\n",
    "end = time.time()\n",
    "print('Building the citation graph takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vertices: 27770\n",
      "Number of edges (after multiple edges removal): 334690\n"
     ]
    }
   ],
   "source": [
    "# check for general information of the graph\n",
    "print('Number of vertices: %d' % g.numberOfNodes())\n",
    "print('Number of edges (after multiple edges removal): %d' % g.numberOfEdges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Computing features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of features is described as follows, and the computation rule is the same for both training and testing set.\n",
    "\n",
    "| Feature                | Explanation                                                        | Value       | Type        |\n",
    "|:----------------------:|:------------------------------------------------------------------:|:-----------:|:-----------:|\n",
    "| Common neighbors       | Number of common neighbors                                         | numercial   | topological |\n",
    "| Jaccard coefficient    | Link-based Jaccard coefficient                                     | numerical   | topological |\n",
    "| Adamic-Adar coefficient| Adamic-Adar coefficient between two nodes                          | numercial   | topological |\n",
    "| In the same k-core     | Whether both nodes/one of them/none of them are in the same k-core | categorical | topological |\n",
    "| Katz index             | (centrality package) By-pair maximum of Katz centrality value      | numerical   | topological |\n",
    "| Katz index             | (linkprediction package) The traditional approach to compute Katz  | numerical   | topological |\n",
    "| Degree                 | By-pair maximum of degree centrality values                        | numerical   | topological |\n",
    "| Betweenness centrality | By-pair maximum of betweenness centrality values                   | numerical   | topological |\n",
    "| PageRank score         | By-pair maximum of PageRank score                                  | numerical   | topological |\n",
    "| Preferential Attachment| Preferential attachment metric of a pair of nodes                  | numerical   | topological |\n",
    "| Resource Allocation    | Resource Allocation matrix of a pair of nodes                      | numerical   | topological |\n",
    "| Cosine similarity      | Cosine similarity between word vectors of titles + abstracts       | numerical   | semantic    |\n",
    "| Title overlap          | Number of overlapping words in title                               | numerical   | meta-data   |\n",
    "| Common authors         | The number of common authors between two articles                  | numerical   | meta-data   |\n",
    "| Temporal difference    | Difference in publication year (absolute value)                    | numerical   | meta-data   |\n",
    "| Same journal           | Whether two articles are published in the same journal             | binary      | meta-data   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the dictionary of (ID STRING - index INT) to accelerate access to a node's ID in the built graph\n",
    "ID = dict(zip(nodes, [nodes.index(n) for n in nodes]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to create such a mapping between an article's ID (e.g. '1001', '1002', '1003') and a node's ID in the graph (0, 1, 2), because it speeds up remarkably the computation time.\n",
    "\n",
    "Until now, we are ready to compute the set of features of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Topological features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 - Number of common neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_neighbors(ds, g):\n",
    "    '''\n",
    "    Feature: The number of common neighbors\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: dataset to compute feature from\n",
    "    g: the graph\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    common_neigh = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        \n",
    "        # collect the number of overlapping words in title\n",
    "        common_neigh[i] = len(\n",
    "            set(g.neighbors(ID[src]))\n",
    "            .intersection(set(g.neighbors(ID[dest])))\n",
    "        )\n",
    "        \n",
    "    return common_neigh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the number of common neighbors for training set takes 13.1309 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the average degree\n",
    "train_common_neigh = common_neighbors(training, g)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the number of common neighbors for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the number of common neighbors for testing set takes 0.7275 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the average degree\n",
    "test_common_neigh = common_neighbors(testing, g)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the number of common neighbors for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  [ 1. 20.  0.  0.  0.]\n",
      "--> Mean = 6.232, Std = 11.137, Non-null ratio: 0.53\n",
      "\n",
      "Testing:  [ 0. 24. 59. 21.  0.]\n",
      "--> Mean = 6.159, Std = 10.945, Non-null ratio: 0.53\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_feat_info('Common neighbors', 'Training', train_common_neigh)\n",
    "print_feat_info('Common neighbors', 'Testing', test_common_neigh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 - Jaccard coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_coeff(ds, g):\n",
    "    '''\n",
    "    Feature: Link-based Jaccard coefficient\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: dataset to compute feature from\n",
    "    g: the graph\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    coeff = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        \n",
    "        # collect the number of overlapping words in title\n",
    "        inters = len(\n",
    "            set(g.neighbors(ID[src]))\n",
    "            .intersection(set(g.neighbors(ID[dest])))\n",
    "        ) # intersection of neighbors\n",
    "        \n",
    "        union = len(\n",
    "            set(g.neighbors(ID[src]))\n",
    "            .union(set(g.neighbors(ID[dest])))\n",
    "        ) # union of neighbors\n",
    "        \n",
    "        coeff[i] = (float(inters)/float(union) if union != 0 else 0)\n",
    "        \n",
    "    return coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing link-based Jaccard coefficient for training set takes 22.8225 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the average degree\n",
    "train_jaccard_coeff = jaccard_coeff(training, g)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing link-based Jaccard coefficient for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing link-based Jaccard coefficient for testing set takes 1.3702 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the average degree\n",
    "test_jaccard_coeff = jaccard_coeff(testing, g)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing link-based Jaccard coefficient for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  [0.05882353 0.09708738 0.         0.         0.        ]\n",
      "--> Mean = 0.058, Std = 0.090, Non-null ratio: 0.53\n",
      "\n",
      "Testing:  [0.         0.07430341 0.06533776 0.22105263 0.        ]\n",
      "--> Mean = 0.061, Std = 0.097, Non-null ratio: 0.53\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_feat_info('Jaccard coefficient', 'Training', train_jaccard_coeff)\n",
    "print_feat_info('Jaccard coefficient', 'Testing', test_jaccard_coeff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 - Adamic-Adar coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adamic_adar_coeff(ds, g):\n",
    "    '''\n",
    "    Feature: Adamic-Adar coefficient\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: the dataset\n",
    "    g: graph\n",
    "    '''\n",
    "    \n",
    "    size = len(ds)\n",
    "    aa_coeff = np.zeros(size)\n",
    "    aa_index = nk.linkprediction.AdamicAdarIndex(g)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        \n",
    "        aa_coeff[i] = aa_index.run(ID[src], ID[dest])\n",
    "        \n",
    "    return aa_coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Adamic-Adar coefficient feature for training set takes 5.6423 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the adamic-adar coefficient\n",
    "train_aa_coeff = adamic_adar_coeff(training, g)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing Adamic-Adar coefficient feature for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Adamic-Adar coefficient feature for testing set takes 0.3349 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the adamic-adar coefficient\n",
    "test_aa_coeff = adamic_adar_coeff(testing, g)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing Adamic-Adar coefficient feature for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  [0.51389834 4.32036615 0.         0.         0.        ]\n",
      "--> Mean = 1.513, Std = 2.740, Non-null ratio: 0.53\n",
      "\n",
      "Testing:  [ 0.          5.37797275 15.05361173  4.89942438  0.        ]\n",
      "--> Mean = 1.498, Std = 2.692, Non-null ratio: 0.53\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_feat_info('Adamic-Adar', 'Training', train_aa_coeff)\n",
    "print_feat_info('Adamic-Adar', 'Testing', test_aa_coeff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4 - In k-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core decomposition of the graph takes 0.1260 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "core_decomp = nk.community.CoreDecomposition(g, storeNodeOrder=True)\n",
    "core_decomp.run()\n",
    "cover_g = core_decomp.getCover()\n",
    "order = 15 # important parameters\n",
    "\n",
    "end = time.time()\n",
    "print('Core decomposition of the graph takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9647 nodes that belong in the 15-core decomposition of this graph\n"
     ]
    }
   ],
   "source": [
    "print('There are %d nodes that belong in the %d-core decomposition of this graph' \n",
    "      % (len(cover_g.getMembers(order)), order))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_kcore(ds, kcore):\n",
    "    '''\n",
    "    Compute feature: whether a pair of nodes is found in the same k-core graph\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: dataset\n",
    "    kcore: the k-core graph after decomposition as a set of nodes index (ranged from 0 to 27,770)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A numpy array of ordinal values: \n",
    "        - 0 if both nodes are not in the kcores, \n",
    "        - 0.5 if one of them is in the kcores, \n",
    "        - 1 of they are both in the k-core\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    same_kcore = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        \n",
    "        # compute whether two nodes are in the given kcore | one of them is in the kcore | none of them\n",
    "        index_src = ID[src] # index of src\n",
    "        index_dest = ID[dest] # index of dest\n",
    "        \n",
    "        if index_src in kcore and index_dest in kcore:\n",
    "            result = 1.0\n",
    "        elif index_src not in kcore and index_dest not in kcore:\n",
    "            result = 0.0\n",
    "        else:\n",
    "            result = 0.5\n",
    "            \n",
    "        same_kcore[i] = result\n",
    "        \n",
    "    return same_kcore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the in k-core feature for training set takes 1.1210 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the position of two nodes wrt k-core\n",
    "train_in_kcore = in_kcore(training, cover_g.getMembers(order))\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the in k-core feature for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the in k-core feature for testing set takes 0.1098 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the position of two nodes wrt k-core\n",
    "test_in_kcore = in_kcore(testing, cover_g.getMembers(order))\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the in k-core feature for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  [0.  1.  0.  0.5 0.5]\n",
      "--> Mean = 0.564, Std = 0.420, Non-null ratio: 0.70\n",
      "\n",
      "Testing:  [1.  1.  1.  1.  0.5]\n",
      "--> Mean = 0.555, Std = 0.418, Non-null ratio: 0.70\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_feat_info('In k-core', 'Training', train_in_kcore)\n",
    "print_feat_info('In k-core', 'Testing', test_in_kcore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.5 - Katz index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (A) *centrality* package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (B) *linkprediction* package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.6 - By-pair maximum of degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_degrees(ds, g):\n",
    "    '''\n",
    "    Feature: Maximum degrees among 2 nodes\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: dataset to compute feature from\n",
    "    g: the graph\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    max_degree = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        \n",
    "        # collect the number of overlapping words in title\n",
    "        src_deg = g.degree(ID[src])\n",
    "        dest_deg = g.degree(ID[dest])\n",
    "        max_degree[i] = max(src_deg, dest_deg)\n",
    "        \n",
    "    return max_degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the by-pair maximum degree for training set takes 1.4165 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the average degree\n",
    "train_degrees = max_degrees(training, g)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the by-pair maximum degree for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the by-pair maximum degree for testing set takes 0.0842 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the average degree\n",
    "test_degrees = max_degrees(testing, g)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the by-pair maximum degree for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  [ 12. 147.   5.  20.  24.]\n",
      "--> Mean = 106.541, Std = 239.810, Non-null ratio: 1.00\n",
      "\n",
      "Testing:  [ 59. 302. 739.  65. 150.]\n",
      "--> Mean = 107.564, Std = 243.680, Non-null ratio: 1.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_feat_info('By-pair max degree', 'Training', train_degrees)\n",
    "print_feat_info('By-pair max degree', 'Testing', test_degrees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.7 - By-pair maximum of betweenness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.8 - By-pair maximum of PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the PageRank index of the graph takes 0.2161 s\n"
     ]
    }
   ],
   "source": [
    "# ====== compute PageRank index ====== #\n",
    "start = time.time()\n",
    "\n",
    "page_rank_g = nk.centrality.PageRank(g)\n",
    "page_rank_g.run()\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the PageRank index of the graph takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pagerank(ds, pr):\n",
    "    '''\n",
    "    Compute feature: average of pagerank\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: dataset to compute feature from\n",
    "    pr: PageRank centrality object\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    max_pr = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        \n",
    "        # collect the average of betweenness centrality of 2 nodes\n",
    "        # log to \"dampen\" too small values\n",
    "        _max = max(pr[ID[src]], pr[ID[dest]])\n",
    "        max_pr[i] = np.log(_max) if _max != 0.0 else 0.0\n",
    "        \n",
    "    return max_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the by-pair maximum page rank for training set takes 1.9228 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the average pagerank on training set\n",
    "train_pagerank = max_pagerank(training, page_rank_g.scores())\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the by-pair maximum page rank for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the by-pair maximum page rank for testing set takes 0.1348 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the average pagerank\n",
    "test_pagerank = max_pagerank(testing, page_rank_g.scores())\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the by-pair maximum page rank for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  [-10.4952174   -9.04507102 -11.03082025 -10.67156463 -10.4743869 ]\n",
      "--> Mean = -9.655, Std = 0.869, Non-null ratio: 1.00\n",
      "\n",
      "Testing:  [-9.71123221 -8.05014677 -7.28060674 -9.78108665 -8.84688464]\n",
      "--> Mean = -9.667, Std = 0.884, Non-null ratio: 1.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_feat_info('By-pair max PageRank', 'Training', train_pagerank)\n",
    "print_feat_info('By-pair max PageRank', 'Testing', test_pagerank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.9 - Preferential Attachment index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pref_attach(ds, pa):\n",
    "    '''\n",
    "    Feature: Preferential Attachment between 2 nodes\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: dataset\n",
    "    pa: prefenrential attachment object\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    pa_result = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        _pa = pa.run(ID[src], ID[dest])\n",
    "        pa_result[i] = np.log(_pa) if _pa != 0 else 0.0\n",
    "        \n",
    "    return pa_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_object = nk.linkprediction.PreferentialAttachmentIndex(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Preferential Attachment feature for training set takes 1.9145 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the Preferential Attachment index\n",
    "train_pref_attach = pref_attach(training, pa_object)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing Preferential Attachment feature for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Preferential Attachment feature for testing set takes 0.1245 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the Preferential Attachment index\n",
    "test_pref_attach = pref_attach(testing, pa_object)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing Preferential Attachment feature for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  [4.27666612 9.35988044 1.60943791 5.6347896  5.12396398]\n",
      "--> Mean = 6.442, Std = 2.200, Non-null ratio: 1.00\n",
      "\n",
      "Testing:  [ 6.9679092   9.51708951 12.01246969  8.1062129   6.95654544]\n",
      "--> Mean = 6.379, Std = 2.223, Non-null ratio: 0.99\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_feat_info('Preferential Attachment', 'Training', train_pref_attach)\n",
    "print_feat_info('Preferential Attachment', 'Testing', test_pref_attach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.10 - Resource Allocation measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_allocation(ds, ra):\n",
    "    '''\n",
    "    Feature: ResourceAllocation index\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: dataset\n",
    "    ra: Resource Allocation object\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    ra_result = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        _ra = ra.run(ID[src], ID[dest])\n",
    "        ra_result[i] = _ra\n",
    "        \n",
    "    return ra_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "ra_object = nk.linkprediction.ResourceAllocationIndex(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Resource Allocation feature for training set takes 5.1549 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute Resource Allocation \n",
    "train_res_alloc = res_allocation(training, ra_object)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing Resource Allocation feature for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Resource Allocation feature for testing set takes 0.3013 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute Resource Allocation \n",
    "test_res_alloc = res_allocation(testing, ra_object)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing Resource Allocation feature for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  [0.14285714 0.22640079 0.         0.         0.        ]\n",
      "--> Mean = 0.125, Std = 0.247, Non-null ratio: 0.53\n",
      "\n",
      "Testing:  [0.         0.31153472 1.34259427 0.29841899 0.        ]\n",
      "--> Mean = 0.125, Std = 0.243, Non-null ratio: 0.53\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_feat_info('Resouce Allocation', 'Training', train_res_alloc)\n",
    "print_feat_info('Resouce Allocation', 'Testing', test_res_alloc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Semantic features: Cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 - TD-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 - word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 doc2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Meta-data features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 - Number of overlapping words in titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 - Number of common authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_authors(ds):\n",
    "    '''\n",
    "    Compute feature: number of common authors\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: dataset to compute feature from\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A numpy array\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    common_auth = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        src_info, dest_info = node_info[ID[src]], node_info[ID[dest]] # get the associated node information by index\n",
    "        \n",
    "        # compute the difference in publication year in absolute value (because we don't know which one cites the other)\n",
    "        common_auth[i] = len(\n",
    "            set(src_info[3].split(','))\n",
    "            .intersection(set(dest_info[3].split(',')))\n",
    "        )\n",
    "        \n",
    "    return common_auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the number of common authors for training set takes 2.7997 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the temporal difference\n",
    "train_common_auth = common_authors(training)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the number of common authors for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the number of common authors for testing set takes 0.1966 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the temporal difference\n",
    "test_common_auth = common_authors(testing)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the number of common authors for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  [0. 0. 0. 0. 0.]\n",
      "--> Mean = 0.083, Std = 0.357, Non-null ratio: 0.06\n",
      "\n",
      "Testing:  [0. 0. 0. 0. 0.]\n",
      "--> Mean = 0.082, Std = 0.351, Non-null ratio: 0.06\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_feat_info('Common authors', 'Training', train_common_auth)\n",
    "print_feat_info('Common authors', 'Testing', test_common_auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 - Temporal difference in publication year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_difference(ds):\n",
    "    '''\n",
    "    Compute feature: Difference in publication year\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: the dataset to compute\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A numpy array where each entry corresponds to the temporal difference of a pair of nodes\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    temp_diff = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        src_info, dest_info = node_info[ID[src]], node_info[ID[dest]] # get the associated node information by index\n",
    "        \n",
    "        # compute the difference in publication year in absolute value (because we don't know which one cites the other)\n",
    "        temp_diff[i] = abs(\n",
    "            int(src_info[1]) - int(dest_info[1])\n",
    "        )\n",
    "        \n",
    "    return temp_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing temporal difference for training set takes 2.8968 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the temporal difference\n",
    "train_temp_diff = temporal_difference(training)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing temporal difference for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing temporal difference for testing set takes 0.2023 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the temporal difference\n",
    "test_temp_diff = temporal_difference(testing)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing temporal difference for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  [0. 1. 2. 4. 5.]\n",
      "--> Mean = 2.795, Std = 2.435, Non-null ratio: 0.85\n",
      "\n",
      "Testing:  [0. 1. 2. 0. 5.]\n",
      "--> Mean = 2.814, Std = 2.443, Non-null ratio: 0.85\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_feat_info('Temporal difference', 'Training', train_temp_diff)\n",
    "print_feat_info('Temporal difference', 'Testing', test_temp_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.4 - Published in the same journal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_journal(ds):\n",
    "    '''\n",
    "    Compute feature: whether two articles are published in the same journal\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: dataset to compute feature from\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A numpy array of binary values (0|1)\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    same_journal = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        src_info, dest_info = node_info[ID[src]], node_info[ID[dest]] # get the associated node information by index\n",
    "        \n",
    "        # 1 if two articles are published in the same journal, 0 otherwise\n",
    "        same_journal[i] = int(\n",
    "            len(src_info[4])>0 and  # journal info of source not null\n",
    "            len(dest_info[4])>0 and # journal info of dest not null\n",
    "            src_info[4] == dest_info[4] # the same journal title\n",
    "        )\n",
    "        \n",
    "    return same_journal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing whether two articles are published in the same journal for training set takes 2.4349 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the temporal difference\n",
    "train_same_journal = same_journal(training)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing whether two articles are published in the same journal for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing whether two articles are published in the same journal for testing set takes 0.1387 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the temporal difference\n",
    "test_same_journal = same_journal(testing)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing whether two articles are published in the same journal for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  [1. 0. 0. 0. 0.]\n",
      "--> Mean = 0.110, Std = 0.313, Non-null ratio: 0.11\n",
      "\n",
      "Testing:  [0. 0. 1. 1. 0.]\n",
      "--> Mean = 0.109, Std = 0.312, Non-null ratio: 0.11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_feat_info('In same journal', 'Training', train_same_journal)\n",
    "print_feat_info('In same journal', 'Testing', test_same_journal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Saving features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Additional operations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
