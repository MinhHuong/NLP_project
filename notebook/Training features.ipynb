{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries and Utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing libraries and setting up parameters takes 0.0008 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "import networkit as nk\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn import preprocessing as pre\n",
    "\n",
    "# working with text\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "end = time.time()\n",
    "print('Importing libraries and setting up parameters takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(nodes, edges):\n",
    "    g = nk.Graph(len(nodes)) # adding nodes\n",
    "\n",
    "    for edge in edges:\n",
    "        if not g.hasEdge(edge[0], edge[1]): # avoid multiple edges\n",
    "            g.addEdge(edge[0], edge[1])\n",
    "            \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to preprocess text\n",
    "def preprocess(text, dg_removal=True, sw_removal=True, stemming=True):\n",
    "    '''\n",
    "    Preprocess text: stopword removal, stemming, digit removal\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text: text on which preprocessing is applied\n",
    "    dg_removal: whether to apply digit removal or not\n",
    "    sw_removal: whether to apply stopword removal or not\n",
    "    stemming: whether to apply stemming or not\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    the text after preprocessing\n",
    "    '''\n",
    "    result = text\n",
    "    \n",
    "    sw = set(nltk.corpus.stopwords.words('english')) # set of stopwords\n",
    "    stemmer = nltk.stem.PorterStemmer() # stemmer\n",
    "    \n",
    "    if dg_removal:\n",
    "        result = re.sub('[0-9]', '', result)\n",
    "    \n",
    "    if sw_removal:\n",
    "        result = ' '.join([token for token in result.split() if token not in sw])\n",
    "        \n",
    "    if stemming:\n",
    "        result = ' '.join([stemmer.stem(token) for token in result.split()])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '../data/' # path to the data\n",
    "path_submission = '../submission/' # path to submission files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading node information takes 0.6638 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== read in node informations ====== #\n",
    "with open(path_data + 'node_information.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    node_info = list(reader)\n",
    "\n",
    "end = time.time()\n",
    "print('Reading node information takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training set takes 8.5168 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== read training data as str ====== #\n",
    "training = np.genfromtxt(path_data + 'training_set.txt', dtype=str)\n",
    "\n",
    "end = time.time()\n",
    "print('Reading training set takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading testing set takes 0.3988 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== read testing data as str ====== #\n",
    "testing = np.genfromtxt(path_data + 'testing_set.txt', dtype=str)\n",
    "\n",
    "end = time.time()\n",
    "print('Reading testing set takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the citation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the citation graph takes 299.2728 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== build the graph ====== #\n",
    "\n",
    "nodes = [element[0] for element in node_info] # create index list to be passed as nodes\n",
    "edges = [(nodes.index(element[0]), nodes.index(element[1])) for element in training if element[2] == '1']\n",
    "g = build_graph(nodes, edges)\n",
    "\n",
    "end = time.time()\n",
    "print('Building the citation graph takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vertices: 27770\n",
      "Number of edges (after multiple edges removal): 334690\n"
     ]
    }
   ],
   "source": [
    "# check for general information of the graph\n",
    "print('Number of vertices: %d' % g.numberOfNodes())\n",
    "print('Number of edges (after multiple edges removal): %d' % g.numberOfEdges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of features is described as follows, and the computation rule is the same for both training and testing set.\n",
    "\n",
    "| Feature                | Explanation                                                        | Type      | Range   |\n",
    "|:----------------------:|:------------------------------------------------------------------:|:---------:|:-------:|\n",
    "| Temporal difference    | Difference in publication year (absolute value)                    | numerical | $\\ge$ 0 |\n",
    "| Common authors         | The number of common authors between two articles                  | numerical | $\\ge$ 0 |\n",
    "| Same journal           | Whether two articles are published in the same journal             | binary    | 0, 1    |\n",
    "| Cosine similarity      | Cosine similarity between word vectors of abstracts                | numerical | [0,1]   |\n",
    "| Title overlap          | Number of overlapping words in title                               | numerical | $\\ge$ 0 |\n",
    "| Degree difference      | Difference in measure of degrees of two nodes (absolute value)     | numerical | $\\ge$ 0 |\n",
    "| Common neighbors       | Number of common neighbors                                         | numercial | $\\ge$ 0 |\n",
    "| Jaccard coefficient    | Link-based Jaccard coefficient                                     | numerical | [0,1]   |\n",
    "| Same cluster           | Check whether two nodes are in the same cluster                    | binary    | [0,1]   |\n",
    "| PageRank difference    | Difference in PageRank index of two nodes (absolute value)         | numerical | $\\ge$ 0 |\n",
    "| Betweenness centrality | Difference in betweenness centrality of two nodes (absolute value) | numerical | $\\ge$ 0 |\n",
    "| In the same k-core     | Whether both nodes/one of them/none of them are in the same k-core | ordinal   |[0,0.5,1]|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the dictionary of (ID-index) to accelerate access to node'ID\n",
    "ID = dict(zip(nodes, [nodes.index(n) for n in nodes]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Temporal difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_difference(ds):\n",
    "    '''\n",
    "    Compute feature: Difference in publication year\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: the dataset to compute\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A numpy array where each entry corresponds to the temporal difference of a pair of nodes\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    temp_diff = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        src_info, dest_info = node_info[ID[src]], node_info[ID[dest]] # get the associated node information by index\n",
    "        \n",
    "        # compute the difference in publication year in absolute value (because we don't know which one cites the other)\n",
    "        temp_diff[i] = abs(\n",
    "            int(src_info[1]) - int(dest_info[1])\n",
    "        )\n",
    "        \n",
    "    return temp_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing temporal difference for training set takes 1.6023 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the temporal difference\n",
    "train_temp_diff = temporal_difference(training)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing temporal difference for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing temporal difference for testing set takes 0.1491 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the temporal difference\n",
    "test_temp_diff = temporal_difference(testing)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing temporal difference for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: [0. 1. 2. 4. 5. 0. 4. 7. 0. 8.]\n",
      "Testing: [0. 1. 2. 0. 5. 4. 0. 1. 7. 0.]\n"
     ]
    }
   ],
   "source": [
    "print('Training:', train_temp_diff[0:10])\n",
    "print('Testing:', test_temp_diff[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Number of common authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_authors(ds):\n",
    "    '''\n",
    "    Compute feature: number of common authors\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: dataset to compute feature from\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A numpy array\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    common_auth = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        src_info, dest_info = node_info[ID[src]], node_info[ID[dest]] # get the associated node information by index\n",
    "        \n",
    "        # compute the difference in publication year in absolute value (because we don't know which one cites the other)\n",
    "        common_auth[i] = len(\n",
    "            set(src_info[3].split(','))\n",
    "            .intersection(set(dest_info[3].split(',')))\n",
    "        )\n",
    "        \n",
    "        \n",
    "    return common_auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the number of common authors for training set takes 2.4319 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the temporal difference\n",
    "train_common_auth = common_authors(training)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the number of common authors for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the number of common authors for testing set takes 0.2100 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the temporal difference\n",
    "test_common_auth = common_authors(testing)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the number of common authors for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "Testing: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print('Training:', train_common_auth[0:10])\n",
    "print('Testing:', test_common_auth[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Same journal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_journal(ds):\n",
    "    '''\n",
    "    Compute feature: whether two articles are published in the same journal\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: dataset to compute feature from\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A numpy array of binary values (0|1)\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    same_journal = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        src_info, dest_info = node_info[ID[src]], node_info[ID[dest]] # get the associated node information by index\n",
    "        \n",
    "        # 1 if two articles are published in the same journal, 0 otherwise\n",
    "        same_journal[i] = int(\n",
    "            len(src_info[4])>0 and  # journal info of source not null\n",
    "            len(dest_info[4])>0 and # journal info of dest not null\n",
    "            src_info[4] == dest_info[4] # the same journal title\n",
    "        )\n",
    "        \n",
    "        \n",
    "    return same_journal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing whether two articles are published in the same journal for training set takes 1.8569 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the temporal difference\n",
    "train_same_journal = same_journal(training)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing whether two articles are published in the same journal for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing whether two articles are published in the same journal for testing set takes 0.1140 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the temporal difference\n",
    "test_same_journal = same_journal(testing)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing whether two articles are published in the same journal for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Testing: [0. 0. 1. 1. 0. 0. 0. 1. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print('Training:', train_same_journal[0:10])\n",
    "print('Testing:', test_same_journal[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cosine similarity in title + abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/huong/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Computing the TF-IDF matrix takes 54.7182 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== corpus is the set of titles + abstracts, apply preprocessing to each article ======#\n",
    "#nltk.download('stopwords') # uncomment if haven't downloaded stopwords\n",
    "corpus = [preprocess(element[2] + ' ' + element[5], dg_removal=True, sw_removal=True, stemming=True) \n",
    "          for element in node_info]\n",
    "vectorizer = TfidfVectorizer(stop_words='english') # create a TF-IDF vectorizer\n",
    "tfidf = vectorizer.fit_transform(corpus) # TD-IDF matrix of the entire corpus (set of abstracts)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the TF-IDF matrix takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim_text(ds, tfidf):\n",
    "    '''\n",
    "    Compute feature: cosine similarity in title and abstract\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: dataset to compute feature from\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A numpy array of cosine values\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    cosines = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        \n",
    "        # collect the cosine similarity\n",
    "        src_vect, dest_vect = tfidf[ID[src]], tfidf[ID[dest]] # get the corresponding vector in TD-IDF matrix\n",
    "        cos = cosine_similarity(src_vect, dest_vect) # compute cosine similarity\n",
    "        cosines[i] = cos\n",
    "        \n",
    "    return cosines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing cosine similarity for training set takes 355.2014 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the cosine similarity\n",
    "train_cosine = cosine_sim_text(training, tfidf)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing cosine similarity for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing cosine similarity for testing set takes 22.1789 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the cosine similarity\n",
    "test_cosine = cosine_sim_text(testing, tfidf)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing cosine similarity for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: [0.19996622 0.06436945 0.02053711 0.05937844 0.09852643 0.39581923\n",
      " 0.18722569 0.08627054 0.04181436 0.06044751]\n",
      "Testing: [0.11804009 0.30786265 0.20753805 0.16112407 0.31824453 0.03466872\n",
      " 0.02490266 0.19991048 0.         0.3283665 ]\n"
     ]
    }
   ],
   "source": [
    "print('Training:', train_cosine[0:10])\n",
    "print('Testing:', test_cosine[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Number of overlapped words in title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap_title(ds):\n",
    "    '''\n",
    "    Compute feature: number of overlapping words in the title\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: dataset to compute feature from\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A numpy array of numerical values\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    overlap_title = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        src_info, dest_info = node_info[ID[src]], node_info[ID[dest]] # get the associated node information by index\n",
    "        \n",
    "        # collect the number of overlapping words in title\n",
    "        src_title, dest_title = preprocess(src_info[2]).split(), preprocess(dest_info[2]).split()\n",
    "        overlap_title[i] = len(\n",
    "            set(src_title)\n",
    "            .intersection(set(dest_title))\n",
    "        )\n",
    "        \n",
    "    return overlap_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing number of overlapping words in title for training set takes 620.0249 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the number of overlapping words in title\n",
    "train_overlap_title = overlap_title(training)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing number of overlapping words in title for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing number of overlapping words in title for testing set takes 35.2748 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the number of overlapping words in title\n",
    "test_overlap_title = overlap_title(testing)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing number of overlapping words in title for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: [2. 1. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Testing: [0. 2. 1. 1. 0. 0. 1. 1. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print('Training:', train_overlap_title[0:10])\n",
    "print('Testing:', test_overlap_title[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Average of degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_degrees(ds, g):\n",
    "    '''\n",
    "    Compute feature: Average degrees of 2 nodes\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: dataset to compute feature from\n",
    "    g: the graph\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A numpy array of numerical values\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    avg_degree = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        \n",
    "        # collect the number of overlapping words in title\n",
    "        src_deg = g.degree(ID[src])\n",
    "        dest_deg = g.degree(ID[dest])\n",
    "        avg_degree[i] = float(src_deg + dest_deg)/2.0\n",
    "        \n",
    "    return avg_degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the average degree for training set takes 3.3527 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the average degree\n",
    "train_avg_degrees = average_degrees(training, g)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the average degree for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the average degree for testing set takes 0.2641 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the average degree\n",
    "test_avg_degrees = average_degrees(testing, g)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the average degree for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: [  9.  113.    3.   17.   15.5  36.5 400.   50.5  70.5  17. ]\n",
      "Testing: [ 38.5 173.5 481.   58.   78.5  24.5   2.5  31.5   4.5  12.5]\n"
     ]
    }
   ],
   "source": [
    "print('Training:', train_avg_degrees[0:10])\n",
    "print('Testing:', test_avg_degrees[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Number of common neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_neighbors(ds, g):\n",
    "    '''\n",
    "    Compute feature: The number of common neighbors\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: dataset to compute feature from\n",
    "    g: the graph\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A numpy array of numerical values\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    common_neigh = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        \n",
    "        # collect the number of overlapping words in title\n",
    "        common_neigh[i] = len(\n",
    "            set(g.neighbors(ID[src]))\n",
    "            .intersection(set(g.neighbors(ID[dest])))\n",
    "        )\n",
    "        \n",
    "    return common_neigh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the number of common neighbors for training set takes 8.2902 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the average degree\n",
    "train_common_neigh = common_neighbors(training, g)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the number of common neighbors for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the number of common neighbors for testing set takes 0.4579 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the average degree\n",
    "test_common_neigh = common_neighbors(testing, g)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the number of common neighbors for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: [ 1. 20.  0.  0.  0. 14. 12.  0.  5.  0.]\n",
      "Testing: [ 0. 24. 59. 21.  0.  0.  0.  6.  0.  4.]\n"
     ]
    }
   ],
   "source": [
    "print('Training:', train_common_neigh[0:10])\n",
    "print('Testing:', test_common_neigh[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Link-based Jaccard coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_coeff(ds, g):\n",
    "    '''\n",
    "    Compute feature: Link-based Jaccard coefficient\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: dataset to compute feature from\n",
    "    g: the graph\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A numpy array of numerical values\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    coeff = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        \n",
    "        # collect the number of overlapping words in title\n",
    "        inters = len(\n",
    "            set(g.neighbors(ID[src]))\n",
    "            .intersection(set(g.neighbors(ID[dest])))\n",
    "        ) # intersection of neighbors\n",
    "        \n",
    "        union = len(\n",
    "            set(g.neighbors(ID[src]))\n",
    "            .union(set(g.neighbors(ID[dest])))\n",
    "        ) # union of neighbors\n",
    "        \n",
    "        coeff[i] = (float(inters)/float(union) if union != 0 else 0)\n",
    "        \n",
    "    return coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing link-based Jaccard coefficient for training set takes 18.4065 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the average degree\n",
    "train_jaccard_coeff = jaccard_coeff(training, g)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing link-based Jaccard coefficient for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing link-based Jaccard coefficient for testing set takes 1.0820 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the average degree\n",
    "test_jaccard_coeff = jaccard_coeff(testing, g)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing link-based Jaccard coefficient for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: [0.05882353 0.09708738 0.         0.         0.         0.23728814\n",
      " 0.01522843 0.         0.03676471 0.        ]\n",
      "Testing: [0.         0.07430341 0.06533776 0.22105263 0.         0.\n",
      " 0.         0.10526316 0.         0.19047619]\n"
     ]
    }
   ],
   "source": [
    "print('Training:', train_jaccard_coeff[0:10])\n",
    "print('Testing:', test_jaccard_coeff[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Same cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Average of PageRank index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the PageRank index of the graph takes 0.1994 s\n"
     ]
    }
   ],
   "source": [
    "# ====== compute PageRank index ====== #\n",
    "start = time.time()\n",
    "\n",
    "page_rank_g = nk.centrality.PageRank(g)\n",
    "page_rank_g.run()\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the PageRank index of the graph takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_pagerank(ds, pr):\n",
    "    '''\n",
    "    Compute feature: average of pagerank\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: dataset to compute feature from\n",
    "    g: the graph\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A numpy array of numerical values\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    avg_pr = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        \n",
    "        # collect the average of betweenness centrality of 2 nodes\n",
    "        # log to \"dampen\" too small values\n",
    "        avg_pr[i] = np.log(float(pr[ID[src]] + pr[ID[dest]])/2.0)\n",
    "        \n",
    "    return avg_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the average page rank for training set takes 1.6807 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the average pagerank\n",
    "train_avg_pr = avg_pagerank(training, page_rank_g.scores())\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the average page rank for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the average page rank for testing set takes 0.1304 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the average pagerank\n",
    "test_avg_pr = avg_pagerank(testing, page_rank_g.scores())\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the average page rank for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: [-10.75542888  -9.28819986 -11.27892234 -10.79563113 -10.66865404\n",
      " -10.28702328  -7.90007968  -9.70137278  -9.57712104 -10.0708701 ]\n",
      "Testing: [-10.11196423  -8.59294688  -7.70950565  -9.84766778  -9.44479929\n",
      " -10.56567714 -11.30701705 -10.06990187 -11.24745838 -10.83202494]\n"
     ]
    }
   ],
   "source": [
    "print('Training:', train_avg_pr[0:10])\n",
    "print('Testing:', test_avg_pr[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Average of betweenness centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute betweenness centrality of every node in the graph takes 436.9762 s\n"
     ]
    }
   ],
   "source": [
    "# ====== compute betweenness centrality ====== #\n",
    "start = time.time()\n",
    "\n",
    "# use the traditional approach of betweeness computation\n",
    "btwn = nk.centrality.Betweenness(g)\n",
    "btwn.run()\n",
    "\n",
    "end = time.time()\n",
    "print('Compute betweenness centrality of every node in the graph takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_betweeness(ds, btwn):\n",
    "    '''\n",
    "    Compute feature: average in betweenness centrality\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: dataset to compute feature from\n",
    "    g: the graph\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A numpy array of numerical values\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    avg_btw = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        \n",
    "        # collect the average of betweenness centrality of 2 nodes\n",
    "        _avg = float(btwn[ID[src]] + btwn[ID[dest]])\n",
    "        avg_btw[i] = np.log(_avg/2.0) if _avg != 0.0 else 0.0\n",
    "        \n",
    "    return avg_btw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the average betweenness for training set takes 1.9107 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the average degree\n",
    "train_avg_btwn = avg_betweeness(training, btwn.scores())\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the average betweenness for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the average betweenness for testing set takes 0.1240 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the average degree\n",
    "test_avg_btwn = avg_betweeness(testing, btwn.scores())\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the average betweenness for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: [10.08688482 11.16590802  8.84607013  7.65815856  9.07845272  8.44864804\n",
      " 15.30895205 12.10404087 12.35357254 11.4065003 ]\n",
      "Testing: [11.89953033 14.12541191 15.37221353 11.0787389  12.47171402  9.63258213\n",
      "  4.84337128 11.06101491  6.74711568  7.60931935]\n"
     ]
    }
   ],
   "source": [
    "print('Training:', train_avg_btwn[0:10])\n",
    "print('Testing:', test_avg_btwn[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Core Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intuition:** retrieve the core of a network, where many articles are connected to each other. Given a pair of articles, if both are found in the core, they are likely to connect to each other (assign value 1). If one is in the core and one is not, they might be connect to each other (assign value 0.5). Otherwise, they are highly unlikely to connect to each other (value 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core decomposition of the graph takes 0.2839 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "core_decomp = nk.community.CoreDecomposition(g, storeNodeOrder=True)\n",
    "core_decomp.run()\n",
    "cover_g = core_decomp.getCover()\n",
    "order = 20\n",
    "\n",
    "end = time.time()\n",
    "print('Core decomposition of the graph takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = 1\n",
    "# for ss in cover_g.subsetSizes():\n",
    "#     print('Subset of order %d has %d elements' % (idx, ss))\n",
    "#     idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7285 nodes that belong in 20-core decomposition of this graph\n"
     ]
    }
   ],
   "source": [
    "print('There are %d nodes that belong in %d-core decomposition of this graph' \n",
    "      % (len(cover_g.getMembers(order)), order))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_kcore(ds, kcore):\n",
    "    '''\n",
    "    Compute feature: whether a pair of nodes is found in the same k-core graph\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: dataset\n",
    "    kcore: the k-core graph after decomposition as a set of nodes index (ranged from 0 to 27,770)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A numpy array of ordinal values: \n",
    "        - 0 if both nodes are not in the kcores, \n",
    "        - 0.5 if one of them is in the kcores, \n",
    "        - 1 of they are both in the k-core\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    same_kcore = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        \n",
    "        # compute whether two nodes are in the given kcore | one of them is in the kcore | none of them\n",
    "        index_src = ID[src] # index of src\n",
    "        index_dest = ID[dest] # index of dest\n",
    "        \n",
    "        if index_src in kcore and index_dest in kcore:\n",
    "            result = 1.0\n",
    "        elif index_src not in kcore and index_dest not in kcore:\n",
    "            result = 0.0\n",
    "        else:\n",
    "            result = 0.5\n",
    "            \n",
    "        same_kcore[i] = result\n",
    "        \n",
    "    return same_kcore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the in k-core feature for training set takes 1.1343 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the position of two nodes wrt k-core\n",
    "train_in_kcore = in_kcore(training, cover_g.getMembers(order))\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the in k-core feature for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the in k-core feature for testing set takes 0.1085 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the position of two nodes wrt k-core\n",
    "test_in_kcore = in_kcore(testing, cover_g.getMembers(order))\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the in k-core feature for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: [0.  1.  0.  0.  0.5 1.  1.  0.5 1.  0. ]\n",
      "Testing: [0.5 1.  1.  1.  0.5 0.5 0.  0.5 0.  0. ]\n"
     ]
    }
   ],
   "source": [
    "print('Training:', train_in_kcore[0:10])\n",
    "print('Testing:', test_in_kcore[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of selected features\n",
    "features = [\n",
    "    'temporal_difference',\n",
    "    'common_authors',\n",
    "    'same_journal',\n",
    "    'cosine_sim',\n",
    "    'overlapping_title',\n",
    "    'average_degrees',\n",
    "    'common_neighbors',\n",
    "    'jaccard_coefficient',\n",
    "    'avg_pagerank',\n",
    "    'average_betweenness',\n",
    "    'in_kcore'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== create array of training feature ====== #\n",
    "training_features = np.array([\n",
    "    train_temp_diff,\n",
    "    train_common_auth,\n",
    "    train_same_journal,\n",
    "    train_cosine,\n",
    "    train_overlap_title,\n",
    "    train_avg_degrees,\n",
    "    train_common_neigh,\n",
    "    train_jaccard_coeff,\n",
    "    train_avg_pr,\n",
    "    train_avg_btwn,\n",
    "    train_in_kcore\n",
    "]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Saving features (training_features) ====== #\n",
    "with open(path_data + 'training_features.csv', 'w', newline='') as f:\n",
    "    csv_out = csv.writer(f)\n",
    "    csv_out.writerow(features)\n",
    "    for row in training_features:\n",
    "        csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== create array of testing features ====== #\n",
    "testing_features = np.array([\n",
    "    test_temp_diff,\n",
    "    test_common_auth,\n",
    "    test_same_journal,\n",
    "    test_cosine,\n",
    "    test_overlap_title,\n",
    "    test_avg_degrees,\n",
    "    test_common_neigh,\n",
    "    test_jaccard_coeff,\n",
    "    test_avg_pr,\n",
    "    test_avg_btwn,\n",
    "    test_in_kcore\n",
    "]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Saving features (testing_features) ====== #\n",
    "with open(path_data + 'testing_features.csv', 'w', newline='') as f:\n",
    "    csv_out = csv.writer(f)\n",
    "    csv_out.writerow(features)\n",
    "    for row in testing_features:\n",
    "        csv_out.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
