{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries and Utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\t\t\t<script type=\"text/javascript\">\n",
       "\t\t\t<!--\n",
       "\t\t\t\t\n",
       "\t\t\t{\n",
       "\t\t\t\tvar element = document.getElementById('NetworKit_script');\n",
       "\t\t\t\tif (element) {\n",
       "\t\t\t\t\telement.parentNode.removeChild(element);\n",
       "\t\t\t\t}\n",
       "\t\t\t\telement = document.createElement('script');\n",
       "\t\t\t\telement.type = 'text/javascript';\n",
       "\t\t\t\telement.innerHTML = 'function NetworKit_pageEmbed(id) { var i, j; var elements; elements = document.getElementById(id).getElementsByClassName(\"Plot\"); for (i=0; i<elements.length; i++) { elements[i].id = id + \"_Plot_\" + i; var data = elements[i].getAttribute(\"data-image\").split(\"|\"); elements[i].removeAttribute(\"data-image\"); var content = \"<div class=\\\\\"Image\\\\\" id=\\\\\"\" + elements[i].id + \"_Image\\\\\" />\"; elements[i].innerHTML = content; elements[i].setAttribute(\"data-image-index\", 0); elements[i].setAttribute(\"data-image-length\", data.length); for (j=0; j<data.length; j++) { elements[i].setAttribute(\"data-image-\" + j, data[j]); } NetworKit_plotUpdate(elements[i]); elements[i].onclick = function (e) { NetworKit_overlayShow((e.target) ? e.target : e.srcElement); } } elements = document.getElementById(id).getElementsByClassName(\"HeatCell\"); for (i=0; i<elements.length; i++) { var data = parseFloat(elements[i].getAttribute(\"data-heat\")); var color = \"#00FF00\"; if (data <= 1 && data > 0) { color = \"hsla(0, 100%, 75%, \" + (data) + \")\"; } else if (data <= 0 && data >= -1) { color = \"hsla(240, 100%, 75%, \" + (-data) + \")\"; } elements[i].style.backgroundColor = color; } elements = document.getElementById(id).getElementsByClassName(\"Details\"); for (i=0; i<elements.length; i++) { elements[i].setAttribute(\"data-title\", \"-\"); NetworKit_toggleDetails(elements[i]); elements[i].onclick = function (e) { NetworKit_toggleDetails((e.target) ? e.target : e.srcElement); } } elements = document.getElementById(id).getElementsByClassName(\"MathValue\"); for (i=elements.length-1; i>=0; i--) { value = elements[i].innerHTML.trim(); if (value === \"nan\") { elements[i].parentNode.innerHTML = \"\" } } elements = document.getElementById(id).getElementsByClassName(\"SubCategory\"); for (i=elements.length-1; i>=0; i--) { value = elements[i].innerHTML.trim(); if (value === \"\") { elements[i].parentNode.removeChild(elements[i]) } } elements = document.getElementById(id).getElementsByClassName(\"Category\"); for (i=elements.length-1; i>=0; i--) { value = elements[i].innerHTML.trim(); if (value === \"\") { elements[i].parentNode.removeChild(elements[i]) } } var isFirefox = false; try { isFirefox = typeof InstallTrigger !== \"undefined\"; } catch (e) {} if (!isFirefox) { alert(\"Currently the function\\'s output is only fully supported by Firefox.\"); } } function NetworKit_plotUpdate(source) { var index = source.getAttribute(\"data-image-index\"); var data = source.getAttribute(\"data-image-\" + index); var image = document.getElementById(source.id + \"_Image\"); image.style.backgroundImage = \"url(\" + data + \")\"; } function NetworKit_showElement(id, show) { var element = document.getElementById(id); element.style.display = (show) ? \"block\" : \"none\"; } function NetworKit_overlayShow(source) { NetworKit_overlayUpdate(source); NetworKit_showElement(\"NetworKit_Overlay\", true); } function NetworKit_overlayUpdate(source) { document.getElementById(\"NetworKit_Overlay_Title\").innerHTML = source.title; var index = source.getAttribute(\"data-image-index\"); var data = source.getAttribute(\"data-image-\" + index); var image = document.getElementById(\"NetworKit_Overlay_Image\"); image.setAttribute(\"data-id\", source.id); image.style.backgroundImage = \"url(\" + data + \")\"; var link = document.getElementById(\"NetworKit_Overlay_Toolbar_Bottom_Save\"); link.href = data; link.download = source.title + \".svg\"; } function NetworKit_overlayImageShift(delta) { var image = document.getElementById(\"NetworKit_Overlay_Image\"); var source = document.getElementById(image.getAttribute(\"data-id\")); var index = parseInt(source.getAttribute(\"data-image-index\")); var length = parseInt(source.getAttribute(\"data-image-length\")); var index = (index+delta) % length; if (index < 0) { index = length + index; } source.setAttribute(\"data-image-index\", index); NetworKit_overlayUpdate(source); } function NetworKit_toggleDetails(source) { var childs = source.children; var show = false; if (source.getAttribute(\"data-title\") == \"-\") { source.setAttribute(\"data-title\", \"+\"); show = false; } else { source.setAttribute(\"data-title\", \"-\"); show = true; } for (i=0; i<childs.length; i++) { if (show) { childs[i].style.display = \"block\"; } else { childs[i].style.display = \"none\"; } } }';\n",
       "\t\t\t\telement.setAttribute('id', 'NetworKit_script');\n",
       "\t\t\t\tdocument.head.appendChild(element);\n",
       "\t\t\t}\n",
       "\t\t\n",
       "\t\t\t\t\n",
       "\t\t\t{\n",
       "\t\t\t\tvar element = document.getElementById('NetworKit_style');\n",
       "\t\t\t\tif (element) {\n",
       "\t\t\t\t\telement.parentNode.removeChild(element);\n",
       "\t\t\t\t}\n",
       "\t\t\t\telement = document.createElement('style');\n",
       "\t\t\t\telement.type = 'text/css';\n",
       "\t\t\t\telement.innerHTML = '.NetworKit_Page { font-family: Arial, Helvetica, sans-serif; font-size: 14px; } .NetworKit_Page .Value:before { font-family: Arial, Helvetica, sans-serif; font-size: 1.05em; content: attr(data-title) \":\"; margin-left: -2.5em; padding-right: 0.5em; } .NetworKit_Page .Details .Value:before { display: block; } .NetworKit_Page .Value { font-family: monospace; white-space: pre; padding-left: 2.5em; white-space: -moz-pre-wrap !important; white-space: -pre-wrap; white-space: -o-pre-wrap; white-space: pre-wrap; word-wrap: break-word; tab-size: 4; -moz-tab-size: 4; } .NetworKit_Page .Category { clear: both; padding-left: 1em; margin-bottom: 1.5em; } .NetworKit_Page .Category:before { content: attr(data-title); font-size: 1.75em; display: block; margin-left: -0.8em; margin-bottom: 0.5em; } .NetworKit_Page .SubCategory { margin-bottom: 1.5em; padding-left: 1em; } .NetworKit_Page .SubCategory:before { font-size: 1.6em; display: block; margin-left: -0.8em; margin-bottom: 0.5em; } .NetworKit_Page .SubCategory[data-title]:before { content: attr(data-title); } .NetworKit_Page .Block { display: block; } .NetworKit_Page .Block:after { content: \".\"; visibility: hidden; display: block; height: 0; clear: both; } .NetworKit_Page .Block .Thumbnail_Overview, .NetworKit_Page .Block .Thumbnail_ScatterPlot { width: 260px; float: left; } .NetworKit_Page .Block .Thumbnail_Overview img, .NetworKit_Page .Block .Thumbnail_ScatterPlot img { width: 260px; } .NetworKit_Page .Block .Thumbnail_Overview:before, .NetworKit_Page .Block .Thumbnail_ScatterPlot:before { display: block; text-align: center; font-weight: bold; } .NetworKit_Page .Block .Thumbnail_Overview:before { content: attr(data-title); } .NetworKit_Page .HeatCell { font-family: \"Courier New\", Courier, monospace; cursor: pointer; } .NetworKit_Page .HeatCell, .NetworKit_Page .HeatCellName { display: inline; padding: 0.1em; margin-right: 2px; background-color: #FFFFFF } .NetworKit_Page .HeatCellName { margin-left: 0.25em; } .NetworKit_Page .HeatCell:before { content: attr(data-heat); display: inline-block; color: #000000; width: 4em; text-align: center; } .NetworKit_Page .Measure { clear: both; } .NetworKit_Page .Measure .Details { cursor: pointer; } .NetworKit_Page .Measure .Details:before { content: \"[\" attr(data-title) \"]\"; display: block; } .NetworKit_Page .Measure .Details .Value { border-left: 1px dotted black; margin-left: 0.4em; padding-left: 3.5em; pointer-events: none; } .NetworKit_Page .Measure .Details .Spacer:before { content: \".\"; opacity: 0.0; pointer-events: none; } .NetworKit_Page .Measure .Plot { width: 440px; height: 440px; cursor: pointer; float: left; margin-left: -0.9em; margin-right: 20px; } .NetworKit_Page .Measure .Plot .Image { background-repeat: no-repeat; background-position: center center; background-size: contain; height: 100%; pointer-events: none; } .NetworKit_Page .Measure .Stat { width: 500px; float: left; } .NetworKit_Page .Measure .Stat .Group { padding-left: 1.25em; margin-bottom: 0.75em; } .NetworKit_Page .Measure .Stat .Group .Title { font-size: 1.1em; display: block; margin-bottom: 0.3em; margin-left: -0.75em; border-right-style: dotted; border-right-width: 1px; border-bottom-style: dotted; border-bottom-width: 1px; background-color: #D0D0D0; padding-left: 0.2em; } .NetworKit_Page .Measure .Stat .Group .List { -webkit-column-count: 3; -moz-column-count: 3; column-count: 3; } .NetworKit_Page .Measure .Stat .Group .List .Entry { position: relative; line-height: 1.75em; } .NetworKit_Page .Measure .Stat .Group .List .Entry[data-tooltip]:before { position: absolute; left: 0; top: -40px; background-color: #808080; color: #ffffff; height: 30px; line-height: 30px; border-radius: 5px; padding: 0 15px; content: attr(data-tooltip); white-space: nowrap; display: none; } .NetworKit_Page .Measure .Stat .Group .List .Entry[data-tooltip]:after { position: absolute; left: 15px; top: -10px; border-top: 7px solid #808080; border-left: 7px solid transparent; border-right: 7px solid transparent; content: \"\"; display: none; } .NetworKit_Page .Measure .Stat .Group .List .Entry[data-tooltip]:hover:after, .NetworKit_Page .Measure .Stat .Group .List .Entry[data-tooltip]:hover:before { display: block; } .NetworKit_Page .Measure .Stat .Group .List .Entry .MathValue { font-family: \"Courier New\", Courier, monospace; } .NetworKit_Page .Measure:after { content: \".\"; visibility: hidden; display: block; height: 0; clear: both; } .NetworKit_Page .PartitionPie { clear: both; } .NetworKit_Page .PartitionPie img { width: 600px; } #NetworKit_Overlay { left: 0px; top: 0px; display: none; position: absolute; width: 100%; height: 100%; background-color: rgba(0,0,0,0.6); z-index: 1000; } #NetworKit_Overlay_Title { position: absolute; color: white; transform: rotate(-90deg); width: 32em; height: 32em; padding-right: 0.5em; padding-top: 0.5em; text-align: right; font-size: 40px; } #NetworKit_Overlay .button { background: white; cursor: pointer; } #NetworKit_Overlay .button:before { size: 13px; display: inline-block; text-align: center; margin-top: 0.5em; margin-bottom: 0.5em; width: 1.5em; height: 1.5em; } #NetworKit_Overlay .icon-close:before { content: \"X\"; } #NetworKit_Overlay .icon-previous:before { content: \"P\"; } #NetworKit_Overlay .icon-next:before { content: \"N\"; } #NetworKit_Overlay .icon-save:before { content: \"S\"; } #NetworKit_Overlay_Toolbar_Top, #NetworKit_Overlay_Toolbar_Bottom { position: absolute; width: 40px; right: 13px; text-align: right; z-index: 1100; } #NetworKit_Overlay_Toolbar_Top { top: 0.5em; } #NetworKit_Overlay_Toolbar_Bottom { Bottom: 0.5em; } #NetworKit_Overlay_ImageContainer { position: absolute; top: 5%; left: 5%; height: 90%; width: 90%; background-repeat: no-repeat; background-position: center center; background-size: contain; } #NetworKit_Overlay_Image { height: 100%; width: 100%; background-repeat: no-repeat; background-position: center center; background-size: contain; }';\n",
       "\t\t\t\telement.setAttribute('id', 'NetworKit_style');\n",
       "\t\t\t\tdocument.head.appendChild(element);\n",
       "\t\t\t}\n",
       "\t\t\n",
       "\t\t\t\t\n",
       "\t\t\t{\n",
       "\t\t\t\tvar element = document.getElementById('NetworKit_Overlay');\n",
       "\t\t\t\tif (element) {\n",
       "\t\t\t\t\telement.parentNode.removeChild(element);\n",
       "\t\t\t\t}\n",
       "\t\t\t\telement = document.createElement('div');\n",
       "\t\t\t\telement.innerHTML = '<div id=\"NetworKit_Overlay_Toolbar_Top\"><div class=\"button icon-close\" id=\"NetworKit_Overlay_Close\" /></div><div id=\"NetworKit_Overlay_Title\" /> <div id=\"NetworKit_Overlay_ImageContainer\"> <div id=\"NetworKit_Overlay_Image\" /> </div> <div id=\"NetworKit_Overlay_Toolbar_Bottom\"> <div class=\"button icon-previous\" onclick=\"NetworKit_overlayImageShift(-1)\" /> <div class=\"button icon-next\" onclick=\"NetworKit_overlayImageShift(1)\" /> <a id=\"NetworKit_Overlay_Toolbar_Bottom_Save\"><div class=\"button icon-save\" /></a> </div>';\n",
       "\t\t\t\telement.setAttribute('id', 'NetworKit_Overlay');\n",
       "\t\t\t\tdocument.body.appendChild(element);\n",
       "\t\t\t\tdocument.getElementById('NetworKit_Overlay_Close').onclick = function (e) {\n",
       "\t\t\t\t\tdocument.getElementById('NetworKit_Overlay').style.display = 'none';\n",
       "\t\t\t\t}\n",
       "\t\t\t}\n",
       "\t\t\n",
       "\t\t\t-->\n",
       "\t\t\t</script>\n",
       "\t\t"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:summarizer.preprocessing.cleaner:'pattern' package not found; tag filters are not available for English\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing libraries and setting up parameters takes 10.2657 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "import networkit as nk\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn import preprocessing as pre\n",
    "\n",
    "# working with text\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "import scipy as sc\n",
    "\n",
    "end = time.time()\n",
    "print('Importing libraries and setting up parameters takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(nodes, edges):\n",
    "    g = nk.Graph(len(nodes)) # adding nodes\n",
    "\n",
    "    for edge in edges:\n",
    "        if not g.hasEdge(edge[0], edge[1]): # avoid multiple edges\n",
    "            g.addEdge(edge[0], edge[1])\n",
    "            \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to preprocess text\n",
    "def preprocess(text, dg_removal=True, sw_removal=True, stemming=True):\n",
    "    '''\n",
    "    Preprocess text: stopword removal, stemming, digit removal\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text: text on which preprocessing is applied\n",
    "    dg_removal: whether to apply digit removal or not\n",
    "    sw_removal: whether to apply stopword removal or not\n",
    "    stemming: whether to apply stemming or not\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    the text after preprocessing\n",
    "    '''\n",
    "    result = text\n",
    "    \n",
    "    sw = set(nltk.corpus.stopwords.words('english')) # set of stopwords\n",
    "    stemmer = nltk.stem.PorterStemmer() # stemmer\n",
    "    \n",
    "    if dg_removal:\n",
    "        result = re.sub('[0-9]', '', result)\n",
    "    \n",
    "    if sw_removal:\n",
    "        result = ' '.join([token for token in result.split() if token not in sw])\n",
    "        \n",
    "    if stemming:\n",
    "        result = ' '.join([stemmer.stem(token) for token in result.split()])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '../data/' # path to the data\n",
    "path_submission = '../submission/' # path to submission files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading node information takes 0.3039 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== read in node informations ====== #\n",
    "with open(path_data + 'node_information.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    node_info = list(reader)\n",
    "\n",
    "end = time.time()\n",
    "print('Reading node information takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training set takes 2.7073 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== read training data as str ====== #\n",
    "training = np.genfromtxt(path_data + 'training_set.txt', dtype=str)\n",
    "\n",
    "end = time.time()\n",
    "print('Reading training set takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading testing set takes 0.1402 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== read testing data as str ====== #\n",
    "testing = np.genfromtxt(path_data + 'testing_set.txt', dtype=str)\n",
    "\n",
    "end = time.time()\n",
    "print('Reading testing set takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the citation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the citation graph takes 177.7908 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== build the graph ====== #\n",
    "\n",
    "nodes = [element[0] for element in node_info] # create index list to be passed as nodes\n",
    "edges = [(nodes.index(element[0]), nodes.index(element[1])) for element in training if element[2] == '1']\n",
    "g = build_graph(nodes, edges)\n",
    "\n",
    "end = time.time()\n",
    "print('Building the citation graph takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vertices: 27770\n",
      "Number of edges (after multiple edges removal): 334690\n"
     ]
    }
   ],
   "source": [
    "# check for general information of the graph\n",
    "print('Number of vertices: %d' % g.numberOfNodes())\n",
    "print('Number of edges (after multiple edges removal): %d' % g.numberOfEdges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of features is described as follows, and the computation rule is the same for both training and testing set.\n",
    "\n",
    "| Feature                | Explanation                                                        | Type      | Range   |\n",
    "|:----------------------:|:------------------------------------------------------------------:|:---------:|:-------:|\n",
    "| Temporal difference    | Difference in publication year (absolute value)                    | numerical | $\\ge$ 0 |\n",
    "| Common authors         | The number of common authors between two articles                  | numerical | $\\ge$ 0 |\n",
    "| Same journal           | Whether two articles are published in the same journal             | binary    | 0, 1    |\n",
    "| Cosine similarity      | Cosine similarity between word vectors of abstracts                | numerical | [0,1]   |\n",
    "| Title overlap          | Number of overlapping words in title                               | numerical | $\\ge$ 0 |\n",
    "| Degree difference      | Difference in measure of degrees of two nodes (absolute value)     | numerical | $\\ge$ 0 |\n",
    "| Common neighbors       | Number of common neighbors                                         | numercial | $\\ge$ 0 |\n",
    "| Jaccard coefficient    | Link-based Jaccard coefficient                                     | numerical | [0,1]   |\n",
    "| Same cluster           | Check whether two nodes are in the same cluster                    | binary    | [0,1]   |\n",
    "| PageRank difference    | Difference in PageRank index of two nodes (absolute value)         | numerical | $\\ge$ 0 |\n",
    "| Betweenness centrality | Difference in betweenness centrality of two nodes (absolute value) | numerical | $\\ge$ 0 |\n",
    "| In the same k-core     | Whether both nodes/one of them/none of them are in the same k-core | ordinal   |[0,0.5,1]|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the dictionary of (ID-index) to accelerate access to node'ID\n",
    "ID = dict(zip(nodes, [nodes.index(n) for n in nodes]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Temporal difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_difference(ds):\n",
    "    '''\n",
    "    Compute feature: Difference in publication year\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: the dataset to compute\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A numpy array where each entry corresponds to the temporal difference of a pair of nodes\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    temp_diff = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        src_info, dest_info = node_info[ID[src]], node_info[ID[dest]] # get the associated node information by index\n",
    "        \n",
    "        # compute the difference in publication year in absolute value (because we don't know which one cites the other)\n",
    "        temp_diff[i] = abs(\n",
    "            int(src_info[1]) - int(dest_info[1])\n",
    "        )\n",
    "        \n",
    "    return temp_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing temporal difference for training set takes 1.6704 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the temporal difference\n",
    "train_temp_diff = temporal_difference(training)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing temporal difference for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing temporal difference for testing set takes 0.1156 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the temporal difference\n",
    "test_temp_diff = temporal_difference(testing)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing temporal difference for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: [0. 1. 2. 4. 5. 0. 4. 7. 0. 8.]\n",
      "Testing: [0. 1. 2. 0. 5. 4. 0. 1. 7. 0.]\n"
     ]
    }
   ],
   "source": [
    "print('Training:', train_temp_diff[0:10])\n",
    "print('Testing:', test_temp_diff[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Number of common authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_authors(ds):\n",
    "    '''\n",
    "    Compute feature: number of common authors\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: dataset to compute feature from\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A numpy array\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    common_auth = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        src_info, dest_info = node_info[ID[src]], node_info[ID[dest]] # get the associated node information by index\n",
    "        \n",
    "        # compute the difference in publication year in absolute value (because we don't know which one cites the other)\n",
    "        common_auth[i] = len(\n",
    "            set(src_info[3].split(','))\n",
    "            .intersection(set(dest_info[3].split(',')))\n",
    "        )\n",
    "        \n",
    "        \n",
    "    return common_auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the number of common authors for training set takes 2.5344 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the temporal difference\n",
    "train_common_auth = common_authors(training)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the number of common authors for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the number of common authors for testing set takes 0.1583 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the temporal difference\n",
    "test_common_auth = common_authors(testing)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the number of common authors for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "Testing: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print('Training:', train_common_auth[0:10])\n",
    "print('Testing:', test_common_auth[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Same journal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_journal(ds):\n",
    "    '''\n",
    "    Compute feature: whether two articles are published in the same journal\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: dataset to compute feature from\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A numpy array of binary values (0|1)\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    same_journal = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        src_info, dest_info = node_info[ID[src]], node_info[ID[dest]] # get the associated node information by index\n",
    "        \n",
    "        # 1 if two articles are published in the same journal, 0 otherwise\n",
    "        same_journal[i] = int(\n",
    "            len(src_info[4])>0 and  # journal info of source not null\n",
    "            len(dest_info[4])>0 and # journal info of dest not null\n",
    "            src_info[4] == dest_info[4] # the same journal title\n",
    "        )\n",
    "        \n",
    "        \n",
    "    return same_journal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing whether two articles are published in the same journal for training set takes 1.4511 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the temporal difference\n",
    "train_same_journal = same_journal(training)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing whether two articles are published in the same journal for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing whether two articles are published in the same journal for testing set takes 0.1035 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the temporal difference\n",
    "test_same_journal = same_journal(testing)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing whether two articles are published in the same journal for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Testing: [0. 0. 1. 1. 0. 0. 0. 1. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print('Training:', train_same_journal[0:10])\n",
    "print('Testing:', test_same_journal[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cosine similarity in title + abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Cosine similarity with TD-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the corpus with preprocessing on words takes 41.8818 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== corpus is the set of titles + abstracts, apply preprocessing to each article ======#\n",
    "\n",
    "# build the corpus\n",
    "#nltk.download('stopwords') # uncomment if haven't downloaded stopwords\n",
    "corpus = [preprocess(element[2] + ' ' + element[5], dg_removal=True, sw_removal=True, stemming=True) \n",
    "          for element in node_info]\n",
    "\n",
    "end = time.time()\n",
    "print('Building the corpus with preprocessing on words takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix of shape: (27770, 17080)\n",
      "Building TF-IDF matrix takes 1.4731 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english') # create a TF-IDF vectorizer\n",
    "tfidf = vectorizer.fit_transform(corpus) # TD-IDF matrix of the entire corpus (set of abstracts)\n",
    "print('TF-IDF matrix of shape:', tfidf.shape)\n",
    "\n",
    "end = time.time()\n",
    "print('Building TF-IDF matrix takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim_text(ds, vectors, is_w2v):\n",
    "    '''\n",
    "    Compute feature: cosine similarity in title and abstract, using normal cosine similarity on TF-IDF\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: dataset to compute feature from\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A numpy array of cosine values\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    cosines = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        \n",
    "        # collect the cosine similarity\n",
    "        src_vect, dest_vect = vectors[ID[src]], vectors[ID[dest]] # get the corresponding vector in TD-IDF matrix\n",
    "\n",
    "        # compute cosine similarity\n",
    "        cos = 1 - sc.spatial.distance.cosine(src_vect, dest_vect) if is_w2v else cosine_similarity(src_vect, dest_vect)\n",
    "        \n",
    "        cosines[i] = cos\n",
    "        \n",
    "    return cosines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing cosine similarity for training set takes 372.6914 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the cosine similarity\n",
    "train_cosine = cosine_sim_text(training, tfidf, False)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing cosine similarity for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing cosine similarity for testing set takes 19.3335 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the cosine similarity\n",
    "test_cosine = cosine_sim_text(testing, tfidf, False)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing cosine similarity for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: [0.19996622 0.06436945 0.02053711 0.05937844 0.09852643 0.39581923\n",
      " 0.18722569 0.08627054 0.04181436 0.06044751]\n",
      "Testing: [0.11804009 0.30786265 0.20753805 0.16112407 0.31824453 0.03466872\n",
      " 0.02490266 0.19991048 0.         0.3283665 ]\n"
     ]
    }
   ],
   "source": [
    "print('Training:', train_cosine[0:10])\n",
    "print('Testing:', test_cosine[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Cosine similarity with word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.utils_any2vec:loading projection weights from ../data/GoogleNews-vectors-negative300.bin.gz\n",
      "INFO:gensim.models.utils_any2vec:loaded (3000000, 300) matrix from ../data/GoogleNews-vectors-negative300.bin.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word2vec of google takes 141.5187 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# reading google vector\n",
    "google_vecs = KeyedVectors.load_word2vec_format(path_data + 'GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "\n",
    "end = time.time()\n",
    "print('Loading word2vec of google takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/huong/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "Building documents for word2vec training takes 15.1954 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# building documents for word2vec training\n",
    "train_tag = []\n",
    "total = len(corpus)\n",
    "processed = 0\n",
    "i = 0\n",
    "#nltk.download('punkt') # uncomment if package 'punkt' not already downloaded\n",
    "for x in corpus:\n",
    "    words = []\n",
    "    sentences = sent_tokenize(x)\n",
    "    for s in sentences:\n",
    "        words.extend(word_tokenize(s)) \n",
    "    doc = words\n",
    "    i = i+1\n",
    "    train_tag.append(doc)\n",
    "\n",
    "end = time.time()\n",
    "print('Building documents for word2vec training takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec(x_data,vector):\n",
    "    print (\"Loading GoogleNews-vectors-negative300.bin\")\n",
    "    google_vecs = vector\n",
    "    print (\"GoogleNews-vectors-negative300.bin loaded\")\n",
    "    \n",
    "    print (\"Averaging Word Embeddings...\")\n",
    "    x_data_embeddings = []\n",
    "    total = len(x_data)\n",
    "    for tagged_plot in x_data:\n",
    "        count = 0  \n",
    "        doc_vector = np.zeros(300)\n",
    "        for sentence in tagged_plot:\n",
    "            try:\n",
    "                doc_vector += google_vecs[sentence]\n",
    "            except KeyError:\n",
    "                doc_vector += [0.0]*300\n",
    "                continue\n",
    "\n",
    "        x_data_embeddings.append(doc_vector)\n",
    "            \n",
    "    return np.array(x_data_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GoogleNews-vectors-negative300.bin\n",
      "GoogleNews-vectors-negative300.bin loaded\n",
      "Averaging Word Embeddings...\n",
      "Word embeddings of shape: (27770, 300)\n",
      "Embedding words takes 12.8759 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# get word embessings\n",
    "x_embeddings = word2vec(train_tag, google_vecs)\n",
    "print('Word embeddings of shape:', x_embeddings.shape)\n",
    "\n",
    "end = time.time()\n",
    "print('Embedding words takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huong/.local/lib/python3.5/site-packages/scipy/spatial/distance.py:644: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing cosine similarity using word embedding for training features takes 20.3391 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute cosine similarity with word embeddings\n",
    "train_cosine_sim_w2v = cosine_sim_text(training, x_embeddings, True)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing cosine similarity using word embedding for training features takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huong/.local/lib/python3.5/site-packages/scipy/spatial/distance.py:644: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing cosine similarity using word embedding for testing features takes 1.2045 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute cosine similarity with word embeddings\n",
    "test_cosine_sim_w2v = cosine_sim_text(testing, x_embeddings, True)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing cosine similarity using word embedding for testing features takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: [0.76839412 0.64235938 0.76174991 0.73752149 0.7232033  0.79878754\n",
      " 0.79923392 0.68025815 0.4998241  0.59656393]\n",
      "Testing: [0.72750916 0.69251008 0.61316547 0.60472135 0.37674628 0.6143832\n",
      " 0.43079299 0.70911597 0.40506944 0.65525613]\n"
     ]
    }
   ],
   "source": [
    "print('Training:', train_cosine_sim_w2v[0:10])\n",
    "print('Testing:', test_cosine_sim_w2v[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Number of overlapped words in title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap_title(ds):\n",
    "    '''\n",
    "    Compute feature: number of overlapping words in the title\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: dataset to compute feature from\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A numpy array of numerical values\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    overlap_title = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        src_info, dest_info = node_info[ID[src]], node_info[ID[dest]] # get the associated node information by index\n",
    "        \n",
    "        # collect the number of overlapping words in title\n",
    "        src_title, dest_title = preprocess(src_info[2]).split(), preprocess(dest_info[2]).split()\n",
    "        overlap_title[i] = len(\n",
    "            set(src_title)\n",
    "            .intersection(set(dest_title))\n",
    "        )\n",
    "        \n",
    "    return overlap_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing number of overlapping words in title for training set takes 616.3972 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the number of overlapping words in title\n",
    "train_overlap_title = overlap_title(training)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing number of overlapping words in title for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing number of overlapping words in title for testing set takes 32.3066 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the number of overlapping words in title\n",
    "test_overlap_title = overlap_title(testing)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing number of overlapping words in title for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: [2. 1. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Testing: [0. 2. 1. 1. 0. 0. 1. 1. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print('Training:', train_overlap_title[0:10])\n",
    "print('Testing:', test_overlap_title[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Maximum degree of a pair of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_degrees(ds, g):\n",
    "    '''\n",
    "    Compute feature: <Maximum degrees among 2 nodes\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: dataset to compute feature from\n",
    "    g: the graph\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A numpy array of numerical values\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    avg_degree = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        \n",
    "        # collect the number of overlapping words in title\n",
    "        src_deg = g.degree(ID[src])\n",
    "        dest_deg = g.degree(ID[dest])\n",
    "        avg_degree[i] = max(src_deg, dest_deg)\n",
    "        #avg_degree[i] = float(src_deg + dest_deg)/2.0\n",
    "        \n",
    "    return avg_degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the by-pair maximum degree for training set takes 2.4667 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the average degree\n",
    "train_degrees = average_degrees(training, g)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the by-pair maximum degree for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the by-pair maximum degree for testing set takes 0.1312 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the average degree\n",
    "test_degrees = average_degrees(testing, g)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the by-pair maximum degree for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: [ 12. 147.   5.  20.  24.  38. 739.  86. 100.  30.]\n",
      "Testing: [ 59. 302. 739.  65. 150.  35.   4.  42.   7.  13.]\n"
     ]
    }
   ],
   "source": [
    "print('Training:', train_degrees[0:10])\n",
    "print('Testing:', test_degrees[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Number of common neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_neighbors(ds, g):\n",
    "    '''\n",
    "    Compute feature: The number of common neighbors\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: dataset to compute feature from\n",
    "    g: the graph\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A numpy array of numerical values\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    common_neigh = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        \n",
    "        # collect the number of overlapping words in title\n",
    "        common_neigh[i] = len(\n",
    "            set(g.neighbors(ID[src]))\n",
    "            .intersection(set(g.neighbors(ID[dest])))\n",
    "        )\n",
    "        \n",
    "    return common_neigh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the number of common neighbors for training set takes 8.2263 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the average degree\n",
    "train_common_neigh = common_neighbors(training, g)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the number of common neighbors for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the number of common neighbors for testing set takes 0.4318 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the average degree\n",
    "test_common_neigh = common_neighbors(testing, g)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the number of common neighbors for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: [ 1. 20.  0.  0.  0. 14. 12.  0.  5.  0.]\n",
      "Testing: [ 0. 24. 59. 21.  0.  0.  0.  6.  0.  4.]\n"
     ]
    }
   ],
   "source": [
    "print('Training:', train_common_neigh[0:10])\n",
    "print('Testing:', test_common_neigh[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Link-based Jaccard coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_coeff(ds, g):\n",
    "    '''\n",
    "    Compute feature: Link-based Jaccard coefficient\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: dataset to compute feature from\n",
    "    g: the graph\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A numpy array of numerical values\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    coeff = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        \n",
    "        # collect the number of overlapping words in title\n",
    "        inters = len(\n",
    "            set(g.neighbors(ID[src]))\n",
    "            .intersection(set(g.neighbors(ID[dest])))\n",
    "        ) # intersection of neighbors\n",
    "        \n",
    "        union = len(\n",
    "            set(g.neighbors(ID[src]))\n",
    "            .union(set(g.neighbors(ID[dest])))\n",
    "        ) # union of neighbors\n",
    "        \n",
    "        coeff[i] = (float(inters)/float(union) if union != 0 else 0)\n",
    "        \n",
    "    return coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing link-based Jaccard coefficient for training set takes 19.7526 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the average degree\n",
    "train_jaccard_coeff = jaccard_coeff(training, g)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing link-based Jaccard coefficient for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing link-based Jaccard coefficient for testing set takes 1.0697 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the average degree\n",
    "test_jaccard_coeff = jaccard_coeff(testing, g)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing link-based Jaccard coefficient for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: [0.05882353 0.09708738 0.         0.         0.         0.23728814\n",
      " 0.01522843 0.         0.03676471 0.        ]\n",
      "Testing: [0.         0.07430341 0.06533776 0.22105263 0.         0.\n",
      " 0.         0.10526316 0.         0.19047619]\n"
     ]
    }
   ],
   "source": [
    "print('Training:', train_jaccard_coeff[0:10])\n",
    "print('Testing:', test_jaccard_coeff[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Maximum by pair of PageRank index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the PageRank index of the graph takes 0.2039 s\n"
     ]
    }
   ],
   "source": [
    "# ====== compute PageRank index ====== #\n",
    "start = time.time()\n",
    "\n",
    "page_rank_g = nk.centrality.PageRank(g)\n",
    "page_rank_g.run()\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the PageRank index of the graph takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pagerank(ds, pr):\n",
    "    '''\n",
    "    Compute feature: average of pagerank\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: dataset to compute feature from\n",
    "    g: the graph\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A numpy array of numerical values\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    max_pr = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        \n",
    "        # collect the average of betweenness centrality of 2 nodes\n",
    "        # log to \"dampen\" too small values\n",
    "        max_pr[i] = np.log(max(pr[ID[src]], pr[ID[dest]]))\n",
    "        \n",
    "    return max_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the by-pair maximum page rank for training set takes 1.7328 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the average pagerank\n",
    "train_pagerank = max_pagerank(training, page_rank_g.scores())\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the by-pair maximum page rank for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the by-pair maximum page rank for testing set takes 0.1327 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the average pagerank\n",
    "test_pagerank = max_pagerank(testing, page_rank_g.scores())\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the by-pair maximum page rank for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: [-10.4952174   -9.04507102 -11.03082025 -10.67156463 -10.4743869\n",
      " -10.27890873  -7.28060674  -9.25626085  -9.19986647  -9.55846355]\n",
      "Testing: [ -9.71123221  -8.05014677  -7.28060674  -9.78108665  -8.84688464\n",
      " -10.35156937 -11.13700203  -9.81535402 -10.94645343 -10.8153264 ]\n"
     ]
    }
   ],
   "source": [
    "print('Training:', train_pagerank[0:10])\n",
    "print('Testing:', test_pagerank[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Maximum of betweenness centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute betweenness centrality of every node in the graph takes 403.0792 s\n"
     ]
    }
   ],
   "source": [
    "# ====== compute betweenness centrality ====== #\n",
    "start = time.time()\n",
    "\n",
    "# use the traditional approach of betweeness computation\n",
    "btwn = nk.centrality.Betweenness(g)\n",
    "btwn.run()\n",
    "\n",
    "end = time.time()\n",
    "print('Compute betweenness centrality of every node in the graph takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_betweeness(ds, btwn):\n",
    "    '''\n",
    "    Compute feature: average in betweenness centrality\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: dataset to compute feature from\n",
    "    g: the graph\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A numpy array of numerical values\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    max_btw = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        \n",
    "        # collect the average of betweenness centrality of 2 nodes\n",
    "        _max = max(btwn[ID[src]], btwn[ID[dest]])\n",
    "        max_btw[i] = np.log(_max) if _max != 0 else 0.0\n",
    "        \n",
    "        #_avg = float(btwn[ID[src]] + btwn[ID[dest]])\n",
    "        #avg_btw[i] = np.log(_avg/2.0) if _avg != 0.0 else 0.0\n",
    "        \n",
    "    return max_btw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the average betweenness for training set takes 4.0639 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the average degree\n",
    "train_btwn = max_betweeness(training, btwn.scores())\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the average betweenness for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the average betweenness for testing set takes 0.2102 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the average degree\n",
    "test_btwn = max_betweeness(testing, btwn.scores())\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the average betweenness for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: [10.5093708  11.39325368  9.53921731  8.04019582  9.38724288  8.87265376\n",
      " 15.9955066  12.63640921 13.02476499 11.99064799]\n",
      "Testing: [12.58866497 14.77807021 15.9955066  11.20495121 13.16245853 10.26706831\n",
      "  5.53651846 11.07954426  7.44026286  8.02905778]\n"
     ]
    }
   ],
   "source": [
    "print('Training:', train_btwn[0:10])\n",
    "print('Testing:', test_btwn[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Core Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intuition:** retrieve the core of a network, where many articles are connected to each other. Given a pair of articles, if both are found in the core, they are likely to connect to each other (assign value 1). If one is in the core and one is not, they might be connect to each other (assign value 0.5). Otherwise, they are highly unlikely to connect to each other (value 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core decomposition of the graph takes 0.1228 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "core_decomp = nk.community.CoreDecomposition(g, storeNodeOrder=True)\n",
    "core_decomp.run()\n",
    "cover_g = core_decomp.getCover()\n",
    "order = 15\n",
    "\n",
    "end = time.time()\n",
    "print('Core decomposition of the graph takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = 1\n",
    "# for ss in cover_g.subsetSizes():\n",
    "#     print('Subset of order %d has %d elements' % (idx, ss))\n",
    "#     idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9647 nodes that belong in 15-core decomposition of this graph\n"
     ]
    }
   ],
   "source": [
    "print('There are %d nodes that belong in %d-core decomposition of this graph' \n",
    "      % (len(cover_g.getMembers(order)), order))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_kcore(ds, kcore):\n",
    "    '''\n",
    "    Compute feature: whether a pair of nodes is found in the same k-core graph\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: dataset\n",
    "    kcore: the k-core graph after decomposition as a set of nodes index (ranged from 0 to 27,770)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A numpy array of ordinal values: \n",
    "        - 0 if both nodes are not in the kcores, \n",
    "        - 0.5 if one of them is in the kcores, \n",
    "        - 1 of they are both in the k-core\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    same_kcore = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        \n",
    "        # compute whether two nodes are in the given kcore | one of them is in the kcore | none of them\n",
    "        index_src = ID[src] # index of src\n",
    "        index_dest = ID[dest] # index of dest\n",
    "        \n",
    "        if index_src in kcore and index_dest in kcore:\n",
    "            result = 1.0\n",
    "        elif index_src not in kcore and index_dest not in kcore:\n",
    "            result = 0.0\n",
    "        else:\n",
    "            result = 0.5\n",
    "            \n",
    "        same_kcore[i] = result\n",
    "        \n",
    "    return same_kcore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the in k-core feature for training set takes 1.0302 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the position of two nodes wrt k-core\n",
    "train_in_kcore = in_kcore(training, cover_g.getMembers(order))\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the in k-core feature for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the in k-core feature for testing set takes 0.1119 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the position of two nodes wrt k-core\n",
    "test_in_kcore = in_kcore(testing, cover_g.getMembers(order))\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the in k-core feature for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: [0.  1.  0.  0.5 0.5 1.  1.  0.5 1.  0. ]\n",
      "Testing: [1.  1.  1.  1.  0.5 0.5 0.  1.  0.  0. ]\n"
     ]
    }
   ],
   "source": [
    "print('Training:', train_in_kcore[0:10])\n",
    "print('Testing:', test_in_kcore[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Adamic-Adar coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adamic_adar_coeff(ds, g):\n",
    "    '''\n",
    "    Compute Adamic-Adar coefficient\n",
    "    '''\n",
    "    \n",
    "    size = len(ds)\n",
    "    aa_coeff = np.zeros(size)\n",
    "    aa_index = nk.linkprediction.AdamicAdarIndex(g)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        \n",
    "        aa_coeff[i] = aa_index.run(ID[src], ID[dest])\n",
    "        \n",
    "    return aa_coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Adamic-Adar coefficient feature for training set takes 4.8188 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the adamic-adar coefficient\n",
    "train_aa_coeff = adamic_adar_coeff(training, g)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing Adamic-Adar coefficient feature for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Adamic-Adar coefficient feature for testing set takes 0.2673 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the adamic-adar coefficient\n",
    "test_aa_coeff = adamic_adar_coeff(testing, g)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing Adamic-Adar coefficient feature for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: [0.51389834 4.32036615 0.         0.         0.         3.17502987\n",
      " 2.46874101 0.         0.9428623  0.        ]\n",
      "Testing: [ 0.          5.37797275 15.05361173  4.89942438  0.          0.\n",
      "  0.          1.46868886  0.          1.27898053]\n"
     ]
    }
   ],
   "source": [
    "print('Training:', train_aa_coeff[0:10])\n",
    "print('Testing:', test_aa_coeff[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Katz index (read from already computed file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== read in katz index of training set ====== #\n",
    "with open(path_data + 'katz_training.txt', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    train_katz_index = list(reader)\n",
    "\n",
    "end = time.time()\n",
    "print('Reading katz feature training takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== read in katz index of training set ====== #\n",
    "with open(path_data + 'katz_testing.txt', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    test_katz_index = list(reader)\n",
    "\n",
    "end = time.time()\n",
    "print('Reading katz feature testing takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Katz index (with ``centrality`` package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing katz centrality (with centrality package) takes 0.0929 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "katz = nk.centrality.KatzCentrality(g)\n",
    "katz.run()\n",
    "\n",
    "end = time.time()\n",
    "print('Computing katz centrality (with centrality package) takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_katz(ds, katz_scores):\n",
    "    '''\n",
    "    Compute Katz index between a pair of nodes (using centrality package and compute the average)\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    katz_result = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1]\n",
    "        _katz = max(katz_scores[ID[src]], katz_scores[ID[dest]])\n",
    "        katz_result[i] = np.log(_katz) if _katz != 0.0 else 0.0\n",
    "        #katz_result[i] = float(katz_scores[ID[src]] + katz_scores[ID[dest]])/2.0\n",
    "        \n",
    "    return katz_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Katz max feature for training set takes 4.1459 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the katz index\n",
    "train_katz = max_katz(training, katz.scores())\n",
    "\n",
    "end = time.time()\n",
    "print('Computing Katz max feature for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Katz average feature for testing set takes 0.2795 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the katz index\n",
    "test_katz = max_katz(testing, katz.scores())\n",
    "\n",
    "end = time.time()\n",
    "print('Computing Katz average feature for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: [-5.5804487  -4.2164855  -5.64652461 -5.25171866 -5.36014155 -4.96955029\n",
      " -2.88068326 -4.86388004 -4.29294528 -5.46850804]\n",
      "Testing: [-4.69918791 -3.80632847 -2.88068326 -4.78584888 -4.22347698 -4.97580013\n",
      " -5.65542043 -5.30505257 -5.6155517  -5.50044342]\n"
     ]
    }
   ],
   "source": [
    "print('Training:', train_katz[0:10])\n",
    "print('Testing:', test_katz[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Katz index (with ``linkprediction`` package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def katz_linkpred(ds, g):\n",
    "    '''\n",
    "    Compute Katz index between a pair of nodes (using linkprediction)\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    katz_result = np.zeros(size)\n",
    "    katz = nk.linkprediction.KatzIndex(g)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        katz_result[i] = katz.run(ID[src], ID[dest])\n",
    "        if (i+1)%1000 == 0:\n",
    "            print('Processed %d data' % (i+1))\n",
    "        \n",
    "    return katz_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention:** only run the two following cells when you really want to do it, because the computation takes around **9 hours** to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000 data\n",
      "Processed 2000 data\n",
      "Processed 3000 data\n",
      "Processed 4000 data\n",
      "Processed 5000 data\n",
      "Processed 6000 data\n",
      "Processed 7000 data\n",
      "Processed 8000 data\n",
      "Processed 9000 data\n",
      "Processed 10000 data\n",
      "Processed 11000 data\n",
      "Processed 12000 data\n",
      "Processed 13000 data\n",
      "Processed 14000 data\n",
      "Processed 15000 data\n",
      "Processed 16000 data\n",
      "Processed 17000 data\n",
      "Processed 18000 data\n",
      "Processed 19000 data\n",
      "Processed 20000 data\n",
      "Processed 21000 data\n",
      "Processed 22000 data\n",
      "Processed 23000 data\n",
      "Processed 24000 data\n",
      "Processed 25000 data\n",
      "Processed 26000 data\n",
      "Processed 27000 data\n",
      "Processed 28000 data\n",
      "Processed 29000 data\n",
      "Processed 30000 data\n",
      "Processed 31000 data\n",
      "Processed 32000 data\n",
      "Processed 33000 data\n",
      "Processed 34000 data\n",
      "Processed 35000 data\n",
      "Processed 36000 data\n",
      "Processed 37000 data\n",
      "Processed 38000 data\n",
      "Processed 39000 data\n",
      "Processed 40000 data\n",
      "Processed 41000 data\n",
      "Processed 42000 data\n",
      "Processed 43000 data\n",
      "Processed 44000 data\n",
      "Processed 45000 data\n",
      "Processed 46000 data\n",
      "Processed 47000 data\n",
      "Processed 48000 data\n",
      "Processed 49000 data\n",
      "Processed 50000 data\n",
      "Processed 51000 data\n",
      "Processed 52000 data\n",
      "Processed 53000 data\n",
      "Processed 54000 data\n",
      "Processed 55000 data\n",
      "Processed 56000 data\n",
      "Processed 57000 data\n",
      "Processed 58000 data\n",
      "Processed 59000 data\n",
      "Processed 60000 data\n",
      "Processed 61000 data\n",
      "Processed 62000 data\n",
      "Processed 63000 data\n",
      "Processed 64000 data\n",
      "Processed 65000 data\n",
      "Processed 66000 data\n",
      "Processed 67000 data\n",
      "Processed 68000 data\n",
      "Processed 69000 data\n",
      "Processed 70000 data\n",
      "Processed 71000 data\n",
      "Processed 72000 data\n",
      "Processed 73000 data\n",
      "Processed 74000 data\n",
      "Processed 75000 data\n",
      "Processed 76000 data\n",
      "Processed 77000 data\n",
      "Processed 78000 data\n",
      "Processed 79000 data\n",
      "Processed 80000 data\n",
      "Processed 81000 data\n",
      "Processed 82000 data\n",
      "Processed 83000 data\n",
      "Processed 84000 data\n",
      "Processed 85000 data\n",
      "Processed 86000 data\n",
      "Processed 87000 data\n",
      "Processed 88000 data\n",
      "Processed 89000 data\n",
      "Processed 90000 data\n",
      "Processed 91000 data\n",
      "Processed 92000 data\n",
      "Processed 93000 data\n",
      "Processed 94000 data\n",
      "Processed 95000 data\n",
      "Processed 96000 data\n",
      "Processed 97000 data\n",
      "Processed 98000 data\n",
      "Processed 99000 data\n",
      "Processed 100000 data\n",
      "Processed 101000 data\n",
      "Processed 102000 data\n",
      "Processed 103000 data\n",
      "Processed 104000 data\n",
      "Processed 105000 data\n",
      "Processed 106000 data\n",
      "Processed 107000 data\n",
      "Processed 108000 data\n",
      "Processed 109000 data\n",
      "Processed 110000 data\n",
      "Processed 111000 data\n",
      "Processed 112000 data\n",
      "Processed 113000 data\n",
      "Processed 114000 data\n",
      "Processed 115000 data\n",
      "Processed 116000 data\n",
      "Processed 117000 data\n",
      "Processed 118000 data\n",
      "Processed 119000 data\n",
      "Processed 120000 data\n",
      "Processed 121000 data\n",
      "Processed 122000 data\n",
      "Processed 123000 data\n",
      "Processed 124000 data\n",
      "Processed 125000 data\n",
      "Processed 126000 data\n",
      "Processed 127000 data\n",
      "Processed 128000 data\n",
      "Processed 129000 data\n",
      "Processed 130000 data\n",
      "Processed 131000 data\n",
      "Processed 132000 data\n",
      "Processed 133000 data\n",
      "Processed 134000 data\n",
      "Processed 135000 data\n",
      "Processed 136000 data\n",
      "Processed 137000 data\n",
      "Processed 138000 data\n",
      "Processed 139000 data\n",
      "Processed 140000 data\n",
      "Processed 141000 data\n",
      "Processed 142000 data\n",
      "Processed 143000 data\n",
      "Processed 144000 data\n",
      "Processed 145000 data\n",
      "Processed 146000 data\n",
      "Processed 147000 data\n",
      "Processed 148000 data\n",
      "Processed 149000 data\n",
      "Processed 150000 data\n",
      "Processed 151000 data\n",
      "Processed 152000 data\n",
      "Processed 153000 data\n",
      "Processed 154000 data\n",
      "Processed 155000 data\n",
      "Processed 156000 data\n",
      "Processed 157000 data\n",
      "Processed 158000 data\n",
      "Processed 159000 data\n",
      "Processed 160000 data\n",
      "Processed 161000 data\n",
      "Processed 162000 data\n",
      "Processed 163000 data\n",
      "Processed 164000 data\n",
      "Processed 165000 data\n",
      "Processed 166000 data\n",
      "Processed 167000 data\n",
      "Processed 168000 data\n",
      "Processed 169000 data\n",
      "Processed 170000 data\n",
      "Processed 171000 data\n",
      "Processed 172000 data\n",
      "Processed 173000 data\n",
      "Processed 174000 data\n",
      "Processed 175000 data\n",
      "Processed 176000 data\n",
      "Processed 177000 data\n",
      "Processed 178000 data\n",
      "Processed 179000 data\n",
      "Processed 180000 data\n",
      "Processed 181000 data\n",
      "Processed 182000 data\n",
      "Processed 183000 data\n",
      "Processed 184000 data\n",
      "Processed 185000 data\n",
      "Processed 186000 data\n",
      "Processed 187000 data\n",
      "Processed 188000 data\n",
      "Processed 189000 data\n",
      "Processed 190000 data\n",
      "Processed 191000 data\n",
      "Processed 192000 data\n",
      "Processed 193000 data\n",
      "Processed 194000 data\n",
      "Processed 195000 data\n",
      "Processed 196000 data\n",
      "Processed 197000 data\n",
      "Processed 198000 data\n",
      "Processed 199000 data\n",
      "Processed 200000 data\n",
      "Processed 201000 data\n",
      "Processed 202000 data\n",
      "Processed 203000 data\n",
      "Processed 204000 data\n",
      "Processed 205000 data\n",
      "Processed 206000 data\n",
      "Processed 207000 data\n",
      "Processed 208000 data\n",
      "Processed 209000 data\n",
      "Processed 210000 data\n",
      "Processed 211000 data\n",
      "Processed 212000 data\n",
      "Processed 213000 data\n",
      "Processed 214000 data\n",
      "Processed 215000 data\n",
      "Processed 216000 data\n",
      "Processed 217000 data\n",
      "Processed 218000 data\n",
      "Processed 219000 data\n",
      "Processed 220000 data\n",
      "Processed 221000 data\n",
      "Processed 222000 data\n",
      "Processed 223000 data\n",
      "Processed 224000 data\n",
      "Processed 225000 data\n",
      "Processed 226000 data\n",
      "Processed 227000 data\n",
      "Processed 228000 data\n",
      "Processed 229000 data\n",
      "Processed 230000 data\n",
      "Processed 231000 data\n",
      "Processed 232000 data\n",
      "Processed 233000 data\n",
      "Processed 234000 data\n",
      "Processed 235000 data\n",
      "Processed 236000 data\n",
      "Processed 237000 data\n",
      "Processed 238000 data\n",
      "Processed 239000 data\n",
      "Processed 240000 data\n",
      "Processed 241000 data\n",
      "Processed 242000 data\n",
      "Processed 243000 data\n",
      "Processed 244000 data\n",
      "Processed 245000 data\n",
      "Processed 246000 data\n",
      "Processed 247000 data\n",
      "Processed 248000 data\n",
      "Processed 249000 data\n",
      "Processed 250000 data\n",
      "Processed 251000 data\n",
      "Processed 252000 data\n",
      "Processed 253000 data\n",
      "Processed 254000 data\n",
      "Processed 255000 data\n",
      "Processed 256000 data\n",
      "Processed 257000 data\n",
      "Processed 258000 data\n",
      "Processed 259000 data\n",
      "Processed 260000 data\n",
      "Processed 261000 data\n",
      "Processed 262000 data\n",
      "Processed 263000 data\n",
      "Processed 264000 data\n",
      "Processed 265000 data\n",
      "Processed 266000 data\n",
      "Processed 267000 data\n",
      "Processed 268000 data\n",
      "Processed 269000 data\n",
      "Processed 270000 data\n",
      "Processed 271000 data\n",
      "Processed 272000 data\n",
      "Processed 273000 data\n",
      "Processed 274000 data\n",
      "Processed 275000 data\n",
      "Processed 276000 data\n",
      "Processed 277000 data\n",
      "Processed 278000 data\n",
      "Processed 279000 data\n",
      "Processed 280000 data\n",
      "Processed 281000 data\n",
      "Processed 282000 data\n",
      "Processed 283000 data\n",
      "Processed 284000 data\n",
      "Processed 285000 data\n",
      "Processed 286000 data\n",
      "Processed 287000 data\n",
      "Processed 288000 data\n",
      "Processed 289000 data\n",
      "Processed 290000 data\n",
      "Processed 291000 data\n",
      "Processed 292000 data\n",
      "Processed 293000 data\n",
      "Processed 294000 data\n",
      "Processed 295000 data\n",
      "Processed 296000 data\n",
      "Processed 297000 data\n",
      "Processed 298000 data\n",
      "Processed 299000 data\n",
      "Processed 300000 data\n",
      "Processed 301000 data\n",
      "Processed 302000 data\n",
      "Processed 303000 data\n",
      "Processed 304000 data\n",
      "Processed 305000 data\n",
      "Processed 306000 data\n",
      "Processed 307000 data\n",
      "Processed 308000 data\n",
      "Processed 309000 data\n",
      "Processed 310000 data\n",
      "Processed 311000 data\n",
      "Processed 312000 data\n",
      "Processed 313000 data\n",
      "Processed 314000 data\n",
      "Processed 315000 data\n",
      "Processed 316000 data\n",
      "Processed 317000 data\n",
      "Processed 318000 data\n",
      "Processed 319000 data\n",
      "Processed 320000 data\n",
      "Processed 321000 data\n",
      "Processed 322000 data\n",
      "Processed 323000 data\n",
      "Processed 324000 data\n",
      "Processed 325000 data\n",
      "Processed 326000 data\n",
      "Processed 327000 data\n",
      "Processed 328000 data\n",
      "Processed 329000 data\n",
      "Processed 330000 data\n",
      "Processed 331000 data\n",
      "Processed 332000 data\n",
      "Processed 333000 data\n",
      "Processed 334000 data\n",
      "Processed 335000 data\n",
      "Processed 336000 data\n",
      "Processed 337000 data\n",
      "Processed 338000 data\n",
      "Processed 339000 data\n",
      "Processed 340000 data\n",
      "Processed 341000 data\n",
      "Processed 342000 data\n",
      "Processed 343000 data\n",
      "Processed 344000 data\n",
      "Processed 345000 data\n",
      "Processed 346000 data\n",
      "Processed 347000 data\n",
      "Processed 348000 data\n",
      "Processed 349000 data\n",
      "Processed 350000 data\n",
      "Processed 351000 data\n",
      "Processed 352000 data\n",
      "Processed 353000 data\n",
      "Processed 354000 data\n",
      "Processed 355000 data\n",
      "Processed 356000 data\n",
      "Processed 357000 data\n",
      "Processed 358000 data\n",
      "Processed 359000 data\n",
      "Processed 360000 data\n",
      "Processed 361000 data\n",
      "Processed 362000 data\n",
      "Processed 363000 data\n",
      "Processed 364000 data\n",
      "Processed 365000 data\n",
      "Processed 366000 data\n",
      "Processed 367000 data\n",
      "Processed 368000 data\n",
      "Processed 369000 data\n",
      "Processed 370000 data\n",
      "Processed 371000 data\n",
      "Processed 372000 data\n",
      "Processed 373000 data\n",
      "Processed 374000 data\n",
      "Processed 375000 data\n",
      "Processed 376000 data\n",
      "Processed 377000 data\n",
      "Processed 378000 data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 379000 data\n",
      "Processed 380000 data\n",
      "Processed 381000 data\n",
      "Processed 382000 data\n",
      "Processed 383000 data\n",
      "Processed 384000 data\n",
      "Processed 385000 data\n",
      "Processed 386000 data\n",
      "Processed 387000 data\n",
      "Processed 388000 data\n",
      "Processed 389000 data\n",
      "Processed 390000 data\n",
      "Processed 391000 data\n",
      "Processed 392000 data\n",
      "Processed 393000 data\n",
      "Processed 394000 data\n",
      "Processed 395000 data\n",
      "Processed 396000 data\n",
      "Processed 397000 data\n",
      "Processed 398000 data\n",
      "Processed 399000 data\n",
      "Processed 400000 data\n",
      "Processed 401000 data\n",
      "Processed 402000 data\n",
      "Processed 403000 data\n",
      "Processed 404000 data\n",
      "Processed 405000 data\n",
      "Processed 406000 data\n",
      "Processed 407000 data\n",
      "Processed 408000 data\n",
      "Processed 409000 data\n",
      "Processed 410000 data\n",
      "Processed 411000 data\n",
      "Processed 412000 data\n",
      "Processed 413000 data\n",
      "Processed 414000 data\n",
      "Processed 415000 data\n",
      "Processed 416000 data\n",
      "Processed 417000 data\n",
      "Processed 418000 data\n",
      "Processed 419000 data\n",
      "Processed 420000 data\n",
      "Processed 421000 data\n",
      "Processed 422000 data\n",
      "Processed 423000 data\n",
      "Processed 424000 data\n",
      "Processed 425000 data\n",
      "Processed 426000 data\n",
      "Processed 427000 data\n",
      "Processed 428000 data\n",
      "Processed 429000 data\n",
      "Processed 430000 data\n",
      "Processed 431000 data\n",
      "Processed 432000 data\n",
      "Processed 433000 data\n",
      "Processed 434000 data\n",
      "Processed 435000 data\n",
      "Processed 436000 data\n",
      "Processed 437000 data\n",
      "Processed 438000 data\n",
      "Processed 439000 data\n",
      "Processed 440000 data\n",
      "Processed 441000 data\n",
      "Processed 442000 data\n",
      "Processed 443000 data\n",
      "Processed 444000 data\n",
      "Processed 445000 data\n",
      "Processed 446000 data\n",
      "Processed 447000 data\n",
      "Processed 448000 data\n",
      "Processed 449000 data\n",
      "Processed 450000 data\n",
      "Processed 451000 data\n",
      "Processed 452000 data\n",
      "Processed 453000 data\n",
      "Processed 454000 data\n",
      "Processed 455000 data\n",
      "Processed 456000 data\n",
      "Processed 457000 data\n",
      "Processed 458000 data\n",
      "Processed 459000 data\n",
      "Processed 460000 data\n",
      "Processed 461000 data\n",
      "Processed 462000 data\n",
      "Processed 463000 data\n",
      "Processed 464000 data\n",
      "Processed 465000 data\n",
      "Processed 466000 data\n",
      "Processed 467000 data\n",
      "Processed 468000 data\n",
      "Processed 469000 data\n",
      "Processed 470000 data\n",
      "Processed 471000 data\n",
      "Processed 472000 data\n",
      "Processed 473000 data\n",
      "Processed 474000 data\n",
      "Processed 475000 data\n",
      "Processed 476000 data\n",
      "Processed 477000 data\n",
      "Processed 478000 data\n",
      "Processed 479000 data\n",
      "Processed 480000 data\n",
      "Processed 481000 data\n",
      "Processed 482000 data\n",
      "Processed 483000 data\n",
      "Processed 484000 data\n",
      "Processed 485000 data\n",
      "Processed 486000 data\n",
      "Processed 487000 data\n",
      "Processed 488000 data\n",
      "Processed 489000 data\n",
      "Processed 490000 data\n",
      "Processed 491000 data\n",
      "Processed 492000 data\n",
      "Processed 493000 data\n",
      "Processed 494000 data\n",
      "Processed 495000 data\n",
      "Processed 496000 data\n",
      "Processed 497000 data\n",
      "Processed 498000 data\n",
      "Processed 499000 data\n",
      "Processed 500000 data\n",
      "Processed 501000 data\n",
      "Processed 502000 data\n",
      "Processed 503000 data\n",
      "Processed 504000 data\n",
      "Processed 505000 data\n",
      "Processed 506000 data\n",
      "Processed 507000 data\n",
      "Processed 508000 data\n",
      "Processed 509000 data\n",
      "Processed 510000 data\n",
      "Processed 511000 data\n",
      "Processed 512000 data\n",
      "Processed 513000 data\n",
      "Processed 514000 data\n",
      "Processed 515000 data\n",
      "Processed 516000 data\n",
      "Processed 517000 data\n",
      "Processed 518000 data\n",
      "Processed 519000 data\n",
      "Processed 520000 data\n",
      "Processed 521000 data\n",
      "Processed 522000 data\n",
      "Processed 523000 data\n",
      "Processed 524000 data\n",
      "Processed 525000 data\n",
      "Processed 526000 data\n",
      "Processed 527000 data\n",
      "Processed 528000 data\n",
      "Processed 529000 data\n",
      "Processed 530000 data\n",
      "Processed 531000 data\n",
      "Processed 532000 data\n",
      "Processed 533000 data\n",
      "Processed 534000 data\n",
      "Processed 535000 data\n",
      "Processed 536000 data\n",
      "Processed 537000 data\n",
      "Processed 538000 data\n",
      "Processed 539000 data\n",
      "Processed 540000 data\n",
      "Processed 541000 data\n",
      "Processed 542000 data\n",
      "Processed 543000 data\n",
      "Processed 544000 data\n",
      "Processed 545000 data\n",
      "Processed 546000 data\n",
      "Processed 547000 data\n",
      "Processed 548000 data\n",
      "Processed 549000 data\n",
      "Processed 550000 data\n",
      "Processed 551000 data\n",
      "Processed 552000 data\n",
      "Processed 553000 data\n",
      "Processed 554000 data\n",
      "Processed 555000 data\n",
      "Processed 556000 data\n",
      "Processed 557000 data\n",
      "Processed 558000 data\n",
      "Processed 559000 data\n",
      "Processed 560000 data\n",
      "Processed 561000 data\n",
      "Processed 562000 data\n",
      "Processed 563000 data\n",
      "Processed 564000 data\n",
      "Processed 565000 data\n",
      "Processed 566000 data\n",
      "Processed 567000 data\n",
      "Processed 568000 data\n",
      "Processed 569000 data\n",
      "Processed 570000 data\n",
      "Processed 571000 data\n",
      "Processed 572000 data\n",
      "Processed 573000 data\n",
      "Processed 574000 data\n",
      "Processed 575000 data\n",
      "Processed 576000 data\n",
      "Processed 577000 data\n",
      "Processed 578000 data\n",
      "Processed 579000 data\n",
      "Processed 580000 data\n",
      "Processed 581000 data\n",
      "Processed 582000 data\n",
      "Processed 583000 data\n",
      "Processed 584000 data\n",
      "Processed 585000 data\n",
      "Processed 586000 data\n",
      "Processed 587000 data\n",
      "Processed 588000 data\n",
      "Processed 589000 data\n",
      "Processed 590000 data\n",
      "Processed 591000 data\n",
      "Processed 592000 data\n",
      "Processed 593000 data\n",
      "Processed 594000 data\n",
      "Processed 595000 data\n",
      "Processed 596000 data\n",
      "Processed 597000 data\n",
      "Processed 598000 data\n",
      "Processed 599000 data\n",
      "Processed 600000 data\n",
      "Processed 601000 data\n",
      "Processed 602000 data\n",
      "Processed 603000 data\n",
      "Processed 604000 data\n",
      "Processed 605000 data\n",
      "Processed 606000 data\n",
      "Processed 607000 data\n",
      "Processed 608000 data\n",
      "Processed 609000 data\n",
      "Processed 610000 data\n",
      "Processed 611000 data\n",
      "Processed 612000 data\n",
      "Processed 613000 data\n",
      "Processed 614000 data\n",
      "Processed 615000 data\n",
      "Computing Katz index feature for training set takes -393.6872 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# set number of threads\n",
    "nk.engineering.setNumberOfThreads(4)\n",
    "\n",
    "# compute the katz index\n",
    "train_katz_linkpred = katz_linkpred(training, g)\n",
    "\n",
    "# end = time.time()\n",
    "print('Computing Katz index feature for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 999 data\n",
      "Processed 1999 data\n",
      "Processed 2999 data\n",
      "Processed 3999 data\n",
      "Processed 4999 data\n",
      "Processed 5999 data\n",
      "Processed 6999 data\n",
      "Processed 7999 data\n",
      "Processed 8999 data\n",
      "Processed 9999 data\n",
      "Processed 10999 data\n",
      "Processed 11999 data\n",
      "Processed 12999 data\n",
      "Processed 13999 data\n",
      "Processed 14999 data\n",
      "Processed 15999 data\n",
      "Processed 16999 data\n",
      "Processed 17999 data\n",
      "Processed 18999 data\n",
      "Processed 19999 data\n",
      "Processed 20999 data\n",
      "Processed 21999 data\n",
      "Processed 22999 data\n",
      "Processed 23999 data\n",
      "Processed 24999 data\n",
      "Processed 25999 data\n",
      "Processed 26999 data\n",
      "Processed 27999 data\n",
      "Processed 28999 data\n",
      "Processed 29999 data\n",
      "Processed 30999 data\n",
      "Processed 31999 data\n",
      "Computing Katz index feature for testing set takes 1168.3391 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the katz index\n",
    "test_katz_linkpred = katz_index(testing, g)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing Katz index feature for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: [5.02650754e-03 5.51846733e-03 0.00000000e+00 1.38756250e-06\n",
      " 1.37575000e-07 5.35477387e-03 5.39283918e-03 1.52531250e-08\n",
      " 5.13756281e-03 6.25000000e-12]\n",
      "Testing: [2.84559375e-07 6.32439694e-04 1.54608918e-03 5.32540828e-04\n",
      " 1.70984375e-06 1.46984375e-07 0.00000000e+00 1.55151381e-04\n",
      " 6.40625000e-10 1.01133166e-04]\n"
     ]
    }
   ],
   "source": [
    "print('Training:', train_katz_index[0:10])\n",
    "print('Testing:', test_katz_index[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_data + 'katz_training.txt', 'wb') as f:\n",
    "    np.savetxt(f, train_katz_linkpred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of selected features\n",
    "features = [\n",
    "    'temporal_difference', # 0\n",
    "    'common_authors', # 1\n",
    "    'same_journal', # 2\n",
    "    'cosine_sim', # 3\n",
    "    'overlapping_title', # 4\n",
    "    'max_degrees', # 5\n",
    "    'common_neighbors', # 6\n",
    "    'jaccard_coefficient', # 7\n",
    "    'max_pagerank', # 8\n",
    "    'max_betweenness', # 9\n",
    "    'in_kcore', # 10\n",
    "    'adamic_adar', # 11\n",
    "    'katz_index', # 12\n",
    "    'cosine_sim_w2v', # 13\n",
    "    'katz_linkpred' # 14\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== create array of training feature ====== #\n",
    "training_features = np.array([\n",
    "    train_temp_diff,\n",
    "    train_common_auth,\n",
    "    train_same_journal,\n",
    "    train_cosine,\n",
    "    train_overlap_title,\n",
    "    train_avg_degrees,\n",
    "    train_common_neigh,\n",
    "    train_jaccard_coeff,\n",
    "    train_pagerank,\n",
    "    train_btwn,\n",
    "    train_in_kcore,\n",
    "    train_aa_coeff,\n",
    "    train_katz,\n",
    "    train_cosine_sim_w2v,\n",
    "    train_katz_linkpred\n",
    "]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Saving features (training_features) ====== #\n",
    "with open(path_data + 'training_features.csv', 'w', newline='') as f:\n",
    "    csv_out = csv.writer(f)\n",
    "    csv_out.writerow(features)\n",
    "    for row in training_features:\n",
    "        csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== create array of testing features ====== #\n",
    "testing_features = np.array([\n",
    "    test_temp_diff,\n",
    "    test_common_auth,\n",
    "    test_same_journal,\n",
    "    test_cosine,\n",
    "    test_overlap_title,\n",
    "    test_avg_degrees,\n",
    "    test_common_neigh,\n",
    "    test_jaccard_coeff,\n",
    "    test_pagerank,\n",
    "    test_btwn,\n",
    "    test_in_kcore,\n",
    "    test_aa_coeff,\n",
    "    test_katz,\n",
    "    test_cosine_sim_w2v,\n",
    "    test_katz_linkpred\n",
    "]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Saving features (testing_features) ====== #\n",
    "with open(path_data + 'testing_features.csv', 'w', newline='') as f:\n",
    "    csv_out = csv.writer(f)\n",
    "    csv_out.writerow(features)\n",
    "    for row in testing_features:\n",
    "        csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replacing/Appending features without recomputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training features takes 10.9378 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== read training features ====== #\n",
    "orig_training_features = np.genfromtxt(path_data + 'training_features.csv', delimiter=',', skip_header=1, dtype=float)\n",
    "\n",
    "end = time.time()\n",
    "print('Reading training features takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading testing features takes 0.4623 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== read testing features as str ====== #\n",
    "orig_testing_features = np.genfromtxt(path_data + 'testing_features.csv', delimiter=',', skip_header=1, dtype=float)\n",
    "\n",
    "end = time.time()\n",
    "print('Reading testing features takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features: (615512, 14)\n",
      "Testing features: (32648, 14)\n"
     ]
    }
   ],
   "source": [
    "print('Training features:', orig_training_features.shape)\n",
    "print('Testing features:', orig_testing_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append katz link prediction to training features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.293029862567551, -5.199655114063047, 0.0, -13.487961947295274, -15.799096614000588, -5.229766804031945, -5.222683282397631, -17.998481436853037, -5.271176472957152, -25.79843965218024]\n",
      "After concatenation, training features has shape: (615512, 15)\n"
     ]
    }
   ],
   "source": [
    "# recompute katz by adding log\n",
    "_train_katz = [np.log(_katz) if _katz != 0.0 else 0.0 for _katz in train_katz_linkpred]\n",
    "print(_train_katz[0:10])\n",
    "\n",
    "# append katz linkpred to training features\n",
    "training_features = np.append(orig_training_features, np.reshape(_train_katz, (-1,1)), axis=1)\n",
    "print('After concatenation, training features has shape:', training_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-15.072323905681971, -7.365925687758105, -6.472026643304163, -7.537850990784671, -13.279108565893594, -15.732939548334132, 0.0, -8.771109264434683, -21.168576853601774, -9.199072438008713]\n",
      "After concatenation, testing features has shape: (32648, 15)\n"
     ]
    }
   ],
   "source": [
    "# read katz linkpred testing\n",
    "test_katz_linkpred = np.genfromtxt(path_data + 'katz_testing.txt')\n",
    "\n",
    "# add log\n",
    "_test_katz = [np.log(_katz) if _katz != 0.0 else 0.0 for _katz in test_katz_linkpred]\n",
    "print(_test_katz[0:10])\n",
    "\n",
    "# append katz linkpred to testing features\n",
    "testing_features = np.append(orig_testing_features, np.reshape(_test_katz, (-1,1)), axis=1)\n",
    "print('After concatenation, testing features has shape:', testing_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change average_degree to maximum_degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After replacement, training features has shape: (615512, 14)\n",
      "After replacement, testing features has shape: (32648, 14)\n"
     ]
    }
   ],
   "source": [
    "index_avg_deg = 5\n",
    "training_features[:,index_avg_deg] = train_degrees\n",
    "testing_features[:,index_avg_deg] = test_degrees\n",
    "print('After replacement, training features has shape:', training_features.shape)\n",
    "print('After replacement, testing features has shape:', testing_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change avg_pagerank to maximum_pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After replacement, training features has shape: (615512, 14)\n",
      "After replacement, testing features has shape: (32648, 14)\n"
     ]
    }
   ],
   "source": [
    "index_pr = 8\n",
    "training_features[:,index_pr] = train_pagerank\n",
    "testing_features[:,index_pr] = test_pagerank\n",
    "print('After replacement, training features has shape:', training_features.shape)\n",
    "print('After replacement, testing features has shape:', testing_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change avg_betweenness to maximum_betweenness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After replacement, training features has shape: (615512, 14)\n",
      "After replacement, testing features has shape: (32648, 14)\n"
     ]
    }
   ],
   "source": [
    "index_btwn = 9\n",
    "training_features[:,index_btwn] = train_btwn\n",
    "testing_features[:,index_btwn] = test_btwn\n",
    "print('After replacement, training features has shape:', training_features.shape)\n",
    "print('After replacement, testing features has shape:', testing_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change in_kcore to order 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After replacement, training features has shape: (615512, 14)\n",
      "After replacement, testing features has shape: (32648, 14)\n"
     ]
    }
   ],
   "source": [
    "index_kcore = 10\n",
    "training_features[:,index_kcore] = train_in_kcore\n",
    "testing_features[:,index_kcore] = test_in_kcore\n",
    "print('After replacement, training features has shape:', training_features.shape)\n",
    "print('After replacement, testing features has shape:', testing_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Katz centrality from average to max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After replacement, training features has shape: (615512, 14)\n",
      "After replacement, testing features has shape: (32648, 14)\n"
     ]
    }
   ],
   "source": [
    "index_katz = 12\n",
    "training_features[:,index_katz] = train_katz\n",
    "testing_features[:,index_katz] = test_katz\n",
    "print('After replacement, training features has shape:', training_features.shape)\n",
    "print('After replacement, testing features has shape:', testing_features.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
