{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries and Utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing libraries and setting up parameters takes 0.0002 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "import networkit as nk\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn import preprocessing as pre\n",
    "\n",
    "# working with text\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "end = time.time()\n",
    "print('Importing libraries and setting up parameters takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(nodes, edges):\n",
    "    g = nk.Graph(len(nodes)) # adding nodes\n",
    "\n",
    "    for edge in edges:\n",
    "        if not g.hasEdge(edge[0], edge[1]): # avoid multiple edges\n",
    "            g.addEdge(edge[0], edge[1])\n",
    "            \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to preprocess text\n",
    "def preprocess(text, dg_removal=True, sw_removal=True, stemming=True):\n",
    "    '''\n",
    "    Preprocess text: stopword removal, stemming, digit removal\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text: text on which preprocessing is applied\n",
    "    dg_removal: whether to apply digit removal or not\n",
    "    sw_removal: whether to apply stopword removal or not\n",
    "    stemming: whether to apply stemming or not\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    the text after preprocessing\n",
    "    '''\n",
    "    result = text\n",
    "    \n",
    "    sw = set(nltk.corpus.stopwords.words('english')) # set of stopwords\n",
    "    stemmer = nltk.stem.PorterStemmer() # stemmer\n",
    "    \n",
    "    if dg_removal:\n",
    "        result = re.sub('[0-9]', '', result)\n",
    "    \n",
    "    if sw_removal:\n",
    "        result = ' '.join([token for token in result.split() if token not in sw])\n",
    "        \n",
    "    if stemming:\n",
    "        result = ' '.join([stemmer.stem(token) for token in result.split()])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '../data/' # path to the data\n",
    "path_submission = '../submission/' # path to submission files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading node information takes 0.3392 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== read in node informations ====== #\n",
    "with open(path_data + 'node_information.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    node_info = list(reader)\n",
    "\n",
    "end = time.time()\n",
    "print('Reading node information takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training set takes 3.0673 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== read training data as str ====== #\n",
    "training = np.genfromtxt(path_data + 'training_set.txt', dtype=str)\n",
    "\n",
    "end = time.time()\n",
    "print('Reading training set takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading testing set takes 0.1534 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== read testing data as str ====== #\n",
    "testing = np.genfromtxt(path_data + 'testing_set.txt', dtype=str)\n",
    "\n",
    "end = time.time()\n",
    "print('Reading testing set takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the citation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the citation graph takes 187.6449 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== build the graph ====== #\n",
    "\n",
    "nodes = [element[0] for element in node_info] # create index list to be passed as nodes\n",
    "edges = [(nodes.index(element[0]), nodes.index(element[1])) for element in training if element[2] == '1']\n",
    "g = build_graph(nodes, edges)\n",
    "\n",
    "end = time.time()\n",
    "print('Building the citation graph takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vertices: 27770\n",
      "Number of edges (after multiple edges removal): 334690\n"
     ]
    }
   ],
   "source": [
    "# check for general information of the graph\n",
    "print('Number of vertices: %d' % g.numberOfNodes())\n",
    "print('Number of edges (after multiple edges removal): %d' % g.numberOfEdges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of features is described as follows, and the computation rule is the same for both training and testing set.\n",
    "\n",
    "| Feature                | Explanation                                                        | Type      | Range   |\n",
    "|:----------------------:|:------------------------------------------------------------------:|:---------:|:-------:|\n",
    "| Temporal difference    | Difference in publication year (absolute value)                    | numerical | $\\ge$ 0 |\n",
    "| Common authors         | The number of common authors between two articles                  | numerical | $\\ge$ 0 |\n",
    "| Same journal           | Whether two articles are published in the same journal             | binary    | 0, 1    |\n",
    "| Cosine similarity      | Cosine similarity between word vectors of abstracts                | numerical | [0,1]   |\n",
    "| Title overlap          | Number of overlapping words in title                               | numerical | $\\ge$ 0 |\n",
    "| Degree difference      | Difference in measure of degrees of two nodes (absolute value)     | numerical | $\\ge$ 0 |\n",
    "| Common neighbors       | Number of common neighbors                                         | numercial | $\\ge$ 0 |\n",
    "| Jaccard coefficient    | Link-based Jaccard coefficient                                     | numerical | [0,1]   |\n",
    "| Same cluster           | Check whether two nodes are in the same cluster                    | binary    | [0,1]   |\n",
    "| PageRank difference    | Difference in PageRank index of two nodes (absolute value)         | numerical | $\\ge$ 0 |\n",
    "| Betweenness centrality | Difference in betweenness centrality of two nodes (absolute value) | numerical | $\\ge$ 0 |\n",
    "| In the same k-core     | Whether both nodes/one of them/none of them are in the same k-core | ordinal   |[0,0.5,1]|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the dictionary of (ID-index) to accelerate access to node'ID\n",
    "ID = dict(zip(nodes, [nodes.index(n) for n in nodes]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Temporal difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_difference(ds):\n",
    "    '''\n",
    "    Compute feature: Difference in publication year\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: the dataset to compute\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A numpy array where each entry corresponds to the temporal difference of a pair of nodes\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    temp_diff = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        src_info, dest_info = node_info[ID[src]], node_info[ID[dest]] # get the associated node information by index\n",
    "        \n",
    "        # compute the difference in publication year in absolute value (because we don't know which one cites the other)\n",
    "        temp_diff[i] = abs(\n",
    "            int(src_info[1]) - int(dest_info[1])\n",
    "        )\n",
    "        \n",
    "    return temp_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing temporal difference for training set takes 1.6704 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the temporal difference\n",
    "train_temp_diff = temporal_difference(training)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing temporal difference for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing temporal difference for testing set takes 0.1156 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the temporal difference\n",
    "test_temp_diff = temporal_difference(testing)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing temporal difference for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: [0. 1. 2. 4. 5. 0. 4. 7. 0. 8.]\n",
      "Testing: [0. 1. 2. 0. 5. 4. 0. 1. 7. 0.]\n"
     ]
    }
   ],
   "source": [
    "print('Training:', train_temp_diff[0:10])\n",
    "print('Testing:', test_temp_diff[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Number of common authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_authors(ds):\n",
    "    '''\n",
    "    Compute feature: number of common authors\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: dataset to compute feature from\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A numpy array\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    common_auth = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        src_info, dest_info = node_info[ID[src]], node_info[ID[dest]] # get the associated node information by index\n",
    "        \n",
    "        # compute the difference in publication year in absolute value (because we don't know which one cites the other)\n",
    "        common_auth[i] = len(\n",
    "            set(src_info[3].split(','))\n",
    "            .intersection(set(dest_info[3].split(',')))\n",
    "        )\n",
    "        \n",
    "        \n",
    "    return common_auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the number of common authors for training set takes 2.5344 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the temporal difference\n",
    "train_common_auth = common_authors(training)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the number of common authors for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the number of common authors for testing set takes 0.1583 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the temporal difference\n",
    "test_common_auth = common_authors(testing)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the number of common authors for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "Testing: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print('Training:', train_common_auth[0:10])\n",
    "print('Testing:', test_common_auth[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Same journal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_journal(ds):\n",
    "    '''\n",
    "    Compute feature: whether two articles are published in the same journal\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: dataset to compute feature from\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A numpy array of binary values (0|1)\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    same_journal = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        src_info, dest_info = node_info[ID[src]], node_info[ID[dest]] # get the associated node information by index\n",
    "        \n",
    "        # 1 if two articles are published in the same journal, 0 otherwise\n",
    "        same_journal[i] = int(\n",
    "            len(src_info[4])>0 and  # journal info of source not null\n",
    "            len(dest_info[4])>0 and # journal info of dest not null\n",
    "            src_info[4] == dest_info[4] # the same journal title\n",
    "        )\n",
    "        \n",
    "        \n",
    "    return same_journal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing whether two articles are published in the same journal for training set takes 1.4511 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the temporal difference\n",
    "train_same_journal = same_journal(training)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing whether two articles are published in the same journal for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing whether two articles are published in the same journal for testing set takes 0.1035 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the temporal difference\n",
    "test_same_journal = same_journal(testing)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing whether two articles are published in the same journal for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Testing: [0. 0. 1. 1. 0. 0. 0. 1. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print('Training:', train_same_journal[0:10])\n",
    "print('Testing:', test_same_journal[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cosine similarity in title + abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the TF-IDF matrix takes 57.3886 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== corpus is the set of titles + abstracts, apply preprocessing to each article ======#\n",
    "\n",
    "#nltk.download('stopwords') # uncomment if haven't downloaded stopwords\n",
    "corpus = [preprocess(element[2] + ' ' + element[5], dg_removal=True, sw_removal=True, stemming=True) \n",
    "          for element in node_info]\n",
    "vectorizer = TfidfVectorizer(stop_words='english') # create a TF-IDF vectorizer\n",
    "tfidf = vectorizer.fit_transform(corpus) # TD-IDF matrix of the entire corpus (set of abstracts)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the TF-IDF matrix takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim_text(ds, tfidf):\n",
    "    '''\n",
    "    Compute feature: cosine similarity in title and abstract\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: dataset to compute feature from\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A numpy array of cosine values\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    cosines = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        \n",
    "        # collect the cosine similarity\n",
    "        src_vect, dest_vect = tfidf[ID[src]], tfidf[ID[dest]] # get the corresponding vector in TD-IDF matrix\n",
    "        cos = cosine_similarity(src_vect, dest_vect) # compute cosine similarity\n",
    "        cosines[i] = cos\n",
    "        \n",
    "    return cosines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing cosine similarity for training set takes 372.6914 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the cosine similarity\n",
    "train_cosine = cosine_sim_text(training, tfidf)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing cosine similarity for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing cosine similarity for testing set takes 19.3335 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the cosine similarity\n",
    "test_cosine = cosine_sim_text(testing, tfidf)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing cosine similarity for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: [0.19996622 0.06436945 0.02053711 0.05937844 0.09852643 0.39581923\n",
      " 0.18722569 0.08627054 0.04181436 0.06044751]\n",
      "Testing: [0.11804009 0.30786265 0.20753805 0.16112407 0.31824453 0.03466872\n",
      " 0.02490266 0.19991048 0.         0.3283665 ]\n"
     ]
    }
   ],
   "source": [
    "print('Training:', train_cosine[0:10])\n",
    "print('Testing:', test_cosine[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Number of overlapped words in title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap_title(ds):\n",
    "    '''\n",
    "    Compute feature: number of overlapping words in the title\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: dataset to compute feature from\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A numpy array of numerical values\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    overlap_title = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        src_info, dest_info = node_info[ID[src]], node_info[ID[dest]] # get the associated node information by index\n",
    "        \n",
    "        # collect the number of overlapping words in title\n",
    "        src_title, dest_title = preprocess(src_info[2]).split(), preprocess(dest_info[2]).split()\n",
    "        overlap_title[i] = len(\n",
    "            set(src_title)\n",
    "            .intersection(set(dest_title))\n",
    "        )\n",
    "        \n",
    "    return overlap_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing number of overlapping words in title for training set takes 616.3972 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the number of overlapping words in title\n",
    "train_overlap_title = overlap_title(training)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing number of overlapping words in title for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing number of overlapping words in title for testing set takes 32.3066 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the number of overlapping words in title\n",
    "test_overlap_title = overlap_title(testing)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing number of overlapping words in title for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: [2. 1. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Testing: [0. 2. 1. 1. 0. 0. 1. 1. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print('Training:', train_overlap_title[0:10])\n",
    "print('Testing:', test_overlap_title[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Average of degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_degrees(ds, g):\n",
    "    '''\n",
    "    Compute feature: Average degrees of 2 nodes\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: dataset to compute feature from\n",
    "    g: the graph\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A numpy array of numerical values\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    avg_degree = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        \n",
    "        # collect the number of overlapping words in title\n",
    "        src_deg = g.degree(ID[src])\n",
    "        dest_deg = g.degree(ID[dest])\n",
    "        avg_degree[i] = float(src_deg + dest_deg)/2.0\n",
    "        \n",
    "    return avg_degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the average degree for training set takes 1.1711 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the average degree\n",
    "train_avg_degrees = average_degrees(training, g)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the average degree for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the average degree for testing set takes 0.0884 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the average degree\n",
    "test_avg_degrees = average_degrees(testing, g)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the average degree for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: [  9.  113.    3.   17.   15.5  36.5 400.   50.5  70.5  17. ]\n",
      "Testing: [ 38.5 173.5 481.   58.   78.5  24.5   2.5  31.5   4.5  12.5]\n"
     ]
    }
   ],
   "source": [
    "print('Training:', train_avg_degrees[0:10])\n",
    "print('Testing:', test_avg_degrees[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Number of common neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_neighbors(ds, g):\n",
    "    '''\n",
    "    Compute feature: The number of common neighbors\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: dataset to compute feature from\n",
    "    g: the graph\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A numpy array of numerical values\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    common_neigh = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        \n",
    "        # collect the number of overlapping words in title\n",
    "        common_neigh[i] = len(\n",
    "            set(g.neighbors(ID[src]))\n",
    "            .intersection(set(g.neighbors(ID[dest])))\n",
    "        )\n",
    "        \n",
    "    return common_neigh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the number of common neighbors for training set takes 8.2263 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the average degree\n",
    "train_common_neigh = common_neighbors(training, g)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the number of common neighbors for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the number of common neighbors for testing set takes 0.4318 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the average degree\n",
    "test_common_neigh = common_neighbors(testing, g)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the number of common neighbors for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: [ 1. 20.  0.  0.  0. 14. 12.  0.  5.  0.]\n",
      "Testing: [ 0. 24. 59. 21.  0.  0.  0.  6.  0.  4.]\n"
     ]
    }
   ],
   "source": [
    "print('Training:', train_common_neigh[0:10])\n",
    "print('Testing:', test_common_neigh[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Link-based Jaccard coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_coeff(ds, g):\n",
    "    '''\n",
    "    Compute feature: Link-based Jaccard coefficient\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: dataset to compute feature from\n",
    "    g: the graph\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A numpy array of numerical values\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    coeff = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        \n",
    "        # collect the number of overlapping words in title\n",
    "        inters = len(\n",
    "            set(g.neighbors(ID[src]))\n",
    "            .intersection(set(g.neighbors(ID[dest])))\n",
    "        ) # intersection of neighbors\n",
    "        \n",
    "        union = len(\n",
    "            set(g.neighbors(ID[src]))\n",
    "            .union(set(g.neighbors(ID[dest])))\n",
    "        ) # union of neighbors\n",
    "        \n",
    "        coeff[i] = (float(inters)/float(union) if union != 0 else 0)\n",
    "        \n",
    "    return coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing link-based Jaccard coefficient for training set takes 19.7526 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the average degree\n",
    "train_jaccard_coeff = jaccard_coeff(training, g)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing link-based Jaccard coefficient for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing link-based Jaccard coefficient for testing set takes 1.0697 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the average degree\n",
    "test_jaccard_coeff = jaccard_coeff(testing, g)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing link-based Jaccard coefficient for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: [0.05882353 0.09708738 0.         0.         0.         0.23728814\n",
      " 0.01522843 0.         0.03676471 0.        ]\n",
      "Testing: [0.         0.07430341 0.06533776 0.22105263 0.         0.\n",
      " 0.         0.10526316 0.         0.19047619]\n"
     ]
    }
   ],
   "source": [
    "print('Training:', train_jaccard_coeff[0:10])\n",
    "print('Testing:', test_jaccard_coeff[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Same cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Average of PageRank index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the PageRank index of the graph takes 0.2074 s\n"
     ]
    }
   ],
   "source": [
    "# ====== compute PageRank index ====== #\n",
    "start = time.time()\n",
    "\n",
    "page_rank_g = nk.centrality.PageRank(g)\n",
    "page_rank_g.run()\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the PageRank index of the graph takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_pagerank(ds, pr):\n",
    "    '''\n",
    "    Compute feature: average of pagerank\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: dataset to compute feature from\n",
    "    g: the graph\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A numpy array of numerical values\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    avg_pr = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        \n",
    "        # collect the average of betweenness centrality of 2 nodes\n",
    "        # log to \"dampen\" too small values\n",
    "        avg_pr[i] = np.log(float(pr[ID[src]] + pr[ID[dest]])/2.0)\n",
    "        \n",
    "    return avg_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the average page rank for training set takes 1.6472 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the average pagerank\n",
    "train_avg_pr = avg_pagerank(training, page_rank_g.scores())\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the average page rank for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the average page rank for testing set takes 0.1157 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the average pagerank\n",
    "test_avg_pr = avg_pagerank(testing, page_rank_g.scores())\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the average page rank for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: [-10.75542888  -9.28819986 -11.27892234 -10.79563113 -10.66865404\n",
      " -10.28702328  -7.90007968  -9.70137278  -9.57712104 -10.0708701 ]\n",
      "Testing: [-10.11196423  -8.59294688  -7.70950565  -9.84766778  -9.44479929\n",
      " -10.56567714 -11.30701705 -10.06990187 -11.24745838 -10.83202494]\n"
     ]
    }
   ],
   "source": [
    "print('Training:', train_avg_pr[0:10])\n",
    "print('Testing:', test_avg_pr[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Average of betweenness centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute betweenness centrality of every node in the graph takes 404.2914 s\n"
     ]
    }
   ],
   "source": [
    "# ====== compute betweenness centrality ====== #\n",
    "start = time.time()\n",
    "\n",
    "# use the traditional approach of betweeness computation\n",
    "btwn = nk.centrality.Betweenness(g)\n",
    "btwn.run()\n",
    "\n",
    "end = time.time()\n",
    "print('Compute betweenness centrality of every node in the graph takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_betweeness(ds, btwn):\n",
    "    '''\n",
    "    Compute feature: average in betweenness centrality\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: dataset to compute feature from\n",
    "    g: the graph\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A numpy array of numerical values\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    avg_btw = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        \n",
    "        # collect the average of betweenness centrality of 2 nodes\n",
    "        _avg = float(btwn[ID[src]] + btwn[ID[dest]])\n",
    "        avg_btw[i] = np.log(_avg/2.0) if _avg != 0.0 else 0.0\n",
    "        \n",
    "    return avg_btw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the average betweenness for training set takes 1.6072 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the average degree\n",
    "train_avg_btwn = avg_betweeness(training, btwn.scores())\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the average betweenness for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the average betweenness for testing set takes 0.1104 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the average degree\n",
    "test_avg_btwn = avg_betweeness(testing, btwn.scores())\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the average betweenness for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: [10.08688482 11.16590802  8.84607013  7.65815856  9.07845272  8.44864804\n",
      " 15.30895205 12.10404087 12.35357254 11.4065003 ]\n",
      "Testing: [11.89953033 14.12541191 15.37221353 11.0787389  12.47171402  9.63258213\n",
      "  4.84337128 11.06101491  6.74711568  7.60931935]\n"
     ]
    }
   ],
   "source": [
    "print('Training:', train_avg_btwn[0:10])\n",
    "print('Testing:', test_avg_btwn[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Core Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intuition:** retrieve the core of a network, where many articles are connected to each other. Given a pair of articles, if both are found in the core, they are likely to connect to each other (assign value 1). If one is in the core and one is not, they might be connect to each other (assign value 0.5). Otherwise, they are highly unlikely to connect to each other (value 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core decomposition of the graph takes 0.1534 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "core_decomp = nk.community.CoreDecomposition(g, storeNodeOrder=True)\n",
    "core_decomp.run()\n",
    "cover_g = core_decomp.getCover()\n",
    "order = 15\n",
    "\n",
    "end = time.time()\n",
    "print('Core decomposition of the graph takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = 1\n",
    "# for ss in cover_g.subsetSizes():\n",
    "#     print('Subset of order %d has %d elements' % (idx, ss))\n",
    "#     idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9647 nodes that belong in 15-core decomposition of this graph\n"
     ]
    }
   ],
   "source": [
    "print('There are %d nodes that belong in %d-core decomposition of this graph' \n",
    "      % (len(cover_g.getMembers(order)), order))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_kcore(ds, kcore):\n",
    "    '''\n",
    "    Compute feature: whether a pair of nodes is found in the same k-core graph\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: dataset\n",
    "    kcore: the k-core graph after decomposition as a set of nodes index (ranged from 0 to 27,770)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A numpy array of ordinal values: \n",
    "        - 0 if both nodes are not in the kcores, \n",
    "        - 0.5 if one of them is in the kcores, \n",
    "        - 1 of they are both in the k-core\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    same_kcore = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        \n",
    "        # compute whether two nodes are in the given kcore | one of them is in the kcore | none of them\n",
    "        index_src = ID[src] # index of src\n",
    "        index_dest = ID[dest] # index of dest\n",
    "        \n",
    "        if index_src in kcore and index_dest in kcore:\n",
    "            result = 1.0\n",
    "        elif index_src not in kcore and index_dest not in kcore:\n",
    "            result = 0.0\n",
    "        else:\n",
    "            result = 0.5\n",
    "            \n",
    "        same_kcore[i] = result\n",
    "        \n",
    "    return same_kcore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the in k-core feature for training set takes 1.0003 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the position of two nodes wrt k-core\n",
    "train_in_kcore = in_kcore(training, cover_g.getMembers(order))\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the in k-core feature for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the in k-core feature for testing set takes 0.0914 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the position of two nodes wrt k-core\n",
    "test_in_kcore = in_kcore(testing, cover_g.getMembers(order))\n",
    "\n",
    "end = time.time()\n",
    "print('Computing the in k-core feature for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: [0.  1.  0.  0.5 0.5 1.  1.  0.5 1.  0. ]\n",
      "Testing: [1.  1.  1.  1.  0.5 0.5 0.  1.  0.  0. ]\n"
     ]
    }
   ],
   "source": [
    "print('Training:', train_in_kcore[0:10])\n",
    "print('Testing:', test_in_kcore[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Adamic-Adar coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adamic_adar_coeff(ds, g):\n",
    "    '''\n",
    "    Compute Adamic-Adar coefficient\n",
    "    '''\n",
    "    \n",
    "    size = len(ds)\n",
    "    aa_coeff = np.zeros(size)\n",
    "    aa_index = nk.linkprediction.AdamicAdarIndex(g)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        \n",
    "        aa_coeff[i] = aa_index.run(ID[src], ID[dest])\n",
    "        \n",
    "    return aa_coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Adamic-Adar coefficient feature for training set takes 4.8188 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the adamic-adar coefficient\n",
    "train_aa_coeff = adamic_adar_coeff(training, g)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing Adamic-Adar coefficient feature for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Adamic-Adar coefficient feature for testing set takes 0.2673 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the adamic-adar coefficient\n",
    "test_aa_coeff = adamic_adar_coeff(testing, g)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing Adamic-Adar coefficient feature for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: [0.51389834 4.32036615 0.         0.         0.         3.17502987\n",
      " 2.46874101 0.         0.9428623  0.        ]\n",
      "Testing: [ 0.          5.37797275 15.05361173  4.89942438  0.          0.\n",
      "  0.          1.46868886  0.          1.27898053]\n"
     ]
    }
   ],
   "source": [
    "print('Training:', train_aa_coeff[0:10])\n",
    "print('Testing:', test_aa_coeff[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Katz index (with ``centrality`` package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing katz centrality (with centrality package) takes 0.1251 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "katz = nk.centrality.KatzCentrality(g)\n",
    "katz.run()\n",
    "\n",
    "end = time.time()\n",
    "print('Computing katz centrality (with centrality package) takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def katz_centrality(ds, katz_scores):\n",
    "    '''\n",
    "    Compute Katz index between a pair of nodes (using centrality package and compute the average)\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    katz_result = np.zeros(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1]\n",
    "        katz_result[i] = float(katz_scores[ID[src]] + katz_scores[ID[dest]])/2.0\n",
    "        \n",
    "    return katz_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the katz index\n",
    "train_katz_avg = katz_centrality(training, katz.scores())\n",
    "\n",
    "end = time.time()\n",
    "print('Computing Katz average feature for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the katz index\n",
    "test_katz_avg = katz_centrality(training, katz.scores())\n",
    "\n",
    "end = time.time()\n",
    "print('Computing Katz average feature for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Katz index (with ``linkprediction`` package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def katz_index(ds, g):\n",
    "    '''\n",
    "    Compute Katz index between a pair of nodes (using linkprediction)\n",
    "    '''\n",
    "    size = len(ds)\n",
    "    katz_result = np.zeros(size)\n",
    "    katz = nk.linkprediction.KatzIndex(g)\n",
    "    \n",
    "    for i in range(size):\n",
    "        src, dest = ds[i][0], ds[i][1] # get the source and dest ID\n",
    "        katz_result[i] = katz.run(ID[src], ID[dest])\n",
    "        if (i+1)%1000 == 0:\n",
    "            print('Processed %d data' % (i+1))\n",
    "        \n",
    "    return katz_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 999 data\n",
      "Processed 1999 data\n",
      "Processed 2999 data\n",
      "Processed 3999 data\n",
      "Processed 4999 data\n",
      "Processed 5999 data\n",
      "Processed 6999 data\n",
      "Processed 7999 data\n",
      "Processed 8999 data\n",
      "Processed 9999 data\n",
      "Processed 10999 data\n",
      "Processed 11999 data\n",
      "Processed 12999 data\n",
      "Processed 13999 data\n",
      "Processed 14999 data\n",
      "Processed 15999 data\n",
      "Processed 16999 data\n",
      "Processed 17999 data\n",
      "Processed 18999 data\n",
      "Processed 19999 data\n",
      "Processed 20999 data\n",
      "Processed 21999 data\n",
      "Processed 22999 data\n",
      "Processed 23999 data\n",
      "Processed 24999 data\n",
      "Processed 25999 data\n",
      "Processed 26999 data\n",
      "Processed 27999 data\n",
      "Processed 28999 data\n",
      "Processed 29999 data\n",
      "Processed 30999 data\n",
      "Processed 31999 data\n",
      "Processed 32999 data\n",
      "Processed 33999 data\n",
      "Processed 34999 data\n",
      "Processed 35999 data\n",
      "Processed 36999 data\n",
      "Processed 37999 data\n",
      "Processed 38999 data\n",
      "Processed 39999 data\n",
      "Processed 40999 data\n",
      "Processed 41999 data\n",
      "Processed 42999 data\n",
      "Processed 43999 data\n",
      "Processed 44999 data\n",
      "Processed 45999 data\n",
      "Processed 46999 data\n",
      "Processed 47999 data\n",
      "Processed 48999 data\n",
      "Processed 49999 data\n",
      "Processed 50999 data\n",
      "Processed 51999 data\n",
      "Processed 52999 data\n",
      "Processed 53999 data\n",
      "Processed 54999 data\n",
      "Processed 55999 data\n",
      "Processed 56999 data\n",
      "Processed 57999 data\n",
      "Processed 58999 data\n",
      "Processed 59999 data\n",
      "Processed 60999 data\n",
      "Processed 61999 data\n",
      "Processed 62999 data\n",
      "Processed 63999 data\n",
      "Processed 64999 data\n",
      "Processed 65999 data\n",
      "Processed 66999 data\n",
      "Processed 67999 data\n",
      "Processed 68999 data\n",
      "Processed 69999 data\n",
      "Processed 70999 data\n",
      "Processed 71999 data\n",
      "Processed 72999 data\n",
      "Processed 73999 data\n",
      "Processed 74999 data\n",
      "Processed 75999 data\n",
      "Processed 76999 data\n",
      "Processed 77999 data\n",
      "Processed 78999 data\n",
      "Processed 79999 data\n",
      "Processed 80999 data\n",
      "Processed 81999 data\n",
      "Processed 82999 data\n",
      "Processed 83999 data\n",
      "Processed 84999 data\n",
      "Processed 85999 data\n",
      "Processed 86999 data\n",
      "Processed 87999 data\n",
      "Processed 88999 data\n",
      "Processed 89999 data\n",
      "Processed 90999 data\n",
      "Processed 91999 data\n",
      "Processed 92999 data\n",
      "Processed 93999 data\n",
      "Processed 94999 data\n",
      "Processed 95999 data\n",
      "Processed 96999 data\n",
      "Processed 97999 data\n",
      "Processed 98999 data\n",
      "Processed 99999 data\n",
      "Processed 100999 data\n",
      "Processed 101999 data\n",
      "Processed 102999 data\n",
      "Processed 103999 data\n",
      "Processed 104999 data\n",
      "Processed 105999 data\n",
      "Processed 106999 data\n",
      "Processed 107999 data\n",
      "Processed 108999 data\n",
      "Processed 109999 data\n",
      "Processed 110999 data\n",
      "Processed 111999 data\n",
      "Processed 112999 data\n",
      "Processed 113999 data\n",
      "Processed 114999 data\n",
      "Processed 115999 data\n",
      "Processed 116999 data\n",
      "Processed 117999 data\n",
      "Processed 118999 data\n",
      "Processed 119999 data\n",
      "Processed 120999 data\n",
      "Processed 121999 data\n",
      "Processed 122999 data\n",
      "Processed 123999 data\n",
      "Processed 124999 data\n",
      "Processed 125999 data\n",
      "Processed 126999 data\n",
      "Processed 127999 data\n",
      "Processed 128999 data\n",
      "Processed 129999 data\n",
      "Processed 130999 data\n",
      "Processed 131999 data\n",
      "Processed 132999 data\n",
      "Processed 133999 data\n",
      "Processed 134999 data\n",
      "Processed 135999 data\n",
      "Processed 136999 data\n",
      "Processed 137999 data\n",
      "Processed 138999 data\n",
      "Processed 139999 data\n",
      "Processed 140999 data\n",
      "Processed 141999 data\n",
      "Processed 142999 data\n",
      "Processed 143999 data\n",
      "Processed 144999 data\n",
      "Processed 145999 data\n",
      "Processed 146999 data\n",
      "Processed 147999 data\n",
      "Processed 148999 data\n",
      "Processed 149999 data\n",
      "Processed 150999 data\n",
      "Processed 151999 data\n",
      "Processed 152999 data\n",
      "Processed 153999 data\n",
      "Processed 154999 data\n",
      "Processed 155999 data\n",
      "Processed 156999 data\n",
      "Processed 157999 data\n",
      "Processed 158999 data\n",
      "Processed 159999 data\n",
      "Processed 160999 data\n",
      "Processed 161999 data\n",
      "Processed 162999 data\n",
      "Processed 163999 data\n",
      "Processed 164999 data\n",
      "Processed 165999 data\n",
      "Processed 166999 data\n",
      "Processed 167999 data\n",
      "Processed 168999 data\n",
      "Processed 169999 data\n",
      "Processed 170999 data\n",
      "Processed 171999 data\n",
      "Processed 172999 data\n",
      "Processed 173999 data\n",
      "Processed 174999 data\n",
      "Processed 175999 data\n",
      "Processed 176999 data\n",
      "Processed 177999 data\n",
      "Processed 178999 data\n",
      "Processed 179999 data\n",
      "Processed 180999 data\n",
      "Processed 181999 data\n",
      "Processed 182999 data\n",
      "Processed 183999 data\n",
      "Processed 184999 data\n",
      "Processed 185999 data\n",
      "Processed 186999 data\n",
      "Processed 187999 data\n",
      "Processed 188999 data\n",
      "Processed 189999 data\n",
      "Processed 190999 data\n",
      "Processed 191999 data\n",
      "Processed 192999 data\n",
      "Processed 193999 data\n",
      "Processed 194999 data\n",
      "Processed 195999 data\n",
      "Processed 196999 data\n",
      "Processed 197999 data\n",
      "Processed 198999 data\n",
      "Processed 199999 data\n",
      "Processed 200999 data\n",
      "Processed 201999 data\n",
      "Processed 202999 data\n",
      "Processed 203999 data\n",
      "Processed 204999 data\n",
      "Processed 205999 data\n",
      "Processed 206999 data\n",
      "Processed 207999 data\n",
      "Processed 208999 data\n",
      "Processed 209999 data\n",
      "Processed 210999 data\n",
      "Processed 211999 data\n",
      "Processed 212999 data\n",
      "Processed 213999 data\n",
      "Processed 214999 data\n",
      "Processed 215999 data\n",
      "Processed 216999 data\n",
      "Processed 217999 data\n",
      "Processed 218999 data\n",
      "Processed 219999 data\n",
      "Processed 220999 data\n",
      "Processed 221999 data\n",
      "Processed 222999 data\n",
      "Processed 223999 data\n",
      "Processed 224999 data\n",
      "Processed 225999 data\n",
      "Processed 226999 data\n",
      "Processed 227999 data\n",
      "Processed 228999 data\n",
      "Processed 229999 data\n",
      "Processed 230999 data\n",
      "Processed 231999 data\n",
      "Processed 232999 data\n",
      "Processed 233999 data\n",
      "Processed 234999 data\n",
      "Processed 235999 data\n",
      "Processed 236999 data\n",
      "Processed 237999 data\n",
      "Processed 238999 data\n",
      "Processed 239999 data\n",
      "Processed 240999 data\n",
      "Processed 241999 data\n",
      "Processed 242999 data\n",
      "Processed 243999 data\n",
      "Processed 244999 data\n",
      "Processed 245999 data\n",
      "Processed 246999 data\n",
      "Processed 247999 data\n",
      "Processed 248999 data\n",
      "Processed 249999 data\n",
      "Processed 250999 data\n",
      "Processed 251999 data\n",
      "Processed 252999 data\n",
      "Processed 253999 data\n",
      "Processed 254999 data\n",
      "Processed 255999 data\n",
      "Processed 256999 data\n",
      "Processed 257999 data\n",
      "Processed 258999 data\n",
      "Processed 259999 data\n",
      "Processed 260999 data\n",
      "Processed 261999 data\n",
      "Processed 262999 data\n",
      "Processed 263999 data\n",
      "Processed 264999 data\n",
      "Processed 265999 data\n",
      "Processed 266999 data\n",
      "Processed 267999 data\n",
      "Processed 268999 data\n",
      "Processed 269999 data\n",
      "Processed 270999 data\n",
      "Processed 271999 data\n",
      "Processed 272999 data\n",
      "Processed 273999 data\n",
      "Processed 274999 data\n",
      "Processed 275999 data\n",
      "Processed 276999 data\n",
      "Processed 277999 data\n",
      "Processed 278999 data\n",
      "Processed 279999 data\n",
      "Processed 280999 data\n",
      "Processed 281999 data\n",
      "Processed 282999 data\n",
      "Processed 283999 data\n",
      "Processed 284999 data\n",
      "Processed 285999 data\n",
      "Processed 286999 data\n",
      "Processed 287999 data\n",
      "Processed 288999 data\n",
      "Processed 289999 data\n",
      "Processed 290999 data\n",
      "Processed 291999 data\n",
      "Processed 292999 data\n",
      "Processed 293999 data\n",
      "Processed 294999 data\n",
      "Processed 295999 data\n",
      "Processed 296999 data\n",
      "Processed 297999 data\n",
      "Processed 298999 data\n",
      "Processed 299999 data\n",
      "Processed 300999 data\n",
      "Processed 301999 data\n",
      "Processed 302999 data\n",
      "Processed 303999 data\n",
      "Processed 304999 data\n",
      "Processed 305999 data\n",
      "Processed 306999 data\n",
      "Processed 307999 data\n",
      "Processed 308999 data\n",
      "Processed 309999 data\n",
      "Processed 310999 data\n",
      "Processed 311999 data\n",
      "Processed 312999 data\n",
      "Processed 313999 data\n",
      "Processed 314999 data\n",
      "Processed 315999 data\n",
      "Processed 316999 data\n",
      "Processed 317999 data\n",
      "Processed 318999 data\n",
      "Processed 319999 data\n",
      "Processed 320999 data\n",
      "Processed 321999 data\n",
      "Processed 322999 data\n",
      "Processed 323999 data\n",
      "Processed 324999 data\n",
      "Processed 325999 data\n",
      "Processed 326999 data\n",
      "Processed 327999 data\n",
      "Processed 328999 data\n",
      "Processed 329999 data\n",
      "Processed 330999 data\n",
      "Processed 331999 data\n",
      "Processed 332999 data\n",
      "Processed 333999 data\n",
      "Processed 334999 data\n",
      "Processed 335999 data\n",
      "Processed 336999 data\n",
      "Processed 337999 data\n",
      "Processed 338999 data\n",
      "Processed 339999 data\n",
      "Processed 340999 data\n",
      "Processed 341999 data\n",
      "Processed 342999 data\n",
      "Processed 343999 data\n",
      "Processed 344999 data\n",
      "Processed 345999 data\n",
      "Processed 346999 data\n",
      "Processed 347999 data\n",
      "Processed 348999 data\n",
      "Processed 349999 data\n",
      "Processed 350999 data\n",
      "Processed 351999 data\n",
      "Processed 352999 data\n",
      "Processed 353999 data\n",
      "Processed 354999 data\n",
      "Processed 355999 data\n",
      "Processed 356999 data\n",
      "Processed 357999 data\n",
      "Processed 358999 data\n",
      "Processed 359999 data\n",
      "Processed 360999 data\n",
      "Processed 361999 data\n",
      "Processed 362999 data\n",
      "Processed 363999 data\n",
      "Processed 364999 data\n",
      "Processed 365999 data\n",
      "Processed 366999 data\n",
      "Processed 367999 data\n",
      "Processed 368999 data\n",
      "Processed 369999 data\n",
      "Processed 370999 data\n",
      "Processed 371999 data\n",
      "Processed 372999 data\n",
      "Processed 373999 data\n",
      "Processed 374999 data\n",
      "Processed 375999 data\n",
      "Processed 376999 data\n",
      "Processed 377999 data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 378999 data\n",
      "Processed 379999 data\n",
      "Processed 380999 data\n",
      "Processed 381999 data\n",
      "Processed 382999 data\n",
      "Processed 383999 data\n",
      "Processed 384999 data\n",
      "Processed 385999 data\n",
      "Processed 386999 data\n",
      "Processed 387999 data\n",
      "Processed 388999 data\n",
      "Processed 389999 data\n",
      "Processed 390999 data\n",
      "Processed 391999 data\n",
      "Processed 392999 data\n",
      "Processed 393999 data\n",
      "Processed 394999 data\n",
      "Processed 395999 data\n",
      "Processed 396999 data\n",
      "Processed 397999 data\n",
      "Processed 398999 data\n",
      "Processed 399999 data\n",
      "Processed 400999 data\n",
      "Processed 401999 data\n",
      "Processed 402999 data\n",
      "Processed 403999 data\n",
      "Processed 404999 data\n",
      "Processed 405999 data\n",
      "Processed 406999 data\n",
      "Processed 407999 data\n",
      "Processed 408999 data\n",
      "Processed 409999 data\n",
      "Processed 410999 data\n",
      "Processed 411999 data\n",
      "Processed 412999 data\n",
      "Processed 413999 data\n",
      "Processed 414999 data\n",
      "Processed 415999 data\n",
      "Processed 416999 data\n",
      "Processed 417999 data\n",
      "Processed 418999 data\n",
      "Processed 419999 data\n",
      "Processed 420999 data\n",
      "Processed 421999 data\n",
      "Processed 422999 data\n",
      "Processed 423999 data\n",
      "Processed 424999 data\n",
      "Processed 425999 data\n",
      "Processed 426999 data\n",
      "Processed 427999 data\n",
      "Processed 428999 data\n",
      "Processed 429999 data\n",
      "Processed 430999 data\n",
      "Processed 431999 data\n",
      "Processed 432999 data\n",
      "Processed 433999 data\n",
      "Processed 434999 data\n",
      "Processed 435999 data\n",
      "Processed 436999 data\n",
      "Processed 437999 data\n",
      "Processed 438999 data\n",
      "Processed 439999 data\n",
      "Processed 440999 data\n",
      "Processed 441999 data\n",
      "Processed 442999 data\n",
      "Processed 443999 data\n",
      "Processed 444999 data\n",
      "Processed 445999 data\n",
      "Processed 446999 data\n",
      "Processed 447999 data\n",
      "Processed 448999 data\n",
      "Processed 449999 data\n",
      "Processed 450999 data\n",
      "Processed 451999 data\n",
      "Processed 452999 data\n",
      "Processed 453999 data\n",
      "Processed 454999 data\n",
      "Processed 455999 data\n",
      "Processed 456999 data\n",
      "Processed 457999 data\n",
      "Processed 458999 data\n",
      "Processed 459999 data\n",
      "Processed 460999 data\n",
      "Processed 461999 data\n",
      "Processed 462999 data\n",
      "Processed 463999 data\n",
      "Processed 464999 data\n",
      "Processed 465999 data\n",
      "Processed 466999 data\n",
      "Processed 467999 data\n",
      "Processed 468999 data\n",
      "Processed 469999 data\n",
      "Processed 470999 data\n",
      "Processed 471999 data\n",
      "Processed 472999 data\n",
      "Processed 473999 data\n",
      "Processed 474999 data\n",
      "Processed 475999 data\n",
      "Processed 476999 data\n",
      "Processed 477999 data\n",
      "Processed 478999 data\n",
      "Processed 479999 data\n",
      "Processed 480999 data\n",
      "Processed 481999 data\n",
      "Processed 482999 data\n",
      "Processed 483999 data\n",
      "Processed 484999 data\n",
      "Processed 485999 data\n",
      "Processed 486999 data\n",
      "Processed 487999 data\n",
      "Processed 488999 data\n",
      "Processed 489999 data\n",
      "Processed 490999 data\n",
      "Processed 491999 data\n",
      "Processed 492999 data\n",
      "Processed 493999 data\n",
      "Processed 494999 data\n",
      "Processed 495999 data\n",
      "Processed 496999 data\n",
      "Processed 497999 data\n",
      "Processed 498999 data\n",
      "Processed 499999 data\n",
      "Processed 500999 data\n",
      "Processed 501999 data\n",
      "Processed 502999 data\n",
      "Processed 503999 data\n",
      "Processed 504999 data\n",
      "Processed 505999 data\n",
      "Processed 506999 data\n",
      "Processed 507999 data\n",
      "Processed 508999 data\n",
      "Processed 509999 data\n",
      "Processed 510999 data\n",
      "Processed 511999 data\n",
      "Processed 512999 data\n",
      "Processed 513999 data\n",
      "Processed 514999 data\n",
      "Processed 515999 data\n",
      "Processed 516999 data\n",
      "Processed 517999 data\n",
      "Processed 518999 data\n",
      "Processed 519999 data\n",
      "Processed 520999 data\n",
      "Processed 521999 data\n",
      "Processed 522999 data\n",
      "Processed 523999 data\n",
      "Processed 524999 data\n",
      "Processed 525999 data\n",
      "Processed 526999 data\n",
      "Processed 527999 data\n",
      "Processed 528999 data\n",
      "Processed 529999 data\n",
      "Processed 530999 data\n",
      "Processed 531999 data\n",
      "Processed 532999 data\n",
      "Processed 533999 data\n",
      "Processed 534999 data\n",
      "Processed 535999 data\n",
      "Processed 536999 data\n",
      "Processed 537999 data\n",
      "Processed 538999 data\n",
      "Processed 539999 data\n",
      "Processed 540999 data\n",
      "Processed 541999 data\n",
      "Processed 542999 data\n",
      "Processed 543999 data\n",
      "Processed 544999 data\n",
      "Processed 545999 data\n",
      "Processed 546999 data\n",
      "Processed 547999 data\n",
      "Processed 548999 data\n",
      "Processed 549999 data\n",
      "Processed 550999 data\n",
      "Processed 551999 data\n",
      "Processed 552999 data\n",
      "Processed 553999 data\n",
      "Processed 554999 data\n",
      "Processed 555999 data\n",
      "Processed 556999 data\n",
      "Processed 557999 data\n",
      "Processed 558999 data\n",
      "Processed 559999 data\n",
      "Processed 560999 data\n",
      "Processed 561999 data\n",
      "Processed 562999 data\n",
      "Processed 563999 data\n",
      "Processed 564999 data\n",
      "Processed 565999 data\n",
      "Processed 566999 data\n",
      "Processed 567999 data\n",
      "Processed 568999 data\n",
      "Processed 569999 data\n",
      "Processed 570999 data\n",
      "Processed 571999 data\n",
      "Processed 572999 data\n",
      "Processed 573999 data\n",
      "Processed 574999 data\n",
      "Processed 575999 data\n",
      "Processed 576999 data\n",
      "Processed 577999 data\n",
      "Processed 578999 data\n",
      "Processed 579999 data\n",
      "Processed 580999 data\n",
      "Processed 581999 data\n",
      "Processed 582999 data\n",
      "Processed 583999 data\n",
      "Processed 584999 data\n",
      "Processed 585999 data\n",
      "Processed 586999 data\n",
      "Processed 587999 data\n",
      "Processed 588999 data\n",
      "Processed 589999 data\n",
      "Processed 590999 data\n",
      "Processed 591999 data\n",
      "Processed 592999 data\n",
      "Processed 593999 data\n",
      "Processed 594999 data\n",
      "Processed 595999 data\n",
      "Processed 596999 data\n",
      "Processed 597999 data\n",
      "Processed 598999 data\n",
      "Processed 599999 data\n",
      "Processed 600999 data\n",
      "Processed 601999 data\n",
      "Processed 602999 data\n",
      "Processed 603999 data\n",
      "Processed 604999 data\n",
      "Processed 605999 data\n",
      "Processed 606999 data\n",
      "Processed 607999 data\n",
      "Processed 608999 data\n",
      "Processed 609999 data\n",
      "Processed 610999 data\n",
      "Processed 611999 data\n",
      "Processed 612999 data\n",
      "Processed 613999 data\n",
      "Processed 614999 data\n",
      "Computing Katz index feature for training set takes 33860.7725 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the katz index\n",
    "train_katz_index = katz_index(training, g)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing Katz index feature for training set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 999 data\n",
      "Processed 1999 data\n",
      "Processed 2999 data\n",
      "Processed 3999 data\n",
      "Processed 4999 data\n",
      "Processed 5999 data\n",
      "Processed 6999 data\n",
      "Processed 7999 data\n",
      "Processed 8999 data\n",
      "Processed 9999 data\n",
      "Processed 10999 data\n",
      "Processed 11999 data\n",
      "Processed 12999 data\n",
      "Processed 13999 data\n",
      "Processed 14999 data\n",
      "Processed 15999 data\n",
      "Processed 16999 data\n",
      "Processed 17999 data\n",
      "Processed 18999 data\n",
      "Processed 19999 data\n",
      "Processed 20999 data\n",
      "Processed 21999 data\n",
      "Processed 22999 data\n",
      "Processed 23999 data\n",
      "Processed 24999 data\n",
      "Processed 25999 data\n",
      "Processed 26999 data\n",
      "Processed 27999 data\n",
      "Processed 28999 data\n",
      "Processed 29999 data\n",
      "Processed 30999 data\n",
      "Processed 31999 data\n",
      "Computing Katz index feature for testing set takes 1168.3391 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# compute the katz index\n",
    "test_katz_index = katz_index(testing, g)\n",
    "\n",
    "end = time.time()\n",
    "print('Computing Katz index feature for testing set takes %.4f s' %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: [5.02650754e-03 5.51846733e-03 0.00000000e+00 1.38756250e-06\n",
      " 1.37575000e-07 5.35477387e-03 5.39283918e-03 1.52531250e-08\n",
      " 5.13756281e-03 6.25000000e-12]\n",
      "Testing: [2.84559375e-07 6.32439694e-04 1.54608918e-03 5.32540828e-04\n",
      " 1.70984375e-06 1.46984375e-07 0.00000000e+00 1.55151381e-04\n",
      " 6.40625000e-10 1.01133166e-04]\n"
     ]
    }
   ],
   "source": [
    "print('Training:', train_katz_index[0:10])\n",
    "print('Testing:', test_katz_index[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save Katz index features into file because it takes too long to run (~9 hours), thus it is not convenient to repeat the process again\n",
    "\n",
    "# first, scale it because some values are too small\n",
    "from sklearn import preprocessing as pre\n",
    "train_katz_index_scale = pre.scale(train_katz_index)\n",
    "test_katz_index_scale = pre.scale(test_katz_index)\n",
    "\n",
    "# zip with node id\n",
    "train_katz_node = zip(nodes,train_katz_index)\n",
    "test_katz_node = zip(nodes,test_katz_index)\n",
    "\n",
    "with open(path_data + 'katz_features_train.csv', 'w', newline='') as f:\n",
    "    csv_out = csv.writer(f)\n",
    "    csv_out.writerow(['id', 'katz'])\n",
    "    for row in train_katz_node:\n",
    "        csv_out.writerow(row)\n",
    "        \n",
    "with open(path_data + 'katz_features_test.csv', 'w', newline='') as f:\n",
    "    csv_out = csv.writer(f)\n",
    "    csv_out.writerow(['id', 'katz'])\n",
    "    for row in test_katz_node:\n",
    "        csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of selected features\n",
    "features = [\n",
    "    'temporal_difference',\n",
    "    'common_authors',\n",
    "    'same_journal',\n",
    "    'cosine_sim',\n",
    "    'overlapping_title',\n",
    "    'average_degrees',\n",
    "    'common_neighbors',\n",
    "    'jaccard_coefficient',\n",
    "    'avg_pagerank',\n",
    "    'average_betweenness',\n",
    "    'in_kcore',\n",
    "    'adamic_adar',\n",
    "    'katz_index'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== create array of training feature ====== #\n",
    "training_features = np.array([\n",
    "    train_temp_diff,\n",
    "    train_common_auth,\n",
    "    train_same_journal,\n",
    "    train_cosine,\n",
    "    train_overlap_title,\n",
    "    train_avg_degrees,\n",
    "    train_common_neigh,\n",
    "    train_jaccard_coeff,\n",
    "    train_avg_pr,\n",
    "    train_avg_btwn,\n",
    "    train_in_kcore,\n",
    "    train_aa_coeff,\n",
    "    train_katz_index\n",
    "]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Saving features (training_features) ====== #\n",
    "with open(path_data + 'training_features.csv', 'w', newline='') as f:\n",
    "    csv_out = csv.writer(f)\n",
    "    csv_out.writerow(features)\n",
    "    for row in training_features:\n",
    "        csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== create array of testing features ====== #\n",
    "testing_features = np.array([\n",
    "    test_temp_diff,\n",
    "    test_common_auth,\n",
    "    test_same_journal,\n",
    "    test_cosine,\n",
    "    test_overlap_title,\n",
    "    test_avg_degrees,\n",
    "    test_common_neigh,\n",
    "    test_jaccard_coeff,\n",
    "    test_avg_pr,\n",
    "    test_avg_btwn,\n",
    "    test_in_kcore,\n",
    "    test_aa_coeff,\n",
    "    test_katz_index\n",
    "]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Saving features (testing_features) ====== #\n",
    "with open(path_data + 'testing_features.csv', 'w', newline='') as f:\n",
    "    csv_out = csv.writer(f)\n",
    "    csv_out.writerow(features)\n",
    "    for row in testing_features:\n",
    "        csv_out.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
