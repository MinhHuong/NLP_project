{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model and Issuing predictions (main script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading libraries takes 0.0002 s\n"
     ]
    }
   ],
   "source": [
    "# import time to find consuming steps\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "# utility libraries\n",
    "import numpy as np\n",
    "import csv as csv\n",
    "from sklearn import preprocessing as pre\n",
    "\n",
    "# classifier for classification\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate, cross_val_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "end = time.time()\n",
    "print('Loading libraries takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading dataset (training, testing, node information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '../data/' # path to the data\n",
    "path_submission = '../submission/' # path to submission files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training set & extracting labels takes 2.9731 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== read training data as str ====== #\n",
    "training = np.genfromtxt(path_data + 'training_set.txt', dtype=str)\n",
    "\n",
    "# ====== extract labels ====== #\n",
    "labels = training[:, 2].astype(int) # get the labels\n",
    "\n",
    "end = time.time()\n",
    "print('Reading training set & extracting labels takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training features takes 9.2582 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== read training features ====== #\n",
    "training_features = np.genfromtxt(path_data + 'training_features.csv', delimiter=',', skip_header=1, dtype=float)\n",
    "\n",
    "end = time.time()\n",
    "print('Reading training features takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading testing features takes 0.4289 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== read testing features as str ====== #\n",
    "testing_features = np.genfromtxt(path_data + 'testing_features.csv', delimiter=',', skip_header=1, dtype=float)\n",
    "\n",
    "end = time.time()\n",
    "print('Reading testing features takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features: (615512, 13)\n",
      "Labels: (615512,)\n",
      "Testing features: (32648, 13)\n"
     ]
    }
   ],
   "source": [
    "print('Training features:', training_features.shape)\n",
    "print('Labels:', labels.shape)\n",
    "print('Testing features:', testing_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_submission(filename, pred):\n",
    "    '''\n",
    "    Write prediction result in a submission file\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filename: name of submission file\n",
    "    pred: prediction array\n",
    "    \n",
    "    '''\n",
    "    with open(path_submission + filename, 'w', newline='') as f:\n",
    "        csv_out = csv.writer(f)\n",
    "        csv_out.writerow(['id','category'])\n",
    "        for row in pred:\n",
    "            csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Scaling features ====== #\n",
    "training_features_scale = pre.scale(training_features)\n",
    "testing_features_scale = pre.scale(testing_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32648"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_size = testing_features.shape[0]\n",
    "testing_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. SVM classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-A. SVM without scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with SVM Linear SVC takes 111.8000 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== training and predicting with SVM ====== #\n",
    "clf_svm = svm.LinearSVC()\n",
    "clf_svm.fit(training_features, labels)\n",
    "pred_svm = list(clf_svm.predict(testing_features))\n",
    "pred_svm = zip(range(len(testing_features)), pred_svm)\n",
    "\n",
    "end = time.time()\n",
    "print('Training with SVM Linear SVC takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_submission('submission_svm_11.csv', pred_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-B. SVM with scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.963 (+/- 0.00)\n",
      "Cross validating takes 18.7288 s\n"
     ]
    }
   ],
   "source": [
    "# ====== choosing C with cross validation ====== #\n",
    "start = time.time()\n",
    "\n",
    "clf_svm_scale = svm.LinearSVC(C=1.0, dual=False)\n",
    "scores = cross_val_score(clf_svm_scale, training_features, labels, cv=5)\n",
    "print(\"Accuracy: %0.3f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "end = time.time()\n",
    "print('Cross validating takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with SVM Linear SVC + scaling takes 1.1296 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== training and prediction with SVM and scaled features ====== #\n",
    "clf_svm_scale.fit(training_features_scale, labels)\n",
    "pred_svm_scale = list(clf_svm_scale.predict(testing_features_scale))\n",
    "pred_svm_scale = zip(range(testing_size), pred_svm_scale)\n",
    "\n",
    "end = time.time()\n",
    "print('Training with SVM Linear SVC + scaling takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_submission('submission_svm_14_scale.csv', pred_svm_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-C. SVM with RBF as kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beware: takes too long to complete\n",
    "\n",
    "# start = time.time()\n",
    "\n",
    "# # ====== training and prediction with SVM and scaled features ====== #\n",
    "# clf_svm_rbf = svm.SVC(kernel='rbf')\n",
    "# clf_svm_rbf.fit(training_features_scale, labels)\n",
    "# pred_svm_rbf = list(clf_svm_rbf.predict(testing_features_scale))\n",
    "# pred_svm_rbf = zip(range(testing_size), pred_svm_rbf)\n",
    "\n",
    "# end = time.time()\n",
    "# print('Training with SVM Linear SVC + scaling takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_submission('submission_svm_rbf_01_scale.csv', pred_svm_rbf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RandomForest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Random Forest takes 9.2478 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== training and prediction with Random Forest ====== #\n",
    "clf_rf = RandomForestClassifier()\n",
    "clf_rf.fit(training_features, labels)\n",
    "pred_rf = list(clf_rf.predict(testing_features))\n",
    "pred_rf = zip(range(testing_size), pred_rf)\n",
    "\n",
    "end = time.time()\n",
    "print('Training with Random Forest takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_submission('submission_rf_09.csv', pred_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Logistic Regression without scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Logistic Regression takes 5.3049 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== training and prediction with Logistic Regression ====== #\n",
    "clf_lg = LogisticRegression()\n",
    "clf_lg.fit(training_features, labels)\n",
    "pred_lg = list(clf_lg.predict(testing_features))\n",
    "pred_lg = zip(range(testing_size), pred_lg)\n",
    "\n",
    "end = time.time()\n",
    "print('Training with Logistic Regression takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_submission('submission_lg_07.csv', pred_lg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Logistic Regression with scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Logistic Regression + scaling takes 2.8288 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== training and prediction with Logistic Regression + scaling ====== #\n",
    "clf_lg_scale = LogisticRegression()\n",
    "clf_lg_scale.fit(training_features_scale, labels)\n",
    "pred_lg_scale = list(clf_lg_scale.predict(testing_features_scale))\n",
    "pred_lg_scale = zip(range(testing_size), pred_lg_scale)\n",
    "\n",
    "end = time.time()\n",
    "print('Training with Logistic Regression + scaling takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_submission('submission_lg_07_scale.csv', pred_lg_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network (simple version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Neural Network without scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Neural Networks takes 256.8148 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "clf_nn = MLPClassifier(\n",
    "    hidden_layer_sizes = (50,60,70,40,50,30,20,10),\n",
    "    activation = 'relu',\n",
    "    solver = 'adam',\n",
    "    early_stopping = True\n",
    ")\n",
    "clf_nn.fit(training_features, labels)\n",
    "pred_nn = clf_nn.predict(testing_features)\n",
    "pred_nn = zip(range(testing_size), pred_nn)\n",
    "\n",
    "end = time.time()\n",
    "print('Training with Neural Networks takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_submission('submission_nn_15.csv', pred_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Neural Network with scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Neural Networks + scaling takes 250.2031 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "clf_nn_scale = MLPClassifier(\n",
    "    hidden_layer_sizes = (50,60,70,40,50,30,20,10),\n",
    "    activation = 'relu',\n",
    "    solver = 'adam',\n",
    "    early_stopping = True\n",
    ")\n",
    "clf_nn_scale.fit(training_features_scale, labels)\n",
    "pred_nn_scale = clf_nn_scale.predict(testing_features_scale)\n",
    "pred_nn_scale = zip(range(testing_size), pred_nn_scale)\n",
    "\n",
    "end = time.time()\n",
    "print('Training with Neural Networks + scaling takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_submission('submission_nn_10_scale.csv', pred_nn_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Gradient Boosting takes 442.9257 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== Training and predicting with Gradient Boosting ====== #\n",
    "clf_gboost = GradientBoostingClassifier(\n",
    "    loss = 'deviance',\n",
    "    n_estimators = 200\n",
    ")\n",
    "clf_gboost.fit(training_features, labels)\n",
    "pred_gboost = clf_gboost.predict(testing_features)\n",
    "pred_gboost = zip(range(testing_size), pred_gboost)\n",
    "\n",
    "end = time.time()\n",
    "print('Training with Gradient Boosting takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_submission('submission_gboost_05.csv', pred_gboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Adaboost takes 300.6069 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ====== Training and predicting with Gradient Boosting ====== #\n",
    "clf_ada = GradientBoostingClassifier(\n",
    "    loss = 'exponential',\n",
    "    n_estimators = 200\n",
    ")\n",
    "clf_ada.fit(training_features, labels)\n",
    "pred_ada = clf_ada.predict(testing_features)\n",
    "pred_ada = zip(range(testing_size), pred_ada)\n",
    "\n",
    "end = time.time()\n",
    "print('Training with Adaboost takes %.4f s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_submission('submission_ada_05.csv', pred_ada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 'adamic_adar' of importance 0.35864\n",
      "Feature 'katz_index' of importance 0.30099\n",
      "Feature 'jaccard_coefficient' of importance 0.24968\n",
      "Feature 'common_neighbors' of importance 0.03826\n",
      "Feature 'average_degrees' of importance 0.02634\n",
      "Feature 'cosine_sim' of importance 0.01515\n",
      "Feature 'in_kcore' of importance 0.00585\n",
      "Feature 'average_betweenness' of importance 0.00179\n",
      "Feature 'overlapping_title' of importance 0.00176\n",
      "Feature 'avg_pagerank' of importance 0.00093\n",
      "Feature 'temporal_difference' of importance 0.00029\n",
      "Feature 'common_authors' of importance 0.00029\n",
      "Feature 'same_journal' of importance 0.00003\n"
     ]
    }
   ],
   "source": [
    "# ====== compute feature importance ====== #\n",
    "# list of selected features\n",
    "features = [\n",
    "    'temporal_difference',\n",
    "    'common_authors',\n",
    "    'same_journal',\n",
    "    'cosine_sim',\n",
    "    'overlapping_title',\n",
    "    'average_degrees',\n",
    "    'common_neighbors',\n",
    "    'jaccard_coefficient',\n",
    "    'avg_pagerank',\n",
    "    'average_betweenness',\n",
    "    'in_kcore',\n",
    "    'adamic_adar',\n",
    "    'katz_index'\n",
    "]\n",
    "\n",
    "idx = np.argsort(-clf_rf.feature_importances_) # sort the indicator of feature important by decreasing order\n",
    "\n",
    "for i in idx:\n",
    "    print('Feature \\'%s\\' of importance %.5f' % (features[i], clf_rf.feature_importances_[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
